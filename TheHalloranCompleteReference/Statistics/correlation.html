<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
        "http://www.w3.org/TR/REC-html40/loose.dtd">
<!-- Take note of the doctype convention...remove 'Transitional' with
     no space before //EN and change 'loose' to 'strict' -->
<HTML>
<HEAD>
<META http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<META name="Author" content="Nobody">
<TITLE>Correlation, The Correlation Coefficent,
Its Confidence Interval, and Using It To Test Hypotheses</TITLE>
<link type="text/css" href="stats.css" rel="stylesheet">
</HEAD>

<BODY>
<P id="title">
Correlation, The Correlation Coefficient, Its Confidence Interval,
and Testing Hypotheses About the Correlation Coefficient
<P>
To understand the concepts discussed on this page, it is necessary to
have an understanding of what the
<A href="stdnorm.html">standard normal distribution</A> and
<A href="student.html">Student's <i>t</i> distribution</A> are, and why they are
important in testing hypotheses about population parameters,
such as how a mean of sample data set is related to the mean of the
population from which it is drawn, or the difference of means in two
sample data sets, and so on.  There is a particular emphasis on knowing
how to determine the critical values and how they relate to the probability
of whether a null hypothesis (the test question) is to be accepted or
rejected.

<hr>
<h1>Correlation</h1>
Correlation is the way in which two or more variables vary with respect
to one another.  The variables may relate to one another in a way that
as one increases, so does the other (the <em>positive</em> correlation),
or as one increases, the other decreases (the <em>negative</em>
correlation), or they may have no relationship at all (a <em>zero</em>
correlation).
<p>
For example, one may believe that school achievement is related to
parental involvement.  One would then find a good (numerical) indicator of
school achievement and one of parental involvement, and then examine
the correlation.

<h1>The Correlation Coefficient:  A Measure of the Relatedness of Data Sets</h1>
When any two variables, <i>x</i> and <i>y</i>, are related in a linear fashion
such that <i>y<sub>i</sub></i> = <i>a</i> + <i>bx<sub>i</sub></i>, where
<i>a</i> and <i>b</i> are the <i>regression coefficients</i> (the
&#147;y-intercept&#148; and &#147;slope,&#148; respectively),
then the measure of the strength of their relatedness can be determined.
<img style="float:left;"
src="regression/linreg5.gif" alt="plot showing regression characteristics">
The figure below shows scattered data plotted as navy blue-colored diamonds
with the mean of all x values shown as the red vertical line and the
mean of all the y values shown as the purple horizontal line.  The
green dashed line is the fitted regression line (determined using
least squares).  The pink-with-dark-green-border circle point on the line
corresponds to a predicted y value:  from any original navy blue diamond
point, you should be able to draw a line to the pink circle point and the
line will be perfectly vertical (perpendicular to the x axis).  The
maroon-colored plus point on the green dashed line corresponds to a predicted
x value:  from any original navy blue diamond point, you should be able to
draw a line to the maroon-colored plus point and the line will be
perfectly horizontal (perpendicular to the y axis).
<p>
One of the navy blue diamonds has been enlarged (white-colored with navy
blue border) for illustrating the important calculations used in
estimating regression.  Variation is measured in both the <i>x</i> and
<i>y</i> directions.  For each component, the total variation is equal to
the sum of the &#147;explained&#148; and &#147;unexplained&#148; variation.
The &#147;explained&#148; variation is the difference between the
and the predicted deviation (<i>y<sub>r</sub></i>, the <i>y</i> value
on the regression line) and the mean (average)
(<span style="text-decoration:overline;"><i>y</i></span>)
of the data set, while the &#147;unexplained&#148; deviation is
the difference between the original data point (<i>y<sub>i</sub></i>)
and the mean.
<p>
The Pearson Product-Moment Correlation Coefficient (Pearson&#146;s <i>r</i>)
has the following equation:
<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>r</i> =
 <td style="border-bottom:1px solid black;">Covariance(XY)
 <td rowspan="2"> =
 <td style="border-bottom:1px solid black;">
     &sum;<sup><i>k</i></sup><sub style="margin-left:-0.5em;"><i>i</i>=1</sub>
      (<i>x<sub>i</sub></i> &minus; <span class="overbar">x</span>)(<i>y<sub>i</sub></i>
    &minus; <span class="overbar">y</span>) / (<i>N</i> &minus; 1)
 <td rowspan="2"> =
 <td style="border-bottom:1px solid black;">
   (<i>N</i> &minus; 1) &sum;<sup><i>k</i></sup><sub
    style="margin-left:-0.5em;"><i>i</i>=1</sub>(<i>x<sub>i</sub></i>
    &minus; <span class="overbar">x</span>)(<i>y<sub>i</sub></i>
    &minus; <span class="overbar">y</span>)
<tr>
 <td>&sigma;<sub><i>x</i></sub> &sigma;<sub><i>y</i></sub>
 <td>
     [&sum;<sup><i>k</i></sup><sub style="margin-left:-0.5em;"><i>i</i>=1</sub>
     (<i>x<sub>i</sub></i> &minus; <span
         class="overbar">x</span>)/(<i>N</i> &minus; 1)]
     [&sum;<sup><i>k</i></sup><sub style="margin-left:-0.5em;"><i>i</i>=1</sub>
      (<i>y<sub>i</sub></i> &minus; <span
         class="overbar">y</span>)/(<i>N</i> &minus; 1)]
 <td>
     &sum;<sup><i>k</i></sup><sub style="margin-left:-0.5em;"><i>i</i>=1</sub>
     (<i>x<sub>i</sub></i> &minus; <span
         class="overbar">x</span>)
     &sum;<sup><i>k</i></sup><sub style="margin-left:-0.5em;"><i>i</i>=1</sub>
      (<i>y<sub>i</sub></i> &minus; <span
         class="overbar">y</span>)
</table>

<br clear="all">
<h1>The Confidence Interval of the Correlation Coefficient</H1>
Two data sets can be correlated (or tested at least for a correlation)
and some <i>r</i> value can be reported.  If <i>r</i> = 0, it is certain
the data are not correlated.  If <i>r</i> = 0.20, one can say that there
is a weak correlation in the data, and that might be sufficient for
some purposes.
<P>
But a critic could argue that the weakly correlated
data could actually be uncorrelated too.  That is, in more than 5 cases
out of a 100, the data sampling could find data that shows no correlation,
or where <i>r</i> = 0, or crosses from a weak positive/negative correlation
to being a negative/positive correlation.  Such cases weaken entirely
the idea that the data is correlated, even weakly.
<P>
To be <i>confident</i> that the data has at least some correlation,
or that there is no probability that <i>r</i> can be zero, a confidence
interval (CI) can be calculated for a correlation coefficient just as it
can be for a sample mean.  If the interval is such that subtracting
the lower limit of CI of an <i>r</i> value from the <i>r</i> value allows the
<i>r</i> value to be zero or to cross the zero value, then the conclusion
is that there are cases in which <i>r</i> could be zero, and the data sets
have no correlation.  If the CI shows however that <i>r</i> could never be
zero or even cross it, then it can be concluded that there is always
at least some correlation in the data, and never a chance that it could
be zero.
<H2>Factors Influencing the CI of <i>r</i></H2>
As with calculating the confidence interval for a sample mean, the
principal influence on the CI is the size/count of the data sets,
namely <i>n</i>.  As <i>n</i> gets larger, the CI gets smaller, showing
that there is greater confidence that the value, whether a mean or
correlation coefficient, is what it is.
<P>
Also influencing the magnitude of the interval is the degree of confidence
desired.  This is namely the alpha (&#945;) level, and the confidence
interval is expressed as 1 - &#945;.  &#945; = 0.05 for a 95% confidence
interval.
<H2>The Test statistic of the CI of <i>r</i></H2>
The following formula is called <i>Fisher's z transformation</i>:<BR>

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>z<sub>r</sub></i> =
 <td style="border-bottom:1px solid black;vertical-align:middle;">1
 <td rowspan="2">ln <span style="font-size:200%;">(</span>
 <td style="border-bottom:1px solid black;">1 + <i>r</i>
 <td rowspan="2" style="font-size:200%;">)
 <td rowspan="2">[1]
<tr>
 <td>2 <td> 1 &minus; <i>r</i>
</table>
<p>
In fact, the population correlation coefficient, &#961;, is distributed
normally with a mean of approximately:

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>z<sub>&rho;</sub></i> =
 <td style="border-bottom:1px solid black;vertical-align:middle;">1
 <td rowspan="2">ln <span style="font-size:200%;">(</span>
 <td style="border-bottom:1px solid black;">1 + <i>&rho;</i>
 <td rowspan="2" style="font-size:200%;">)
<tr>
 <td>2 <td> 1 &minus; <i>&rho;</i>
</table>
<p>
with a variance which is approximately:

<p align="center">
<table class="eqnserif">
<tr><td style="border-bottom:1px solid black;">1
<tr><td>&radic;<span class="overbar"><i>n</i> &minus; 3</span>
</table>
<p>
The formula for <i>z</i> happens to be the formula for the
inverse hyperbolic tangent of <i>r</i>:

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>z<sub>r</sub></i> =
 <td style="border-bottom:1px solid black;vertical-align:middle;">1
 <td rowspan="2">ln <span style="font-size:200%;">(</span>
 <td style="border-bottom:1px solid black;">1 + <i>r</i>
 <td rowspan="2"><span style="font-size:200%;">)</span> =
   tanh<sup>&minus;1</sup> <i>r</i>
<tr>
 <td>2 <td> 1 &minus; <i>r</i>
</table>
On a Microsoft Excel spreadsheet, the built-in formula
<SPAN style="font:normal 100% 'Arial Narrow',Arial,sans-serif;">FISHER()</SPAN>
can be used to convert <i>r</i> to the Fisher <i>z</i>.  It actually
applies the formula above.  As an exercise, you can put the three formulae
on the spreadsheet--equation 1, the hyperbolic arctan of <i>r</i>, and
<SPAN style="font:normal 100% 'Arial Narrow',Arial,sans-serif;">
FISHER(<i>r</i>)</SPAN>--on the spreadsheet, and see that all values are
identical.
<P>
The confidence interval for <i>z<sub>r</sub></i> is given as:

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>z<sub>r</sub></i> &plusmn;
     <i>z</i><sub>1&minus;&alpha;/2</sub>
  <td style="border-bottom:1px solid black;">1
<tr><td>&radic;<span class="overbar"><i>n</i> &minus; 3</span>
</table>
<p>
The value <i>z</i><sub>1&minus;&alpha;2</sub>
happens to be the value for the standard normal distribution where the limit
sets the area under the curve representing the probability that the true
value of the correlation coefficient, <i>r</i>, could be a value within
the limit.  Note that the extent of the limit can be modified by the number
correlated members in the data sets, that is, <i>n</i>.  As <i>n</i>
increases, the confidence interval narrows, which is what would be expected
as greater numbers of data values are analyzed.

<H2>Computing the CI of <i>r</i></H2>
Suppose that two data sets to be correlated have 20 <i>x,y</i> pairs
of values.

<OL>
<LI>The correlation coefficient, <i>r</i>, is
<A href="corrcoeff.html">calculated</A> in the usual manner.  For the
20-pair bivariate data sets, suppose <i>r</i> = <b>0.79</b>.
<LI>Transform the <i>r</i> value to the Fisher <i>z</i>, using the
formulae in equation <b>1</b> (alternatively, computing the hyperbolic
arctangent on a calculator).  <i>z<sub>r</sub></i> = <b>1.07143</b>.
<LI>For calculating a 95% confidence interval, &#945; = <b>0.05</b>,
and (<b>1</b> - &#945; / <b>2</b>) = <b>0.475</b>.  Looking up any table
or using a spreadsheet equation to determine the <i>z</i> value of
the standard normal distribution
(a normal distribution in which &#956; = <b>0</b> and &#963; = <b>1</b>)
where the area until the curve is 0.4750 starting from zero,
<i>z</i> = <b>1.96</b>.  (On MS Excel, the INVSTDNORM() function.)

<LI>The interval is now calculated with all the inputs:

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2">1.07143 &plusmn; 1.96
  <td style="border-bottom:1px solid black;">1
  <td rowspan="2">= 1.07143 &plusmn; 0.47537
<tr><td>&radic;<span class="overbar">20  &minus; 3</span>
</table>
<p>
The lower limit is therefore <b>0.59606</b> and the upper limit
is <b>1.54680</b>.

<LI>The lower and upper limits are now back-transformed to obtain the
corresponding limits as the lower and upper limit of the <i>r</i> value.
One merely algebraically manipulates equation <b>[1]</b> to get
equation <b>[2]</b>:

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>r</i> =
  <td style="border-bottom:1px solid black;">
     <i>e</i><sup>2<i>z</i></sup> &minus; 1
  <td rowspan="2"> [2]
<tr><td><i>e</i><sup>2<i>z</i></sup> + 1
</table>
<p>
This equation also happens to be <b>tanh</b> (<i>z</i>).
<P>
Applying the formula, <i>r</i> is <b>0.534</b> when <i>z</i> = <b>0.59606</b>,
and <i>r</i> is <b>0.913</b> when <i>z</i> = <b>1.54680</b>.
</OL>
<P>
The final interpretation is that, although <i>r</i> is determined to
be <b>0.79</b>, there is a 95% probability that <i>r</i> is not lower
than <b>0.53</b> and that it is not higher than <b>0.91</b>.  While this
span certainly doesn't cross the zero point or even approach--in which
case, a better than weak correlation is clear--it also means that the
correlation is not perfect, in which case <i>r</i> might be nearly equal
to <b>1.00</b>.
<H1>Testing Hypotheses About The Correlation Coefficient</H1>
<H2>Testing Whether the Population Correlation Coefficient, &#961;, Is Zero</H2>
While the confidence interval of the sample correlation coefficient
<i>r</i> is an expression as to where the true population correlation
coefficient &#961; might be--and if it could possibly be zero, the meaning
that the data sets are unrelated, it is possible to test the hypothesis
about whether &#961; = <b>0</b> as well, using familiar statistical tests.
The null hypothesis, <i>H</i><sub>0</sub>, is that &#961; = <b>0</b>.
<P>
The statistic:

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>t</i> =
  <td style="border-bottom:1px solid black;">
    <i>r</i>&radic;<span class="overbar"><i>n</i> &minus; 2</span>
<tr><td>&radic;<span class="overbar">1 &minus; <i>r</i><sup>2</sup></span>
</table>
<p>
follows Student's <i>t</i> distribution with <i>n</i> - 2 degrees of freedom.
<P>
Applying the formula in the example above:

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>t</i> =
  <td style="border-bottom:1px solid black;">
    0.79 &radic;<span class="overbar">20 &minus; 2</span>
  <td rowspan="2"> = 5.467
<tr><td>&radic;<span class="overbar">1 &minus; 0.6241</span>
</table>
<p>
The critical value of the <i>t</i> distribution for a significance
level of <b>0.05</b> and <b>20 - 2 = 18</b> degrees of freedom is
<b>2.1009</b>.  (This is found in the usual way of using
<A href="student.html">Student's t distribution</A>.)  Because the
<b>5.467 &gt; 2.1009</b>, the null hypothesis (<i>H</i><sub>0</sub>) is rejected,
and the conclusion is that the population correlation coefficient, &#961;,
cannot be equal to zero.  The sample data sets of two variables therefore are
drawn from a population in which data can be said to have some relatedness.
<H2>Testing Whether the Population Correlation Coefficient, &#961;, Is
A Particular Value (That Value Not Being Zero)</H2>
The previous test can be used only for testing whether &#961; = <b>0</b>.
<P>
If there is a desire to test that &#961; = &#961;<sub>0</sub>, then a
test statistic whose values are distributed approximately as the
standard normal is used instead:

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>z</i> =
  <td style="border-bottom:1px solid black;">
        <i>z<sub>r</sub></i> &minus; <i>z</i><sub>&rho;<sub>0</sub></sub>
<tr><td>1 / &radic;<span class="overbar"><i>n</i> &minus; 3</span>
</table>
<P>
Suppose there is a question about whether the population correlation
coefficient is 0.90, that is, the null hypothesis, <i>H</i><sub>0</sub>,
is that &#961; = <b>0.90</b>.  Apply the Fisher <i>z</i> transformation
(equation 1) to the sample correlation coefficient <b>0.79</b> and
to the hypothesized population correlation coefficient <b>0.90</b>,
to obtain, respectively, <i>z<sub>r</sub></i> = <b>1.07143</b> and
<i>z<sub>&#961;<sub>0</sub></sub></i> = <b>1.47222</b>.  Then calculate
the <i>Z</i> statistic:

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>z</i> =
  <td style="border-bottom:1px solid black;">
        1.07143 &minus; 1.47222
  <td rowspan="2"> = &minus;1.65
<tr><td>1 / &radic;<span class="overbar">20 &minus; 3</span>
</table>

<p>
For a level of signifance of &#945; = <b>0.05</b>, the critical value
of the standard normal distribution is <i>z</i> = <b>-1.96</b>.  Since
<b>-1.65 &gt; -1.96</b>, the null hypothesis can <i><b>not</b></i> be
rejected, and there is therefore the possibility that &#961; = <b>0.90</b>.
<P>
The <i>Z</i> statistic calculation above can be used for data sets with
<i>n</i> &#8805; <b>25</b>.  When sample sizes are less than 25 and
greater than 10, the transformation of Hotelling should be used instead:


<P>The source used for the information presented above:
Wayne W. Daniel, <i>Introductory Statistics with Applications</i>,
Boston: Houghton Mifflin, 1977.
</BODY>
</HTML>
