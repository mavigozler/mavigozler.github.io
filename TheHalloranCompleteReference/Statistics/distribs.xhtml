<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN"
     " http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd" [
    <!ENTITY mathml "http://www.w3.org/1998/Math/MathML">
]>
<html xmlns="http://www.w3.org/1999/xhtml"
  xmlns:m="http://www.w3.org/1998/Math/MathML">
<head>
<title>Functions and Distributions</title>
<meta http-equiv="content-Type" content="text/html; charset=utf-8" />
<meta name="Generator" content="Microsoft Word 97" />
<meta name="Optimizer" content="S. M. Halloran" />
<link type="text/css" href="stats.css" rel="stylesheet" />
<style type="text/css">
  math {color:blue;font-size:115%;margin:0 auto;}
</style>
</head>

<body>

<h1>Probability Density Functions</h1>
<p>
Probability density functions of discrete variables are expressed as
<i>p</i>(<i>r</i>).  Probability density functions of continuous variables
are expressed as <i>f</i>(<i>x</i>).  They are normalized such that
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<munder><mo>∑</mo><mi>r</mi></munder>
<mi>p</mi><mrow><mo>(</mo><mi>r</mi><mo>)</mo></mrow><mo>=</mo><mn>1</mn><mrow>
<mspace width="1ex"/><mtext> and </mtext><mspace width="1ex"/></mrow><mrow>
<msubsup><mo>∫</mo><mrow><mo>-</mo><mo>∞</mo></mrow><mo>∞</mo></msubsup></mrow>
<mrow><mi>f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mrow><mi>d</mi>
<mi>x</mi></mrow><mo>=</mo><mn>1</mn></math>

<p>
Statisticians use the <i>distribution function</i> while physicists
call it the <i>cumulative function</i>:
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mi>P</mi><mrow><mo>(</mo><mi>r</mi><mo>)
</mo></mrow><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mo>
-</mo><mo>∞</mo></mrow><mi>r</mi></munderover></mrow><mi>p</mi><mrow><mo>(</mo>
<mi>i</mi><mo>)</mo></mrow><mrow><mspace width="1ex"/><mtext> and </mtext>
<mspace width="1ex"/></mrow><mi>F</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo>
</mrow><mo>=</mo><mrow><msubsup><mo>∫</mo><mrow><mo>-</mo><mo>∞</mo></mrow><mi>
x</mi></msubsup></mrow><mrow><mi>f</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo>
</mrow></mrow><mrow><mi>d</mi><mi>t</mi></mrow></math>



<h2>Moments</h2>
<p>
An <b>expectation value</b> is an algebraic moment of order <i>r</i>
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mi>μ</mi>
<msub><mo>'</mo><mi> r</mi></msub><mo>=</mo><mi>E</mi><mrow><mo>(</mo><mi>x</mi>
<mo>'</mo><mo>)</mo> </mrow><mo>=</mo><munder><mo>∑</mo><mi>k</mi></munder>
<msup><mi>k</mi><mi>r</mi> </msup><mi>p</mi><mrow><mo>(</mo><mi>k</mi><mo>)</mo>
</mrow><mrow><mspace width= "1ex"/><mtext> or </mtext><mspace width="1ex"/>
</mrow><mrow><msubsup><mo>∫</mo> <mrow><mo>-</mo><mo>∞</mo></mrow><mo>∞</mo>
</msubsup></mrow><msup><mi>x</mi><mi> r</mi></msup><mrow><mi>f</mi><mrow><mo>
(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><mi>d</mi><mi>x</mi></mrow>
</math>

<p>
&mu;&prime;<sub>0</sub> = 1 as defined by normalization condition, and
&mu;&prime;<sub>1</sub> is the mean, also called expectation value of the
distribution.
</p>
<p>
Central moments of order <i>r</i> are defined:
</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>
μ</mi><mi>r</mi> </msub><mo>=</mo><mi>E</mi><mrow><mo>(</mo><msup><mrow><mo>
[</mo><mi>k</mi><mo> −</mo><mi>E</mi><mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow>
<mo>]</mo></mrow><mi> r</mi></msup><mo>)</mo></mrow><mrow><mspace width="1ex"/>
<mtext> or </mtext> <mspace width="1ex"/></mrow><mi>E</mi><mrow><mo>(</mo><msup>
<mrow><mo>[</mo><mi> x</mi><mo>−</mo><mi>E</mi><mrow><mo>(</mo><mi>x</mi><mo>)
</mo></mrow><mo>]</mo> </mrow><mi>r</mi></msup><mo>)</mo></mrow></math>

<p>
of which &mu;<sub>2</sub> is the variance of the distribution.
</p>
<p>
The third and fourth central moments are usually not used, but definitions
of skewness &gamma;<sub>1</sub> and kurtosis &gamma;<sub>2</sub> are defined
as:
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>
γ</mi><mn>1</mn> </msub><mo>=</mo><mfrac><msub><mi>μ</mi><mn>3</mn></msub><mrow>
<msubsup><mi> μ</mi><mn>2</mn><mrow><mn>3</mn><mo>/</mo><mn>2</mn></mrow>
</msubsup></mrow> </mfrac><mrow><mspace width="1ex"/>
<mtext>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</mtext><mspace 
width="1ex"/></mrow> <msub><mi>γ</mi><mn>2</mn></msub><mo>=</mo><mfrac><msub>
<mi>μ</mi><mn>4</mn> </msub><mrow><msubsup><mi>μ</mi><mn>2</mn><mn>2</mn>
</msubsup></mrow></mfrac> <mo>-</mo><mn>3</mn></math>

<h2>Characteristic Function</h2>
<p>
For a distribution in a continuous variable <i>x</i>, the Fourier transform
of the probability density function
</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϕ</mi>
<mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow><mo>=</mo><mi>E</mi><mrow><mo>(</mo>
<msup><mi>e</mi><mrow><mi>i</mi><mi>x</mi><mi>t</mi></mrow></msup><mo>)</mo>
</mrow><mo>=</mo><mrow><msubsup><mo>∫</mo><mrow><mo>-</mo><mo>∞</mo></mrow><mo>
∞</mo></msubsup></mrow><msup><mi>e</mi><mrow><mi>i</mi><mi>x</mi><mi>t</mi>
</mrow></msup><mrow><mi>f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>
<mrow><mi>d</mi><mi>x</mi></mrow></math>
<p>
is called its <i>characteristic function</i>.  Its properties include that
&phi;(0) = 1 and |&phi;(<i>t</i>)| &le; 1.
</p>
<h1>Distributions used to generate random data samples</h1>

<h2>Bernoulli</h2>
<p>
This is simply related to the probablity for two mutually exclusive
events.
</p>
<h2>Binomial</h2>
<p>
The probability function is:
</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mo>
(</mo><mtable><mtr><mtd><mi>n</mi></mtd></mtr><mtr><mtd><mi>x</mi></mtd></mtr>
</mtable><mo>)</mo></mrow><msup><mi>p</mi><mi>x</mi></msup><msup><mrow><mo>
(</mo><mn>1</mn><mo>-</mo><mi>p</mi><mo>)</mo></mrow><mrow><mi>n</mi><mo>-</mo>
<mi>x</mi></mrow></msup></math>

<p>
for all values <i>x</i> which are integers.
</p>

<h2>Poisson</h2>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow>
<msup><mi>e</mi><mrow><mo>-</mo><mi>x</mi></mrow></msup><msup><mi>μ</mi><mi>
x</mi></msup></mrow><mrow><mi>x</mi><mo>!</mo></mrow></mfrac></math>


<p>
A probability distribution centered at &#956;.
</p>

<h2>Normal</h2>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mn>
1</mn><mrow><mi>σ</mi><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt></mrow>
</mfrac><msup><mi>e</mi><mrow><mo>-</mo><msup><mrow><mo>(</mo><mi>x</mi><mo>
-</mo><mi>μ</mi><mo>)</mo></mrow><mn>2</mn></msup><mo>/</mo><mn>2</mn><msup><mi>
σ</mi><mn>2</mn></msup></mrow></msup></math>

<p>
The standard normal has a &#956; = 0 and a &#963; = 1.
</p>

<h2>Uniform</h2>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML" ><mrow>
<mi>f</mi><mrow> <mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac>
<mn>1</mn><mrow> <mi>b</mi><mo>-</mo><mi>a</mi></mrow></mfrac><mrow><mspace 
width="1ex"/> <mtext>&nbsp;&nbsp;&nbsp;for&nbsp;&nbsp;&nbsp;</mtext><mspace 
width="1ex"/></mrow><mi>a</mi><mo>&lt;</mo><mi>x</mi><mo> &lt;</mo><mi>b</mi>
</math>

<h2>Student's <i>t</i></h2>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow>
<mo>Γ</mo><mrow><mo>[</mo><mrow><mo>(</mo><mi>ν</mi><mo>+</mo><mn>1</mn><mo>)
</mo></mrow><mo>/</mo><mn>2</mn><mo>]</mo></mrow></mrow><mrow><mo>Γ</mo><mrow>
<mo>[</mo><mi>ν</mi><mo>/</mo><mn>2</mn><mo>]</mo></mrow><msqrt><mrow><mi>ν</mi>
<mi>π</mi></mrow></msqrt></mrow></mfrac><mo> </mo><mfrac><mn>1</mn><msup><mrow>
<mo>(</mo><mn>1</mn><mo>+</mo><msup><mi>x</mi><mn>2</mn></msup><mo>/</mo><mi>
ν</mi><mo>)</mo></mrow><mrow><mrow><mo>(</mo><mi>ν</mi><mo>+</mo><mn>1</mn><mo>)
</mo></mrow><mo>/</mo><mn>2</mn></mrow></msup></mfrac></math>

<p>
with &#957; degrees of freedom.
</p>

<h2><i>F</i></h2>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow>
<mo>Γ</mo><mrow><mo>[</mo><mrow><mo>(</mo><msub><mi>ν</mi><mn>1</mn></msub><mo>
+</mo><msub><mi>ν</mi><mn>2</mn></msub><mo>)</mo></mrow><mo>/</mo><mn>2</mn><mo>
]</mo></mrow></mrow><mrow><mo>Γ</mo><mrow><mo>[</mo><msub><mi>ν</mi><mn>1</mn>
</msub><mo>/</mo><mn>2</mn><mo>]</mo></mrow><mo>Γ</mo><mrow><mo>[</mo><msub><mi>
ν</mi><mn>2</mn></msub><mo>/</mo><mn>2</mn><mo>]</mo></mrow></mrow></mfrac>
<msup><mrow><mo>(</mo><mfrac><mrow><msub><mi>ν</mi><mn>1</mn></msub></mrow>
<mrow><msub><mi>ν</mi><mn>2</mn></msub></mrow></mfrac><mo>)</mo></mrow><mrow>
<msub><mi>ν</mi><mn>1</mn></msub><mo>/</mo><mn>2</mn></mrow></msup><mfrac><mrow>
<msup><mi>x</mi><mrow><mrow><mo>(</mo><msub><mi>ν</mi><mn>1</mn></msub><mo>
-</mo><mn>2</mn><mo>)</mo></mrow><mo>/</mo><mn>2</mn></mrow></msup></mrow><msup>
<mrow><mo>[</mo><mn>1</mn><mo>+</mo><mi>x</mi><mrow><mo>(</mo><msub><mi>ν</mi>
<mn>1</mn></msub><mo>/</mo><msub><mi>ν</mi><mn>2</mn></msub><mo>)</mo></mrow>
<mo>]</mo></mrow><mrow><mrow><mo>(</mo><msub><mi>ν</mi><mn>1</mn></msub><mo>
+</mo><msub><mi>ν</mi><mn>2</mn></msub><mo>)</mo></mrow><mo>/</mo><mn>2</mn>
</mrow></msup></mfrac></math>

<p>
&#957;<sub>1</sub> is the numerator degrees of freedom, and
&#957;<sub>2</sub> is the denominator degrees of freedom.
<i>x</i> must be greater than zero for all <i>x</i>.
</p>

<h2>Cauchy</h2>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mn>
1</mn><mrow><mi>π</mi><mi>b</mi><mrow><mo>[</mo><mn>1</mn><mo>+</mo><msup><mrow>
<mo>{</mo><mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>a</mi><mo>)</mo></mrow><mo>
/</mo><mi>b</mi><mo>}</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow>
</mfrac></math>

<h2>Laplace</h2>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mn>
1</mn><mrow><mn>2</mn><mi>b</mi></mrow></mfrac><msup><mi>e</mi><mrow><mo>-</mo>
<mrow><mo>|</mo><mi>x</mi><mo>-</mo><mi>a</mi><mo>|</mo></mrow><mo>/</mo><mi>
b</mi></mrow></msup></math>

<p>
Also called the <i>double exponential distribution</i>.
</p>

<h2>Logistic</h2>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><msup>
<mi>e</mi><mrow><mo>-</mo><mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>a</mi><mo>)
</mo></mrow><mo>/</mo><mi>b</mi></mrow></msup><mrow><mi>b</mi><msup><mrow><mo>
[</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mo>(</mo><mi>
x</mi><mo>-</mo><mi>a</mi><mo>)</mo></mrow><mo>/</mo><mi>b</mi></mrow></msup>
<mo>]</mo></mrow><mn>2</mn></msup></mrow></mfrac></math>

<h2>Log Normal</h2>
<p>
For all <i>x</i> &gt; 0,
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mn>
1</mn><mrow><mi>x</mi><mi>σ</mi><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt>
</mrow></mfrac><msup><mi>e</mi><mrow><mo>-</mo><msup><mrow><mo>(</mo><mrow><mo>
ln</mo><mi>x</mi></mrow><mo>-</mo><mi>μ</mi><mo>)</mo></mrow><mn>2</mn></msup>
<mo>/</mo><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></msup></math>

<h2>&chi;<sup>2</sup> (chi-square)</h2>
<p>
For all <i>x</i> &gt; 0,
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow>
<msup><mi>x</mi><mrow><mrow><mo>(</mo><mi>ν</mi><mo>-</mo><mn>2</mn><mo>)</mo>
</mrow><mo>/</mo><mn>2</mn></mrow></msup><msup><mi>e</mi><mrow><mo>-</mo><mi>
x</mi><mo>/</mo><mn>2</mn></mrow></msup></mrow><mrow><msup><mn>2</mn><mrow><mi>
ν</mi><mo>/</mo><mn>2</mn></mrow></msup><mo>Γ</mo><mrow><mo>(</mo><mi>ν</mi><mo>
/</mo><mn>2</mn><mo>)</mo></mrow></mrow></mfrac></math>

<p>
with &nu; degrees of freedom.
</p>

<h2>Exponential</h2>
<p>
For all <i>x</i> &gt; 0,
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow>
<msup><mi>e</mi><mrow><mo>-</mo><mi>x</mi><mo>/</mo><mi>b</mi></mrow></msup>
</mrow><mi>b</mi></mfrac></math>

<h2>Gamma</h2>
<p>
For all <i>x</i> &gt; 0,
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow>
<msup><mi>x</mi><mrow><mi>a</mi><mo>-</mo><mn>1</mn></mrow></msup><msup><mi>
e</mi><mrow><mo>-</mo><mi>x</mi><mo>/</mo><mi>b</mi></mrow></msup></mrow><mrow>
<mo>Γ</mo><mrow><mo>(</mo><mi>a</mi><mo>)</mo></mrow><msup><mi>b</mi><mi>a</mi>
</msup></mrow></mfrac></math>

<h2>Weibull</h2>
<p>
For all <i>x</i> &gt; 0,
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow>
<mi>a</mi><msup><mi>x</mi><mrow><mi>a</mi><mo>-</mo><mn>1</mn></mrow></msup>
<msup><mi>e</mi><mrow><mo>-</mo><mi>x</mi><mo>/</mo><mi>b</mi></mrow></msup>
</mrow><msup><mi>b</mi><mi>a</mi></msup></mfrac></math>


<h2>Beta</h2>
<p>
For 0 &le; <i>x</i> &le; 1:
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>
f</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow>
<mo>Γ</mo><mrow><mo>(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo>)</mo></mrow><msup>
<mi>x</mi><mrow><mi>a</mi><mo>-</mo><mn>1</mn></mrow></msup><msup><mrow><mo>
(</mo><mn>1</mn><mo>-</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>b</mi><mo>-</mo>
<mn>1</mn></mrow></msup></mrow><mrow><mo>Γ</mo><mrow><mo>(</mo><mi>a</mi><mo>)
</mo></mrow><mo>Γ</mo><mrow><mo>(</mo><mi>b</mi><mo>)</mo></mrow></mrow></mfrac>
</math>

<h1>Hypothesis Testing</h1>

<h2>Hypotheses about population proportions</h2>
<p>
A study of the operative mortality of 32 patients surgically treated for
aspergilloma was indicated to be 34%. Suppose some internist believes the
proportion of deaths is much higher (= 0.60). Suppose that the internist
justifies this proportion by saying only three patients sampled in another
study showed a 0.33 death proportion (i.e., 1 out of the 3 died).
</p>

<p>
We first specify the test or <i>null</i> hypothesis:
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>
H</mi><mn>0</mn></msub><mo>:</mo><mi>p</mi><mo>=</mo><mstyle fontweight="bold">
<mn>0.60</mn></mstyle></math>

<p>
In the event the null hypothesis is rejected, an alternative hypothesis is
specified so that it is accepted. In this case, the logical hypothesis is that
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>
H</mi><mn>1</mn></msub><mo>:</mo><mi>p</mi><mo>≠</mo><mstyle fontweight="bold">
<mn>0.60</mn></mstyle></math>

<p>
Note that in determining the probability of the hypothesis, we are interested
in the possibility that <i>p</i> is neither &gt;0.60 or &lt;0.60. The
statistical test must be <i>two-sided.</i> There are times when the null
hypothesis tested might be <i>p</i> &gt; 0.60 or <i>p</i> &lt; 0.60, in which
case the alternative hypothesis would be testing <i>one-sidedness.</i> However,
one-sided hypothesis testing is rarely appropriate and is valid only when
</p>

<ol style="list-style: lower-roman;">
<li>the excluded possibility <i>cannot occur</i> (i.e. suppose
    <i>H</i><sub>1</sub> is <i>p</i> &gt; <i>p</i><sub>0</sub>,
    therefore <i>p</i> &lt; <i>p</i><sub>0</sub> cannot possibly occur),
    and </li>
<li>the excluded possibility is of <i>absolutely no interest.</i>
    One-sided testing should be critically assessed when presented
    in the data. </li>
</ol>

<p>
In the testing of proportions, the <b>one-sample <i>Z</i> test</b> is the
appropriate test statistic.  Choice of the <i>Z</i> test requires the
following assumptions:
</p>

<ol>
<li>Although random sampling is preferable, it is not absolute.
The sample must not be biased.</li>
<li>Independent observations are absolutely required.  A good example
is measuring a urine creatinine from one patient at one hospital and
assaying another urine creatinine from a different patient at another
hospital.  These are independent.  If we measure urine creatinine from
the first patient the next day, this is not independent. </li>
<li>Sample size is important.  The closer <i>p</i><sub>0</sub> or
1 &minus; <i>p</i><sub>0</sub> is to zero, the larger the sample required.
In general <i>np</i><sub>0</sub> and <i>n</i>
(1 &minus; <i>p</i><sub>0</sub>) should be &gt; 5. </li>
</ol>

<p>
The <i>Z</i> value is assessed as
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi><mo>=
</mo><mfrac><mrow><mover><mi>p</mi><mo>^</mo></mover><mo>-</mo><msub><mi>p</mi>
<mn>0</mn></msub></mrow><msub><mi>s</mi><mrow><mrow><mtext>prop</mtext></mrow>
</mrow></msub></mfrac></math>

<p>
where <i>p<span style="position:relative;left:-1ex;">&circ;</span></i>
is the sample proportion and <i>s</i><sub>prop</sub> is defined as
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>
s</mi><mrow><mrow><mtext>prop</mtext></mrow></mrow></msub><mo>=</mo><msqrt>
<mrow><mfrac><mrow><mover><mi>p</mi><mo>^</mo></mover><mrow><mo>(</mo><mn>1</mn>
<mo>-</mo><mover><mi>p</mi><mo>^</mo></mover><mo>)</mo></mrow></mrow><mi>n</mi>
</mfrac></mrow></msqrt></math>


<p>
The numerator of the <i>Z</i> statistic tests the difference between the sample
proportion and the test proportion. The denominator is the estimated standard
deviation of the sample proportion. In general, either a large difference in
the sample proportion from the test proportion, or a small estimated standard
deviation will result in a large Z statistic, which makes it possible to reject
the null hypothesis.
</p>
<p>
When the sample is large enough the one-sample <i>Z</i> statistic has an
approximately normal standard distribution. When does one reject or accept the
null hypothesis? It depend upon &#945;, the <i>level of significance</i>.
Generally significance levels of <b>0.10, 0.05, 0.01,</b> and <b>0.001</b> are
chosen. Now we calculate the example above:
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>
s</mi><mrow><mrow><mtext>prop</mtext></mrow></mrow></msub><mo>=</mo><msqrt>
<mrow><mfrac><mrow><mrow><mo>(</mo><mn>0.34</mn><mo>)</mo></mrow><mrow><mo>
(</mo><mn>1</mn><mo>-</mo><mn>0.34</mn><mo>)</mo></mrow></mrow><mn>32</mn>
</mfrac></mrow></msqrt><mo>=</mo><mn>0.084</mn></math>
<p></p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi><mo>=
</mo><mfrac><mrow><mn>0.34</mn><mo>-</mo><mn>0.60</mn></mrow><mn>0.084</mn>
</mfrac><mo>=</mo><mo>-</mo><mn>3.10</mn></math>

<p>
A search of a table of <i>Z</i> statistic indicates a two-tailed probability of
<i>p</i>= <b>0.0019</b>. If our level of significance is <b>0.01</b>, we reject 
the null hypothesis: the probability that the test proportion is different from 
the sample proportion is &lt; <b>0.01</b>. The hypothesis of the internist is 
questionable. Suppose that having rejected the null hypothesis, we find we were 
wrong: we have made a <b>Type I error</b>. The null hypothesis was true. 
Suppose the null hypothesis is accepted and it is found it was not true: a <b>
Type II error.</b> Making Type I errors results from a choice of the level of 
significance. Making Type II errors can be controlled by selection of the
appropriate sample size.
</p>

<h2>Hypotheses about the differences in population proportions</h2>
<p>
In this case the <b>two-sample <i>Z</i> test</b> is used.
</p>

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi><mo>=
</mo><mfrac><mrow><msub><mover><mi>p</mi><mo>^</mo></mover><mn>1</mn></msub><mo>
-</mo><msub><mover><mi>p</mi><mo>^</mo></mover><mn>2</mn></msub></mrow><msub>
<mi>s</mi><mrow><mrow><mtext>diff</mtext></mrow></mrow></msub></mfrac></math>
<p></p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>
s</mi><mrow><mrow><mtext>diff</mtext></mrow></mrow></msub><mo>=</mo><msqrt>
<mrow><mfrac><mrow><msub><mover><mi>p</mi><mo>^</mo></mover><mn>1</mn></msub>
<mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mover><mi>p</mi><mo>^</mo></mover>
<mn>1</mn></msub><mo>)</mo></mrow><msub><mover><mi>p</mi><mo>^</mo></mover><mn>
2</mn></msub></mrow><mn>0.084</mn></mfrac></mrow></msqrt><mo>=</mo><mo>-</mo>
<mn>3.10</mn></math>

<p>
The following assumptions pertain: </p>

<ol>
<li>As long as the samples are not biased,
     random sampling is not essential. </li>
<li>The observations within one sample must be independent of the
     observations of the other sample.  Moreover, the observations
     <i>within</i> each sample must be independent of each other. </li>
<li>Sample size is important.  The closer <i>p</i><sub>1</sub> or
     <i>p</i><sub>2</sub> is to 0 or 1, then a large sample may be
     necessary.</li>
</ol>

<p>
As an example of the two-sample <i>Z</i> test, suppose the stress of a parent
is assessed after admission of parent's child to an intensive care unit. 
233 parents whose child was expected to ICU were compared to 262 parents whose 
child's admission was unexpected. 56% of the planned admission parents and 
75% of the unexpected-admission parents rated their child's condition as 
&quot;extremely severe upon admission.&quot; Assuming the parents had no 
contact within and between each group, the observations are independent. A <i>
s</i><sub>diff</sub> of <b>0.042</b> can be calculared from the data given, and 
a <i>Z</i> statistic of &minus;4.52 is obtained. We find from this a <i>p</i> 
&lt; <b>0.0001</b>. Being &lt; <b>0.01</b>, we reject the hypothesis that these 
population proportions are equal: there is a significant difference.
</p>



</body>
</html>

