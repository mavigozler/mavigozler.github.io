<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN"
               "http://www.w3.org/TR/MathML2/dtd/xhtml-math11-f.dtd" [
  <!ENTITY mathml "http://www.w3.org/1998/Math/MathML">
]>
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<title>Reference in Biostatistics</title>
<meta http-equiv="content-Type" content="text/html; charset=iso-8859-1" />
<meta name="author" content="S. M. Halloran" />
<link type="text/css" href="stats.css" rel="stylesheet" />
<style type="text/css">
	math {font-size:120%;}
</style>
</head>

<body>

<h1>Fundamentals</h1>

<h2>Presentation/Reporting of Data</h2>
<p>Data can be presented in a variety of ways.  Within text, data is
reported in the context of connecting the significance of a value
to an explanation of a phenomenon.  In tabular form, several related
values are presented as a means of comparison or to summarize.
In a visual form, as in a chart or plot, contrasts in values are
more apparent since they are represented by bars either horizontal
or vertical, or by points in a Cartesian coordinate grid, perhaps
connected by lines.  Histograms shows frequency distributions or
cumulative frequency distribtions (sigmoidal curves).</p>

<h3>Frequency Distributions</h3>
<p>The preparation of a histogram, or frequency distribution, is not
always straightforward.  The first and most difficult aspect is
to decide on the class grouping or interval if the data are continuous.
</p>

<h2>Descriptive Statistics</h2>

<h3>Measures of Natural Phenomena Are Normally Distributed</h3>
<p>Whenever someone is interested in looking at a characteristic, trait,
property, parameter, whatever, of some entity, some thought is given
to determining a suitable way of measuring or evaluating this characteristic
to the satisfaction of peers, and then measurements are taken and
the data collected.
</p><p>
Usually, not every subject or entity or species is measured in the
<em>population</em> of interest, but only a <em>sample</em> of the
population is measured.  Subjects/entities are selected <em>at random</em>,
with the idea that each subject/entity is <em>independent</em> of the
other subjets/entities in terms of how they contribute to the measurement
or data collection.  The idea is that no one can claim there is a
<em>bias</em> or confounding factor that influences the reliability
of what is measured, or what is reported in the comparison of measurements.
</p><p>
As data is collected on the <em>sample</em> group representing the
population, it is usually observed on plots called <em>histograms</em>,
where the count of individuals having a certain measurement <i>x</i>
are plotted on a vertical scale against a horiontal scale encompassing
the range of the measurement <i>x</i> values themselves, that the
data takes on the appearance of a bell-shaped curve.  This is called
a Gaussian curve, or Gaussian distribution, in honor of a famous
mathematician who was involved in the development of statistics.
The word <em>distribution</em> emphasizes that the data distributes
or scatters or spreads itself in such a pattern, whatever pattern that
is.
</p><p>
Because the bell-shaped or Gaussian distribution of data occurs routinely
when measuring natural phenomemon, this is said be the way data
naturally or <i>normally</i> distributes itself.  This is an important
distinction.  For many of the techniques used in statistics, the tests
of comparison and analysis, can be used or have meaning only when the
data is normally distributed, <i>or</i> when data itself can be
transformed in such a way so as to approximate data that is normally
distributed data.
</p><p>
Note that last point well:  there are many measurements
of natural phenomena where the distribution is known to be skewed, not
Gaussian, such as measurements that fall close to along a zero point
for which there can be no negative (less than zero) values.  For example,
in medicine, a random sampling of healthy people show liver enzyme
activities that fall in a skewed distribution.  By transforming the values
(usually by taking their logarthim or using some power function), the
curve now approximates a normal distribution and is amenable to
the common tests of parametric statistics.
</p>
<h3>Describing A Set of Data</h3>
<p>When data obtained in a random manner is collected from a population,
and that data, when collected in sufficient numbers, approximates
a Gaussian distribution and can therefore be said to be normally
distributed, certain parameters can be used to describe that data,
and the variation of the data.
</p><p>
One parameter describing the data set is its <em>average</em> or
<em>mean</em>.  This is basically the sum of all the data divided
by the count of the data in the set:
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
  	 <mover>
    <mi>x</mi>
    <mo>-</mo>
    </mover>
    <mo>=</mo>
    <mfrac>
    <mrow>
      <mo>&sum;</mo>
      <msub><mi>x</mi><mi>i</mi></msub>
    </mrow>
    <mrow>
    <mi>n</mi>
    </mrow>
   </mfrac>
  </mrow>
</math>
<p>
This is basically the first-order moment of the data.
</p>
<h3>Describing Variation</h3>
<p>Another well-known parameter is the <em>variance</em>,
which is the sum of the square of the differences of the data with
the mean:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
  	 <msup>
    <mi>&sigma;</mi>
    <mn>2</mn>
    </msup>
    <mo>=</mo>
    <mfrac>
    <mrow>
    	<mo>&sum;</mo>
    	<msup>
    	<mrow>
      <mo>(</mo>
      <msub><mi>x</mi><mi>i</mi></msub>
      <mo>-</mo>
      <mover><mi>x</mi><mo>-</mo></mover>
      <mo>)</mo>
      </mrow>
      <mn>2</mn>
      </msup>
    </mrow>
    <mrow>
    <mi>n</mi>
    <mo>-</mo>
    <mn>1</mn>
    </mrow>
   </mfrac>
  </mrow>
</math>

<p>
This is the second-order moment of the data.  Note that denominator
contains a value, <i>n</i> &#150; 1, which is called the
<a href="glossary.html#df"><i>degrees of freedom</i></a>.
</p><p>
The square root of the variance, the <em>standard deviation</em>
is more commonly reported measure of variation.  When a Gaussian
curve has been fitted to normally distributed data, the peak of
the curve, dividing it into symmetrical halves, will represent the
mean of the data, and the point where the concavity of the curve
faces downard switches to the concavity of the curve (on either side)
facing upward&#151;this is known as the <i>inflection point</i>&#151;
is the point on the curve whose difference from the mean represents
the standard deviation.</p>
<h3>The Mathematically Described Normal Distribution</h3>
<p>The Gaussian curve can be imposed or drawn on a data set known to be
normally distributed.  To draw it, knowledge of the mean and standard
deviation is all that is required.  The equation for the curve is
thus:
</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
  	 <mrow>
    <mi>f</mi>
    <mo>(</mo>
    <mi>x</mi>
    <mo>) =</mo>
    </mrow>
    <mfrac>
       <mrow><mn>1</mn></mrow>
      <mrow><mi>&sigma;</mi><msqrt><mn>2&pi;</mn></msqrt></mrow>
    </mfrac>
    <mrow>
     <msup>
      <mo>e</mo>
    	 <mrow>
    	  <msup>
         <mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>&mu;</mi><mo>)</mo></mrow>
         <mn>2</mn>
        </msup>
       <mo>/</mo>
       <mn>2</mn><msup><mi>&sigma;</mi><mn>2</mn></msup>
       </mrow>
      </msup>
     </mrow>
  </mrow>
</math>

<p>
Since the peak of the curve represents the mean, it is easy to guess
that where the first derivative of the equation is equal to zero, then
<i>x</i> equals the mean:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
  	 <mrow>
    <mrow><msup><mi>f</mi><mo>'</mo></msup></mrow>
    <mrow>
    <mo>(</mo>
    <mi>x</mi>
    <mo>)</mo></mrow>
    <mo>=</mo>
    <mo>-</mo>
    </mrow>
    <mfrac>
       <mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>&mu;</mi><mo>)</mo></mrow>
      <mrow><msup><mi>&sigma;</mi><mn>3</mn></msup><msqrt><mn>2&pi;</mn></msqrt></mrow>
    </mfrac>
    <mrow>
     <msup>
      <mo>e</mo>
    	 <mrow>
    	  <msup>
         <mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>&mu;</mi><mo>)</mo></mrow>
         <mn>2</mn>
        </msup>
       <mo>/</mo>
       <mn>2</mn><msup><mi>&sigma;</mi><mn>2</mn></msup>
       </mrow>
      </msup>
     </mrow>
  </mrow>
</math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow><msup><mi>f</mi><mo>'</mo></msup></mrow>
  <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mn>0</mn>
  <mo>;</mo>
  <mrow><mi>x</mi><mo>=</mo><mi>&mu;</mi></mrow>
 </math>
 <p>
And to prove that the inflection point of the curve represents the
standard deviation, one can determine the value of <i>x</i> when the
second derivative is zero:
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
  	 <mrow>
    <mrow><msup><mi>f</mi><mo>''</mo></msup></mrow>
    <mrow>
    <mo>(</mo>
    <mi>x</mi>
    <mo>)</mo></mrow>
    <mo>=</mo>
    <mfenced open="[" close="]"><mrow>
    <mfrac>
    	<msup>
       <mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>&mu;</mi><mo>)</mo></mrow>
      <mn>2</mn></msup>
      <mrow><msup><mi>&sigma;</mi><mn>5</mn></msup><msqrt><mn>2&pi;</mn></msqrt></mrow>
    </mfrac>
    <mo>-</mo>
    <mfrac>
       <mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>&mu;</mi><mo>)</mo></mrow>
      <mrow><msup><mi>&sigma;</mi><mn>3</mn></msup><msqrt><mn>2&pi;</mn></msqrt></mrow>
    </mfrac>
   </mrow></mfenced>
    </mrow>
   <mrow>
     <msup>
      <mo>e</mo>
    	 <mrow>
    	  <msup>
         <mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>&mu;</mi><mo>)</mo></mrow>
         <mn>2</mn>
        </msup>
       <mo>/</mo>
       <mn>2</mn><msup><mi>&sigma;</mi><mn>2</mn></msup>
       </mrow>
      </msup>
     </mrow>
  </mrow>
</math>

<p>
There is an interesting and legitimate transformation that can be
done on <i>x</i> to produce a value <i>z</i> such that:
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mi>z</mi>
		<mo>=</mo>
		<mfrac>
			<mrow>
				<mi>x</mi>
				<mo>-</mo>
				<mi>&mu;</mi>
			</mrow>
			<mi>&sigma;</mi>
		</mfrac>
	</mstyle>
</math>
<p>
The histogram when plotted against <i>z</i> now has a mean &mu; of zero (0)
and a standard deviation &sigma; of one (1).  This is called the
<b>standard normal distribution</b>.  Measurements that have been
transformed by subtracting the mean and then dividing by the standard
deviation have been said to be <i>standardized</i>.
</p><p>
The original function is now described as:
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mi>f</mi>
		<mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
		<mo>=</mo>
		<mrow>
			<mfrac>
				<mn>1</mn>
				<mrow><mi>&sigma;</mi>
					<msqrt><mrow><mn>2</mn><mi>&pi;</mi></mrow></msqrt>
				</mrow>
			</mfrac>
		</mrow>
		<msup>
			<mi>e</mi>
			<mrow><mo>-</mo><msup><mi>z</mi><mn>2</mn></msup><mo>/</mo><mn>2</mn>
			</mrow>
		</msup>
	</mstyle>
</math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mi>f</mi>
		<mo>'</mo><mo>'</mo>
		<mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
		<mo>=</mo><mn>0</mn><mo>;</mo>
		<mi>x</mi><mo>=</mo><mo>&plusmn;</mo><mi>&sigma;</mi>
	</mstyle>
</math>
<p>
Therefore, the standard deviation is the point <i>x</i> is the
inflection point of the Gaussian curve.</p>

<h3>Random Sampling Multiple Times From A Population</h3>
<p>
Several members of a population randomly selected, each independently
able to produce a measure of a parameter <i>x</i>, will produce
each produce values <i>x<sub>i</sub></i> that can be averaged to
produce a mean <span style="text-decoration:overline;"><i>x</i></span>.
</p><p>
If the randomly sampled group is returned to the population, another
group from the same population is randomly drawn and its mean of
the parameter <i>x</i> is determined.  There is now the mean of the
first group,
<span style="text-decoration:overline;"><i>x</i></span><sub>1</sub>
and the mean of the second group,
<span style="text-decoration:overline;"><i>x</i></span><sub>2</sub>.
</p><p>
If the process is repeated, returning the members of the sampling
group to the population and then conducting another random sampling
and getting the mean of the group, then there will be a number <i>i</i>
of sampling group means as data:
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mrow><mo>{</mo>
			<msub>
			<mover><mrow><mi>x</mi></mrow><mo>&#175;</mo></mover>
			<mn>1</mn>
			</msub>
			<mo>,</mo>
			<msub>
				<mover><mrow><mi>x</mi></mrow><mo>&#175;</mo></mover>
				<mn>2</mn>
			</msub>
			<mo>,</mo>
			<mo>...</mo>
			<mo>,</mo>
			<msub>
			<mover><mrow><mi>x</mi></mrow><mo>&#175;</mo></mover>
				<mi>i</mi>
			</msub>
			<mo>}</mo>
		</mrow>
	</mstyle>
</math>
<p>
Suppose <i>i</i> is to be all the possible sampling groups.  If the
sample size is <i>n</i> and the population size is <i>N</i>, then the
number of all possible samples of size <i>n</i> with replacement during
sampling is <i>N&nbsp;<sup>n</sup></i>.  The mean of all the sample
means obtained can be expressed as:
</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<msub>
			<mi>&mu;</mi>
			<mover><mrow><mi>x</mi></mrow><mo>&#175;</mo></mover>
		</msub>
		<mo>=</mo>
		<mfrac>
			<mrow><mo>&sum;</mo>
				<mrow>
				<msub>
					<mover><mrow><mi>x</mi></mrow>
					<mo>&#175;</mo>
					</mover>
					<mi>i</mi>
				</msub>
				</mrow>
			</mrow>
			<mrow>
				<msup>
					<mi>N</mi>
					<mi>n</mi>
				</msup>
			</mrow>
		</mfrac>
	</mstyle>
</math>
<p>
The mean of all sample means,
<i>&mu;</i>&nbsp;<sub><span style="text-decoration:overline;"><i>x</i></span></sub>,
is <em>exactly</em> equal to the population
mean, <i>&mu;</i>.
</p><p>
The variance of the sample means, is the population variance,
&sigma;<sup>2</sup>, divided
by the sample size of the group, <i>n</i>:
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<msub><mi>&sigma;</mi><mover><mrow><mi>x</mi></mrow><mo>&#175;</mo></mover>
		</msub>
		<mo>=</mo>
		<mfrac>
			<mrow><msup><mi>&#963;</mi><mn>2</mn></msup></mrow>
			<mrow><mi>n</mi></mrow>
		</mfrac>
	</mstyle>
</math>
<p>
The square root of [4] is referred to as the <i>standard error of the
mean</i>, (sometimes abbreviated S.E.M. or SEM).
</p><p>
The source used for the information in this section:<br />
Wayne W. Daniel, <i>Introductory Statistics with Applications</i>,
Boston: Houghton Mifflin, 1977.
</p>

<h2>Probability</h2>
<p>Probability is concerned with the relative likelihood that a certain
event will occur or not occur.  Probabilities can be ascertained
either empirically or theoretically.  Empirically derived probabilities
are everywhere:  survival of a certain type of cancer based on known
survival rates, the odds on a horse winning based on prior races.
Theoretical probabilities can be determined based on known properties
of systems, where the math is certain.
</p>
<h3>Permutations</h3>
<p>A <i>permutation</i> is an arrangment of objects in a specific sequence.
For instance, a horse (H), a cow (C), and a sheep (S), can be arranged
in a linear fashion in six different ways:  HCS, HSC, CHS, CSH, SHC, and
SCH.  This number of permutations can be calculated more readily by
determining that for the first position, there are 3 possibilities, for the
second position, 2 possibilities remain after the first was selected, and for
the third position, only 1 possibility remains.  The product of these
possibilities is 3 &times; 2 &times; 1 = 6.
</p><p>
When a set of <i>n</i> objects is to be arranged <i>n</i> ways, this is represented
as:
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<msub><mrow></mrow><mi>n</mi></msub>
		<msub><mi>P</mi><mi>n</mi></msub>
		<mo>=</mo><mi>n</mi><mrow><mo>(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>)</mo>
		</mrow><mrow><mo>(</mo><mi>n</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
		<mo>...</mo><mrow><mo>(</mo><mn>3</mn><mo>)</mo></mrow><mrow><mo>(</mo>
			<mn>2</mn><mo>)</mo></mrow><mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow>
		<mo>=</mo><mi>n</mi><mo>!</mo></mstyle>
</math>
<p>
Note that this formula corresponds to arranging objects in a line.  If objects
are arranged in a circle, then the following formula holds:
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<msub><mrow></mrow><mi>n</mi></msub>
		<mi>P</mi>
		<msub><mo>'</mo><mi>n</mi></msub>
		<mo>=</mo>
		<mfrac><mrow><mi>n</mi><mo>!</mo></mrow><mi>n</mi></mfrac>
		<mo>=</mo><mrow>
			<mo>(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>)</mo>
		</mrow>
		<mo>!</mo>
	</mstyle>
</math>
<p>
Note that the <i><sub>n</sub>P&prime;<sub>n</sub></i> notation is used for
circular permutation.  If the cow, horse and sheep were arranged in a circular
fashion, it can only be arranged in two ways:  (3 &minus; 1)! = 2! = 2.
</p><p>
Suppose that there are <i>n</i> objects but only <i>r</i> objects will be used
at a time.  The number of linear permutations
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<msub><mrow></mrow><mi>n</mi></msub><msub><mi>P</mi><mi>r</mi></msub>
		<mo>=</mo>
		<mfrac>
			<mrow><mi>n</mi><mo>!</mo></mrow>
			<mrow><mrow><mo>(</mo><mi>n</mi><mo>-</mo><mi>r</mi><mo>)</mo></mrow>
				<mo>!</mo></mrow>
		</mfrac>
	</mstyle>
</math>
<p>
The relationship between two events can be described as being
mutually exclusive or conditionally dependent.  Two events, <i>X</i>
and <i>Y</i> are <b>mutually exclusive</b> if the occurrence of
one precludes the occurrence of the other.  A coin can be flipped
to heads or tails, but not both, and so the results of coin flip
are mutually exclusive.  Two events, <i>X</i> and <i>Y</i>, are
<b>conditionally dependent</b> if the outcome of <i>Y</i> depends on
<i>X</i>, or <i>X</i> depends on <i>Y</i>.
</p><p>
In the case of mutually exclusive events, the probability of event
1 or 2 is the sum of their individual probabilities.  This
is called the <b>additive rule</b>.  Thus,
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mi>P</mi><mrow><mo>(</mo><mi>X</mi><mrow><mspace width="1ex"></mspace>
		<mtext>or</mtext><mspace width="1ex"></mspace></mrow><mi>Y</mi><mo>)</mo>
		</mrow><mo>=</mo>
		<mi>P</mi>
		<mrow><mo>(</mo><mi>X</mi><mo>)</mo></mrow>
		<mo>+</mo>
		<mi>P</mi><mrow><mo>(</mo><mi>Y</mi><mo>)</mo>
		</mrow>
		<mo>)</mo>
	</mstyle>
</math>
<p>
For conditionally dependent events, the probability of event
2 occurring so long as event 1 has occurered is the product
of the probability of event 1 and of event 2 given event 1.  This is
the <b>multiplicative rule</b>.  Thus,
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true"><mi>P</mi>	<mrow><mo>(</mo>
			<mi>X</mi><mrow><mspace width="1ex"></mspace>
				<mtext>and</mtext><mspace width="1ex"></mspace></mrow>
			<mi>Y</mi><mo>)</mo></mrow>
		<mo>=</mo><mrow><mo>[</mo><mi>P</mi><mrow><mo>(</mo><mi>X</mi><mo>)</mo>
			</mrow><mo>]</mo></mrow>
		<mrow><mo>[</mo><mi>P</mi><mrow><mo>(</mo><mi>Y</mi><mo>)</mo>
			</mrow><mo>)</mo></mrow><mo>]</mo></mstyle>
</math>
<p>
<b>Independent</b> events are events that have no relationship to any
other event.
</p>
<h3>Binomial Distribution</h3>
<p>
The binomial distribution shows the probabilities of different outcomes
for a series of random events, each of which can have only one
of two values.  Coin flipping is one example.  Since the probability
of a head or tail is 0.5, the probability of flipping heads twice
is the product of the number of times of the probability of flipping
heads, name 0.5 &times; 0.5 = 0.25.
</p><p>
When the example requires ascertaining a larger number of possibilities,
it becomes difficult to write out all possibilities and then sum the
number that meet the criteria.  Suppose a kid puts on shoes 10 times:
what is the probability he gets it wrong exactly 7 of those times?
The following formula, called the <b>binomial expansion</b>, applies:
</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mrow><mo>(</mo><mtable><mtr><mtd><mi>n</mi></mtd></mtr>
		<mtr><mtd><mi>r</mi></mtd></mtr></mtable>
			<mo>)</mo></mrow><msup><mi>p</mi><mi>r</mi></msup>
		<msup><mi>q</mi><mrow><mi>n</mi><mo>-</mo><mi>r</mi></mrow></msup>
	</mstyle>
</math>
<p>
where <i>n</i> = the number of attempts (events), <i>r</i> = number
of favorable outcomes in all attempts (events), <i>p</i> = probability
of a successuful outcome (or the outcome of interest) in one
attempt (event), and <i>q</i> = probability of a failed outcome (or
other outcome) = 1 &minus; <i>p</i>.
</p><p>
In the example, if the kid has a 0.5 probability of getting it wrong,
then
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mrow><mo>(</mo><mtable><mtr><mtd><mn>10</mn></mtd></mtr>
				<mtr><mtd><mn>7</mn></mtd></mtr></mtable>
			<mo>)</mo>
			</mrow><msup><mn>0.5</mn><mn>7</mn></msup>
		<msup><mn>0.5</mn><mrow><mn>10</mn><mo>-</mo><mn>7</mn></mrow>
		</msup><mo>=</mo><mn>0.1172</mn>
	</mstyle>
</math>
<p>
Thus there's a 12% probability if the kid were putting them on at random.
</p><p>
If one plots the probability versus the number of attempts (<i>n</i>),
a distribution curve much like the normal distribution begins to appear,
especially as <i>n</i> gets larger.  This is the <b>binomial distribution</b>,
and it has the following parameters:
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
<mtext>mean = </mtext><mi>n</mi><mi>p</mi></mstyle></math>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mtext>variance = </mtext><mi>n</mi><mi>p</mi><mi>q</mi>
	</mstyle>
</math>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true"><mtext>SD = </mtext><msqrt><mrow><mi>n</mi><mi>p</mi>
				<mi>q</mi></mrow></msqrt>
	</mstyle>
</math>

<h2>Inference in Statistics</h2>

<h3>Hypothesis Testing</h3>

<h4>Hypotheses about population proportions</h4>
<p>
A study of the operative mortality of 32 patients surgically
treated for aspergilloma was indicated to be 34%.  Suppose some
internist believes the proportion of deaths is much higher (= 0.60).
Suppose that the internist justifies this proportion by saying only
three patients sampled in another study showed a 0.33 death proportion
(i.e., 1 out of the 3 died).
</p><p>
We first specify the test or <i>null</i> hypothesis:
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<msub><mi>H</mi><mn>0</mn></msub><mo>:</mo>
		<mi>p</mi><mo>=</mo><mn bold="true">0.60</mn></mstyle>
</math>
<p>
In the event the null hypothesis is rejected, an alternative
hypothesis is specified so that it is accepted.  In this case,
the logical hypothesis is that
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<msub><mi>H</mi><mn>0</mn></msub><mo>:</mo>
		<mi>p</mi><mo>&ne;</mo><mn bold="true">0.60</mn></mstyle>
</math>

<p>
Note that in determining the probability of the hypothesis, we
are interested in the possibility that  <i>p</i> is neither &gt;0.60
or &lt;0.60.  The statistical test must be <i>two-sided.</i>
There are times when the null hypothesis tested might be <i>p</i> &gt; 0.60
or <i>p</i> &lt; 0.60, in which case the alternative hypothesis would
be testing <i>one-sidedness.</i>  However, one-sided hypothesis
testing is rarely appropriate and is valid only when
</p>
<ol style="list-style: lower-roman;">
<li>the excluded possibility <i>cannot occur</i> (i.e. suppose
    <i>H</i><sub>1</sub> is <i>p</i> &gt; <i>p</i><sub>0</sub>,
    therefore <i>p</i> &lt; <i>p</i><sub>0</sub> cannot possibly occur),
   and</li>
<li>the excluded possibility is of <i>absolutely no interest.</i>
    One-sided testing should be critically assessed when presented
   in the data.</li>
</ol>
<p>
In the testing of proportions, the <b>one-sample <i>Z</i> test</b> is the
appropriate test statistic.  Choice of the <i>Z</i> test requires the
following assumptions:
</p>
<ol>
<li>Although random sampling is preferable, it is not absolute.
The sample must not be biased.</li>
<li>Independent observations are absolutely required.  A good example
is measuring a urine creatinine from one patient at one hospital and
assaying another urine creatinine from a different patient at another
hospital.  These are independent.  If we measure urine creatinine from
the first patient the next day, this is not independent.</li>
<li>Sample size is important.  The closer <i>p</i><sub>0</sub> or
1 &minus; <i>p</i><sub>0</sub> is to zero, the larger the sample required.
In general <i>np</i><sub>0</sub> and <i>n</i>
(1 &minus; <i>p</i><sub>0</sub>) should be &gt; 5.</li>
</ol>

<p>
The <i>Z</i> value is assessed as</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mi>Z</mi><mo>=</mo><mfrac><mrow><mover><mi>p</mi><mo>&#94;</mo></mover>
				<mo>-</mo><msub><mi>p</mi><mn>0</mn></msub></mrow>
			<mrow><msub><mi>s</mi><mrow><mrow><mtext>prop</mtext></mrow></mrow>
				</msub>
			</mrow>
		</mfrac>
	</mstyle>
</math>
<p>
where <i>p<span style="position:relative;left:-1ex;">&circ;</span></i>
is the sample proportion and <i>s</i><sub>prop</sub> is defined as
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true"><msub><mi>s</mi><mrow><mtext>prop</mtext></mrow>
		</msub>
		<mo>=</mo><msqrt><mrow>	<mfrac><mrow><mover><mi>p</mi><mo>&#94;</mo></mover>
		<mrow><mo>(</mo><mn>1</mn><mo>-</mo><mover><mi>p</mi><mo>&#94;</mo></mover>
			<mo>)</mo></mrow></mrow><mi>n</mi></mfrac></mrow></msqrt>
	</mstyle>
</math>
<p>
The numerator of the <i>Z</i> statistic tests the difference
between the sample proportion and the test proportion.
The denominator is the estimated standard deviation of the sample
proportion.  In general, either a large difference in the sample
proportion from the test proportion, or a small estimated standard
deviation will result in a large Z statistic, which makes it possible
to reject the null hypothesis.
</p><p>
When the sample is large enough the one-sample <i>Z</i> statistic has an
approximately normal standard distribution.  When does one reject or
accept the null hypothesis?  It depend upon &alpha;, the
<i>level of significance</i>.  Generally significance levels of
<b>0.10, 0.05, 0.01,</b> and <b>0.001</b> are chosen.
Now we calculate the example above:
</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<msub><mi>s</mi><mrow><mtext>prop</mtext></mrow></msub>
		<mo>=</mo><msqrt><mrow><mfrac><mrow><mrow><mo>(</mo><mn>0.34</mn>
	<mo>)</mo></mrow>	<mrow><mo>(</mo><mn>1</mn><mo>-</mo><mn>0.34</mn>
	<mo>)</mo></mrow></mrow><mn>32</mn></mfrac></mrow></msqrt><mo>=</mo>
		<mn>0.084</mn></mstyle>
</math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mi>Z</mi><mo>=</mo><mfrac><mrow><mn>0.34</mn><mo>-</mo>
		<mn>0.60</mn></mrow><mn>0.084</mn></mfrac><mo>=</mo><mn>-3.10</mn>
	</mstyle>
</math>

<p>
A search of a table of <i>Z</i> statistic indicates a two-tailed
probability of <i>p</i>= <b>0.0019</b>.  If our level of significance
is <b>0.01</b>, we reject the null hypothesis:  the probability that
the test proportion is different from the sample proportion
is &lt; <b>0.01</b>.  The hypothesis of the internist is questionable.
Suppose that having rejected the null hypothesis, we find we were wrong:
we have made a <b>Type I error</b>.  The null hypothesis was true.
Suppose the null hypothesis is accepted and it is found it was not true:
a <b>Type II error.</b>  Making Type I errors results from a choice of
the level of significance.  Making Type II errors can be controlled
by selection of the appropriate sample size.  &beta; is the probability
of making a type II error.</p>

<table style="margin:auto;">
<col style="padding:0.25em 0.5em" />
<col span="2" style="text-align:center;padding:0.25em 0.5em" />
<tr><th rowspan="2"></th><th colspan="2">Truth</th></tr>
<tr><th>No difference</th><th>Difference</th></tr>
<tr><th><i>H</i><sub>0</sub> accepted </th><td> 1 &minus; &alpha; </td>
	<td> &beta;</td></tr>
<tr><th><i>H</i><sub>0</sub> rejected </th><td> &alpha; </td>
	<td> 1 &minus; &beta;</td></tr>
</table>

<h4>Hypotheses about the differences in population proportions</h4>
<p>
In this case the <b>two-sample <i>Z</i> test</b> is used.</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mi>Z</mi><mo>=</mo><mfrac><mrow><msub><mover><mi>p</mi><mo>&#94;</mo>
		</mover><mn>1</mn></msub><mo>-</mo><msub><mover><mi>p</mi>
		<mo>&#94;</mo></mover><mn>2</mn></msub></mrow><mrow><msub>
		<mi>s</mi><mrow><mtext>diff</mtext></mrow></msub></mrow></mfrac></mstyle>
</math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<msub><mi>s</mi><mrow><mtext>diff</mtext>	</mrow></msub><mo>=</mo>
		<msqrt><mrow><mfrac><mrow><msub><mover><mi>p</mi><mo>&#94;</mo></mover>
		<mn>1</mn></msub>	<mrow><mo>(</mo><mn>1</mn><mo>-</mo>
		<msub><mover><mi>p</mi><mo>&#94;</mo></mover><mn>1</mn></msub>
	<mo>)</mo></mrow>	<msub><mover><mi>p</mi><mo>&#94;</mo></mover>
		<mn>2</mn></msub></mrow><mrow><mn>0.084</mn></mrow></mfrac>
	</mrow></msqrt><mo>=</mo><mn>-3.10</mn></mstyle></math>
<p>
The following assumptions pertain:
</p>
<ol>
<li>As long as the samples are not biased,
   random sampling is not essential.</li>
<li>The observations within one sample must be independent of the
     observations of the other sample.  Moreover, the observations
   <i>within</i> each sample must be independent of each other.</li>
<li>Sample size is important.  The closer <i>p</i><sub>1</sub> or
     <i>p</i><sub>2</sub> is to 0 or 1, then a large sample may be
   necessary.</li>
</ol>
<p>
As an example of the two-sample <i>Z</i> test, suppose the stress of a
parent is assessed after admission of parent&#146;s child to
an intensive care unit.  233 parents whose child was expected
to ICU were compared to 262 parents whose child&#146;s admission
was unexpected.  56% of the planned admission parents and
75% of the unexpected-admission parents rated their child&#146;s
condition as &quot;extremely severe upon admission.&quot;
Assuming the parents had no contact within and between each
group, the observations are independent.
A <i>s</i><sub>diff</sub> of <b>0.042</b> can be calculated
from the data given, and a <i>Z</i> statistic of &minus;4.52 is obtained.
We find from this a <i>p</i> &lt; <b>0.0001</b>.  Being &lt; <b>0.01</b>,
we reject the hypothesis that these population proportions are equal:
there is a significant difference.</p>

<h1>Testing Differences Between Means and Analysis of Variance</h1>

<p>This table shows the type of test that should be done depending on the
type of data in hand:</p>

<table style="margin:auto;">
<tr><th>Data Organization </th><th>Parametric</th>  <th>Nonparametric</th></tr>
<tr><td>One sample</td>  <td>One-sample <i>t</i> test </td>
	<td>Wilcoxon signed-rank test</td></tr>
<tr><td>Matched pairs (repeated measures)</td>
<td colspan="2">One-sample test on differences within pairs</td></tr>
<tr><td>Two independent samples</td>
<td>Two-sample <i>t</i> test</td>   <td>Wilcoxon rank sum test</td></tr>
<tr><td>Several independent samples</td>
<td>One way ANOVA</td>           <td>Kruskal-Wallis test</td></tr>
</table>

<p>
There is a separate <a href="anova.html">document</a> on the
analysis of variance (ANOVA) with testing of the means of the independent.</p>


<h2>One Sample <i>t</i> Testing</h2>
<p>Suppose a single random sample of size <i>n</i> is drawn and the data shows a mean
<span style="text-decoration:overline;"><i>x</i></span> and a standard
deviation <i>s</i>.  The sample is drawn from a population with mean
<i>&mu;</i> and standard deviation <i>&sigma;</i>.</p>
<p>
The null hypothesis is whether some mean <i>&mu;</i><sub>0</sub> is equal to
the population mean <i>&mu;</i> while the alternative is that the hypothesized
<i>&mu;</i><sub>0</sub> is not equal.</p>
<p>
<i>H</i><sub>0</sub>:  <i>&mu; = &mu;</i><sub>0</sub><br />
<i>H</i><sub>A</sub>:  <i>&mu; &ne; &mu;</i><sub>0</sub></p>

<p>
If <i>n</i> is large, then the normal deviate <i>Z</i> =
(<span style="text-decoration:overline;"><i>X</i></span> &minus; &mu;) /
<i>&sigma;</i><sub><span style="text-decoration:overline;"><i>X</i></span></sub>
could be used, since <i>s</i><sub><span style="text-decoration:overline;"><i>X</i></span></sub> comes close to
<i>&sigma;</i><sub><span style="text-decoration:overline;"><i>X</i></span></sub>.
</p><p>
Instead of <i>Z</i>, a different distribution, Student's <i>t</i> distribution is used.
Unlike <i>Z</i>, the <i>t</i> distribution depends on the <b>degrees of freedom</b>,
symbolized <i>&nu;</i>, which is usually related to the sample size.  In fact
<i>&nu;</i> = <i>n</i> &minus; 1 for a one-sample test.
</p><p>
Hence one calculates the probability by first calculating the <i>t</i> statistic:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mi>t</mi><mo>=</mo><mfrac><mrow><mover><mi>X</mi>	<mo>&#175;</mo>
		</mover><mo>-</mo><mi>&#956;</mi></mrow><mrow><mi>s</mi><mo>/</mo>
		<msqrt><mrow><mi>n</mi></mrow></msqrt></mrow></mfrac></mstyle>
</math>
<p>
If |<i>t</i>| &ge; <i>t</i><sub><i>&alpha;</i>(2),<i>&nu;</i></sub>,
null hypothesis <i>H</i><sub>0</sub> is rejected.
<i>t</i><sub><i>&alpha;</i>(2),<i>&nu;</i></sub> is a critical value on
the <i>t</i> distribution having <i>&nu;</i> degrees of freedom and where
the &alpha; level of significance is used in the two-tailed fashion.</p>

<h3>Sample Size Determination</h3>
<p>Suppose one wants to know how big the sample size must be in a single
sampling from a population in order to determine <i>&mu;</i> to some
degree of probability.  We know from the <i>t</i> equation that we can
rearrange to solve for <i>n</i>:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true"><mi>n</mi><mo>=</mo><mfrac><mrow><msup><mi>s</mi>
		<mn>2</mn></msup><mrow><msubsup><mi>t</mi><mrow><mi>&#945;</mi>
		<mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow><mo>,</mo><mi>n</mi>
		<mo>-</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow></mrow>
	<mrow><msup><mi>d</mi><mn>2</mn></msup></mrow></mfrac></mstyle>
</math>
<p>
<i>d</i> is an expression of the confidence interval for sample mean
<span style="text-decoration:overline;"><i>X</i></span>, which is
(<i>s</i>/&radic;<i>n</i>)<i>t</i><sup>2</sup><sub>&alpha;(2),<i>&nu;</i></sub>,
<i>s</i> is the estimated variance in the sample from a normal population.</p>
<p>
In addition suppose, the sample size determination is to be made so that the
probability of committing a Type I error and a Type II error are considered:</p>

<table class="eqnserif">
<tr><td rowspan="2"><i>n</i> =</td>
 <td style="border-bottom:1px solid black;">
<i>s</i><sup>2</sup>(<i>t</i><sub>&alpha;,<i>&nu;</i></sub> +
<i>t</i><sub>&beta;(1),<i>&nu;</i></sub>)<sup>2</sup></td></tr>
<tr><td align="center"><i>&delta;</i> <sup>2</sup></td></tr>
</table>

<p>
&alpha; is either &alpha;(1) or &alpha;(2) depending on whether a one- or
two-tailed test is used, whereas &beta; is for a one-tailed determination
(see example below).</p>
<p>
<b>Examples</b>.  Suppose a drug is used to achieve weight loss.  Weight change data (in kg)
is given as follows:  0.2, -0.5, -1.3, -1.6, -0.7, 0.4, -0.1, 0.0,
-0.6, -1.1, -1.2, and -0.8.  For <i>n</i> = 12 data (<i>&nu;</i> = 11),
<span style="text-decoration:overline;"><i>X</i></span> = -0.61 kg,
<i>s</i><sup>2</sup> = 0.4008 kg<sup>2</sup>, <i>s</i> / &radic;<i>n</i>
= 0.18 kg.  If we wanted to determine the sample size such that the
95% confidence intervale was not wider than 0.5 kg, then <i>d</i> =
0.25 kg (half the interval), and &alpha; = 0.05.</p>
<p>
Suppose we guess that <i>n</i> = 40 is necessary in order to find a
<i>&nu;</i> for <i>t</i>.  The critical value is <i>t</i><sub>0.05(2),39</sub> =
2.023.  The sample size estimate is:</p>
<p style="margin-left:2em;">
<i>n</i> = (0.4008)(2.023)<sup>2</sup> / (0.25)<sup>2</sup> = 26.2.</p>
<p>
Rounding up estimate the sample size if <i>&nu;</i> = 26 if <i>n</i> = 27.
<i>t</i><sub>0.05(2),26</sub> = 2.056</p>
<p style="margin-left:2em;">
<i>n</i> = (0.4008)(2.056)<sup>2</sup> / (0.25)<sup>2</sup> = 27.1.</p>
<p>
It is therefore concluded that a sample size &gt; 27 is required to obtain
such a confidence interval.</p>
<p>
Suppose we want to test <i>H</i><sub>0</sub>: <i>&mu;</i> = <i>&mu;</i><sub>0</sub>.
Testing will be with &alpha;=0.05 with a 90% chance of detecting a population
mean different from <i>&mu;</i><sub>0</sub> = 0 by as little as 1.0.  Suppose
we collect data and find <i>s</i><sup>2</sup> = 1.5682.  Suppose we guess that
<i>n</i> = 20 is required.   <i>t</i><sub>0.05(2),19</sub> = 2.093.  Since
&beta; = 1 &minus; 0.90 = 0.10, then <i>t</i><sub>0.10(1),19</sub> = 1.328.
The first estimate is then</p>
<p style="margin-left:2em;">
<i>n</i> = (1.5682)(2.093 + 1.328)<sup>2</sup> / (1.0)<sup>2</sup> = 18.4.</p>
<p>
Now alter the estimate for <i>n</i> = 19: <i>t</i><sub>0.05(2),18</sub> = 2.101
and <i>t</i><sub>0.10(1),18</sub> = 1.330.</p>
<p style="margin-left:2em;">
<i>n</i> = (1.5682)(2.101 + 1.330)<sup>2</sup> / (1.0)<sup>2</sup> = 18.5.</p>
<p>
Thus a sample size of 19 is required.</p>

<h3>Power of One-Sample <i>t</i> Testing</h3>
<p>What is the probability of correctly rejecting a false <i>H</i><sub>0</sub>?</p>
<p style="margin-left:2em;">
<i>t</i><sub><i>&beta;</i>(1),<i>&nu;</i></sub> = [&delta;/(<i>s</i>/&radic;<i>n</i>)] &minus;
<i>t</i><sub><i>&alpha;,&nu;</i></sub></p>
<p>
Suppose that a sample (<i>n</i> = 12) with <i>s</i><sup>2</sup> = 1.5682 is from
a population with mean <i>&mu;</i>.  We want to know the probability of detecting
a difference <i>&mu; &minus; &mu;</i><sub>0</sub> of 1.0.  We find that
<i>t</i><sub>0.05(2),11</sub> = 2.201:</p>
<p style="margin-left:2em;">
<i>t</i><sub><i>&beta;</i>(1),11</sub> = [1.0/(&radic;1.5682/&radic;12)] &minus;
2.201 = 0.57</p>
<p>
For <i>t</i><sub><i>&beta;</i>(1),11</sub> = 0.57 and <i>&nu;</i> = 11, &beta;
&gt; 0.25, and the power 1 &minus; &beta; &lt; 0.75.</p>


<h2>Two-Sample <i>t</i> Testing:  Comparing Two Means</h2>
<p>The <i>z</i> test is a measure of whether two particular population
means are significantly different from one another:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true"><msub><mi>H</mi><mn>0</mn></msub>
		<mo>:</mo><msub><mi>&#956;</mi><mn>1</mn></msub><mo>=</mo>
		<msub><mi>&#956;</mi><mo>2</mo></msub></mstyle>
</math>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true"><msub><mi>H</mi><mn>1</mn></msub>
		<mo>:</mo><msub><mi>&#956;</mi><mn>1</mn></msub><mo>&#8800;</mo>
		<msub><mi>&#956;</mi><mo>2</mo></msub>
	</mstyle>
</math>
<p>
The <i>t</i> test is actually a measure of whether the <b>differences</b>
between two sample or populations means are significantly different
from zero.</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true"><msub><mi>H</mi><mn>0</mn></msub>
		<mo>:</mo><msub><mi>&#956;</mi><mn>1</mn></msub><mo>-</mo><msub>
	<mi>&#956;</mi><mn>2</mn></msub><mo>=</mo><mo>0</mo></mstyle>
</math>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<msub><mi>H</mi><mn>1</mn></msub><mo>:</mo><msub><mi>&#956;</mi><mn>1</mn>
		</msub><mo>-</mo><msub><mi>&#956;</mi><mn>2</mn></msub><mo>&#8800;</mo>
		<mo>0</mo></mstyle>
</math>
<p>
Suppose we want to know the variance of the difference of the means
between sample from population 1 and the sample from population 2, which
is expressed as <i>s</i><sup>2</sup><sub><span
style="text-decoration:overline;"><i>X</i></span><sub>1</sub> &minus; <span
style="text-decoration:overline;"><i>X</i></span><sub>2</sub></sub>, and
is a reasonable estimator of <i>&sigma;</i><sup>2</sup><sub><span
style="text-decoration:overline;"><i>X</i></span><sub>1</sub> &minus; <span
style="text-decoration:overline;"><i>X</i></span><sub>2</sub></sub>.
This can be shown to be</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true"><mrow><msubsup><mi>&#963;</mi><mrow>
	<msub><mover><mi>X</mi><mo>&#175;</mo></mover><mn>1</mn></msub>
	<mo>-</mo><msub><mover><mi>X</mi><mo>&#175;</mo></mover>
	<mn>2</mn></msub></mrow><mn>2</mn></msubsup></mrow>
	<mo>=</mo><mrow><msubsup><mi>&#963;</mi><mrow><msub><mover><mi>X</mi>
	<mo>&#175;</mo></mover><mn>1</mn></msub></mrow><mn>2</mn></msubsup>
	</mrow><mo>+</mo><mrow><msubsup><mi>&#963;</mi><mrow><msub>
	<mover><mi>X</mi><mo>&#175;</mo></mover><mn>2</mn></msub>
	</mrow><mn>2</mn></msubsup></mrow><mo>=</mo><mfrac><mrow><mrow>
	<msubsup><mi>&#963;</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mrow>
	<mrow><msub><mi>n</mi><mn>1</mn></msub></mrow></mfrac><mo>+</mo>
	<mfrac><mrow><mrow><msubsup><mi>&#963;</mi><mn>2</mn><mn>2</mn>
	</msubsup></mrow></mrow><mrow><msub><mi>n</mi><mn>2</mn></msub>
</mrow></mfrac></mstyle></math>
<p>
Since population variances are assumed to be equal (i.e,
<i>&sigma;</i><sup>2</sup><sub>1</sub> =
<i>&sigma;</i><sup>2</sup><sub>2</sub>), the equation can be expressed as</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true"><mrow><msubsup><mi>&#963;</mi><mrow>
	<msub><mover><mi>X</mi><mo>&#175;</mo></mover><mn>1</mn></msub>
	<mo>-</mo><msub><mover><mi>X</mi><mo>&#175;</mo></mover>
	<mn>2</mn></msub></mrow><mn>2</mn></msubsup></mrow><mo>=</mo>
	<mfrac><mrow><msup><mi>&#963;</mi><mn>2</mn></msup></mrow>
	<mrow><msub><mi>n</mi><mn>1</mn></msub></mrow></mfrac><mo>+</mo>
		<mfrac><mrow><msup><mi>&#963;</mi><mn>2</mn></msup></mrow>
	<mrow><msub><mi>n</mi><mn>2</mn></msub></mrow></mfrac></mstyle>
</math>
<p>
To solve this, a good estimate of <i>&sigma;</i><sup>2</sup> is necessary.
Both <i>s</i><sub>1</sub><sup>2</sup> and <i>s</i><sub>2</sub><sup>2</sup>
are estimates of <i>&sigma;</i><sup>2</sup>, and to combine them a <i>pooled
variance</i> <i>s</i><sub><i>p</i></sub><sup>2</sup> is calculated:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true"><mrow><msubsup><mi>s</mi><mi>p</mi>
	<mn>2</mn></msubsup></mrow><mo>=</mo><mfrac>	<mrow><mi>S</mi><msub>
	<mi>S</mi><mn>1</mn></msub><mo>+</mo><mi>S</mi><msub><mi>S</mi>
	<mn>2</mn></msub></mrow><mrow><msub><mi>&#957;</mi><mn>1</mn></msub>
	<mo>+</mo><msub><mi>&#957;</mi><mn>2</mn></msub></mrow></mfrac></mstyle>
</math>
<p>
where SS<sub>1</sub> and SS<sub>2</sub> are the sum of the squared differences
from the sample of population 1 and of population 2, respectively, and
<i>&nu;</i><sub>1</sub> and <i>&nu;</i><sub>2</sub> are their degrees of freedom.</p>
<p>
Then:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true">
		<mrow><msubsup><mi>s</mi><mrow><msub><mover>	<mi>X</mi><mo>&#175;</mo></mover>
		<mn>1</mn></msub><mo>-</mo><msub><mover><mi>X</mi><mo>&#175;</mo></mover>
		<mn>2</mn></msub></mrow><mn>2</mn></msubsup></mrow><mo>=</mo><mfrac><mrow>
		<mrow><msubsup><mi>s</mi><mi>p</mi><mn>2</mn></msubsup></mrow></mrow><mrow>
		<msub><mi>n</mi><mn>1</mn></msub></mrow></mfrac><mo>+</mo><mfrac><mrow>
		<mrow><msubsup><mi>s</mi><mi>p</mi>	<mn>2</mn></msubsup></mrow></mrow>
	<mrow><msub><mi>n</mi><mn>2</mn></msub></mrow></mfrac></mstyle>
</math>
<p>
For calculating <i>t</i>, obtain <i>&sigma;</i><sub><span
style="text-decoration:overline;"><i>X</i></span><sub>1</sub> &minus; <span
style="text-decoration:overline;"><i>X</i></span><sub>2</sub></sub> as the
square root and then</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
	<mstyle displaystyle="true"><mi>t</mi><mo>=</mo><mfrac><mrow><msub>
	<mover><mi>X</mi>	<mo>&#175;</mo></mover><mn>1</mn></msub><mo>-</mo>
	<msub><mover><mi>X</mi><mo>&#175;</mo></mover><mn>2</mn></msub></mrow>
	<msqrt><mrow><mfrac><mrow><mrow><msubsup><mi>s</mi><mi>p</mi><mn>2</mn>
	</msubsup></mrow>	</mrow><mrow><msub><mi>n</mi><mn>1</mn></msub>
	</mrow></mfrac><mo>+</mo><mfrac><mrow><mrow><msubsup><mi>s</mi><mi>p</mi>
	<mn>2</mn></msubsup></mrow></mrow><mrow><msub><mi>n</mi><mn>2</mn></msub>
	</mrow></mfrac></mrow></msqrt></mfrac></mstyle>
</math>
<p>
For getting the correct <i>t</i> values, the degrees of freedom will be pooled
too:  <i>&nu;</i> = <i>&nu;</i><sub>1</sub> + <i>&nu;</i><sub>2</sub> =
 <i>n</i><sub>1</sub> + <i>n</i><sub>2</sub> &minus; 2.  The null hypothesis
 <i>H</i><sub>0</sub> is rejected if |<i>t</i>| &ge; <i>t</i><sub>&alpha;(2),<i>&nu;</i></sub>
 	in a two-tailed test.</p>

<h2>Multi-Group Single Factor Comparison:  One-Way ANOVA</h2>


<h2>Two Factor ANOVA</h2>


<h2>Comparison of Repeated Observations:  The Before and After T Test</h2>


<h2>Repeated Measures ANOVA</h2>


<h2>MANOVA:  Multi-Variate ANOVA</h2>

<h1>Regression &amp; Correlation</h1>

<h2>Simple Regression and Correlation</h2>
<p>For information on the correlation coefficient, see this
<a href="corrcoeff.html">supporting document</a>.</p>
<h2>Multiple Regression</h2>

<h2>Logistic Regression</h2>

<h2>Principle Components and Factor Analysis</h2>

<h2>Path Analysis and Structural Equation Modeling</h2>


<h1>Nonparametric Statistics</h1>

<h2>Categorical Frequency Data Significance Tests</h2>

<h2>Categorical Data Measures of Association</h2>

<h2>Ranked Data Significance Tests</h2>

<h2>Ranked Dat Measures of Association</h2>

<h2>Survival (Life-Table) Analysis</h2>

</body>
</html>

