<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
        "http://www.w3.org/TR/REC-html40/loose.dtd">
<!-- Take note of the doctype convention...remove 'Transitional' with
     no space before //EN and change 'loose' to 'strict' -->
<HTML>
<HEAD>
<META http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<META name="Author" content="Nobody">
<title>Parametric Statistics</title>
<link type="text/css" href="stats.css" rel="stylesheet">
<style type="text/css">
   .formula, .formula td { border:0;background:none;}
   .formula table td { padding:0;margin:0; vertical-align:middle;}
	.numerator { border-bottom: 1px solid black; }
   .numerator-center { border-bottom: 1px solid black; text-align:center;}
   .formulanum { padding-left : 2em; font-size:115%; }
</style>
</head>

<body>
<P id="title">
The Chi-Square (<i>&Chi;</i><sup>2</sup>) Distribution and Its Uses

<h1>The Chi-Square Distribution</h1>
When data on a quantitatible characteristic (continuous variable)
of a population is collected randomly, with each subject from
which the data is taken (measured) being independent of other
subjects, a plot of the count/number of subjects with the characteristic
versus the scale of the continuous variable (i.e., a histogram) shows
that a curve that is typically normally distributed, or bell-shaped
(Gaussian) in appearance.  The peak of the curve represents the
mean or average of the data set, while the scattering of data is
characterized by the variance, that is, the ratio of the sum of
the square of the difference of the data with the mean and the
degrees of freedom of the data set.  (The square root of the variance
is the standard deviation, a valid descriptor of the scattering of
the data as well.)
<p>
Rather than measure all of the subjects (members) of a population,
it is well understood that a portion or <i>sample</i> of the population
can be measured and its characteristics said to be represenative of the
entire population.  The validity of this sampling becomes strong as
the size of the sampling group itself increases.  With each sampling,
a mean and variance can be calculated.  If the subjects are replaced
back into the population, and several samplings are done with replacment,
a data set of several sample means and their variances is accumulated.

<p>
These too have distributions as the number of sample means and their
variances accumulate.  The distribution of sample means is itself
normally distributed, having a Gaussian or bell shape.  The variance
of this distribution (flatness or sharpness of the peak) actually
depends on the size of each sample, and is the population variance
&sigma;<sup>2</sup> divided by the sample size:
<p align="center">
<table class="formula">
<tr>
 <td rowspan="2">
 <i>&sigma;</i><sub><span style="text-decoration:overline;"><i>x</i></sub></span><sup>2</sup> =
 <td style="border-bottom:1px solid black;">&sigma;</span><sup>2</sup>
 <td rowspan="2" class="formulanum">[1]
<tr>
 <td align="center"><i>n</i>
</table>
<p>
The sample variance <i>s</i><sup>2</sup> is estimated by
<p align="center">
<table class="formula">
<tr>
 <td rowspan="2"><br><br><i>s</i><sup>2</sup> =
 <td>
  <table align="center" style="float:left;margin:0;padding:0;text-align:center;">
   <tr><td style="font-size:75%;"><i>n</i>
   <tr><td>
    <span style="font-size:150%;font-weight:bold;">&Sigma;</span>
   <tr><td style="font-size:75%;"><i>i</i> = 1
  </table>
 <td>
     (<i>x<sub>i</sub></i> &#150;
        <span style="text-decoration:overline;"><i>x</i></span>) <sup>2</sup>
 <td rowspan="2" class="formulanum">[2]
<tr>
 <td style="border-top:1px solid black;" align="center" colspan="2"><i>n</i> &#150; 1
</table>
<p>
Multiply both sides of the equation by degrees of freedom <i>n</i> &#150; 1
and the right side of the equation below gives the expression for the sum
of the squares of the differences in the sample.
<p align="center">
<table class="formula">
<tr>
 <td>(<i>n</i> &#150; 1)<i> s</i><sup>2</sup> =
 <td>
  <table align="center" style="float:left;margin:0;padding:0;text-align:center;">
   <tr><td style="font-size:75%;"><i>n</i>
   <tr><td>
    <span style="font-size:150%;font-weight:bold;">&Sigma;</span>
   <tr><td style="font-size:75%;"><i>i</i> = 1
  </table>
 <td>
     (<i>x<sub>i</sub></i> &#150;
        <span style="text-decoration:overline;"><i>x</i></span>) <sup>2</sup>
</table>
<p>
When this is taken as a fraction of the population variance, a variate
known as the chi-square is such that its histogram shows a set
of distributions that vary with the degrees of freedom <i>n</i> &#150; 1
of the samplings.  These distributions have a mean equal to the
number of degrees of freedom and a variance equal to two times
the degrees of freedom.
<p align="center">
<table class="formula">
<tr>
 <td>
 <td rowspan="2" style="vertical-align:bottom;">
      (<i>n</i> &#150; 1)<i> s</i><sup>2</sup>
 <td>
 <td style="vertical-align:bottom;" rowspan="2">
  <table align="center" style="float:left;margin:0;padding:0;text-align:center;">
   <tr><td style="font-size:75%;"><i>n</i>
   <tr><td>
    <span style="font-size:150%;font-weight:bold;">&Sigma;</span>
   <tr><td style="font-size:75%;"><i>i</i> = 1
  </table>
 <td rowspan="2"><br>
     (<i>x<sub>i</sub></i> &#150;
        <span style="text-decoration:overline;"><i>x</i></span>) <sup>2</sup>
<tr>
  <td><br><br><br>&chi;<sup>2</sup> = <td><br><br><br> =
  <td>
<tr>
 <td>
 <td style="border-top:1px solid black;" align="center">&sigma; <sup>2</sup>
 <td>
 <td align="center" colspan="2" style="border-top:1px solid black;">&sigma; <sup>2</sup>
</table>
<p>
The usefulness of the &chi;<sup>2</sup> distribution, as with most
statistical distributions, is the ability to calculate a probability,
in this case that a sample variance is found for a particular population
(or vice versa) given a certain degrees of freedom (sample size).

<h1>Chi-Square Test of Independence</h1>
Researchers often characterize in multiple ways the subjects
in their population, and each characteristic itself can be classed or
grouped in two or more ways.
<p>
It is particularly important to know whether any two characteristic or
classifications/groupings within characteristics are related to one
another, or rather whether two characteristics or classifications are
<i>independent</i> and not associated.
<p>
Suppose a social scientist wants to see if there is an association
between smoking and aggressive driving.  For both characteristics
he has multiple classifications.  For grouping the smoking classification,
he has nonsmokers, light smokers (less than or equal to half a pack per day),
and heavy smokers (more than half a pack per day).  For his test of
aggressive driving, he decides on a point system where he assigns
2 point for moving violations and 5 points for at-fault accidents (whether
or not there was injury or death), and his classifications are
nonaggressive (0 points over last 10 years), slightly aggressive (less than
equal to 5 points over last 10 years), moderately aggressive (more
than 5 and less than or equal to 10 points over the same time), and
highly aggressive (more than 10 points).  Suppose 1000 licensed
drivers are surveyed, and equal numbers of individuals with respect
to sex and age groups are selected to eliminate these as confounding
variables.  A table is thus constructed:

<table align="center">
<tr>
 <th>
 <th colspan="3">Smoking
<tr>
 <th>Driving Aggressiveness
 <th>Non-Smokers
 <th>Light Smokers
 <th>Heavy Smokers
 <th>Total
<tr>
 <th>Non-Aggressive
 <td>
 <td>
 <td>
 <td> 635
<tr>
 <th>Slightly Aggressive
 <td>
 <td>
 <td>
 <td> 196
<tr>
 <th>Moderately Aggressive
 <td>
 <td>
 <td>
 <td>  78
<tr>
 <th>Heavy Aggressive
 <td>
 <td>
 <td>
 <td> 635
<tr>
 <th>Total
 <td>
 <td>
 <td>
 <td>1000
</table>

<h1>Chi-Square Test of Homogeneity</h1>


The source used for the information presented above:
Wayne W. Daniel, <i>Introductory Statistics with Applications</i>,
Boston: Houghton Mifflin, 1977.
</body>
</HTML>
