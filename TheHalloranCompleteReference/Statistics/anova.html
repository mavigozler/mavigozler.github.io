<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
        "http://www.w3.org/TR/REC-html40/loose.dtd">
<html lang="en">
<HEAD>
<title>Analysis of Variance</title>
<meta http-equiv="content-Type" content="text/html; charset=utf-8">
<meta name="author" content="S. M. Halloran">
<link href="stats.css" rel="stylesheet">
<style>
	body {margin-bottom:3em;}
	.datum {color:blue;font-weight:bold;font-size:125%;}
   .calctable {font-size:90%;}
	.left TD { text-align:left;}
   .simple {background:none;}
   .simple TD {font:normal 90% Arial,Helvetica,sans-serif;background:none;}
   #bartletts TH { font-family:"Times New Roman","Times Roman",Times,serif;
            font-size:110%;}
</style>
</head>

<body>
<p id="title">
Analysis of Variance

<h1>Concepts</h1>

<dl>
<dt>Experiment
  <dd>Any process or activity that yields an outcome or observation
<dt>Independent variable
  <dd>The variable which the experimenter can manipulate
<dt>Dependent variable
  <dd>The variable in an experiment reflecting the effects which
       result from manipulation of the independent variable.
<dt>Nuisance variable
  <dd>Any variable which produces unwanted or undesired effects
      on the response or dependent variable.  They are generally
      controlled by sampling from a class of experimental units
      which keep constant the nuisance variable.
<dt>Treatment
  <dd>Used synonymously with independent variable.
<dt>Experimental unit
  <dd>Smallest entity to which the treatment is applied.  Also called
      the subjects if the unit is an animal or person.
<dt>Measurement
  <dd>The value of the dependent variable, representing the effect
      of a particular treatment in the experiment.
<dt>Experimental error
  <dd>A representation (measurement) of the variation in response
      of the experimental units to a treatment.  Because of the lack
      or uniformity inherent in the subjects and any lack of uniformity
      which may exist in the procedure, this error is possible.
<dt>Replication
  <dd>Reproduction of treatment to the subjects to make possible
      computation of experimental error, improvement of the precision
      of the experiment thereby, and generalization of the experimental
      results.
<dt>Randomization
  <dd>The random assignment of experimental units to different
      treatments.  Randomization may apply to initial selection
      of experimental units or to selection of treatments.
<dt>Power
  <dd>Refers to the power of hypothesis testing, but also to the
      power of any statistical test that is conducted.
<dt>External and internal validity
  <dd>External validity refers to the ability of the results
      to be generalized, which depends upon the proper use of
      randomization and replication.  Internal validity refers
      to results free from bias in the results
</dl>

<h1>Designs</h1>

<h2>The Completely Randomized Experimental Design</h2>

For all treatments in the experiment, all experimental units have an
equal probability of being assigned to the various treatment levels.

<h2>The Randomized Complete Block Design</h2>

If certain nuisance variables can not be ignored and it is necessary
to partition experimental units into homogeneous subgroups for the
purposes of keeping the nuisance variables constant for the treatments,
this design is necessary.  Treatments are then randomly assigned to
units within the subgroups (blocks).  The variable with defines the
subgroup or block is called the blocking variable.  The paired
comparisons test is an example of the randomized complete block
design.

<h2>The Latin Square</h2>

This design derives its name for assignment of subjects to treatments
based on the fact that the levels of extraneous nuisance variables are
equal to the number of levels of treatments.  Suppose treatments A, B,
and C are to used with time-of-day and age of the subjects to be blocked.

<center>
<table style="text-align:center;">
<col span="4" style="padding:0.5em 1em;">
<tr>
   <th>
   <th colspan="3">Age group of subject
<tr>
   <th>Time of day
   <th>young
   <th>middle-aged
   <th>elderly
<tr>
   <td>morning
   <td>A<br>(<i>S</i><sub>1</sub>)
   <td>B<br>(<i>S</i><sub>3</sub>)
   <td>C<br>(<i>S</i><sub>9</sub>)
<tr>
   <td>midday
   <td>C<br>(<i>S</i><sub>5</sub>)
   <td>A<br>(<i>S</i><sub>6</sub>)
   <td>B<br>(<i>S</i><sub>2</sub>)
<tr>
   <td>afternoon
   <td>B<br>(<i>S</i><sub>4</sub>)
   <td>C<br>(<i>S</i><sub>8</sub>)
   <td>A<br>(<i>S</i><sub>7</sub>)
</table>
</center>


<h2>Display of the data</h2>

The table represents the production by employees when promised a
different reward for increased production.  The data should be
displayed in this manner.

<table align="center" style="text-align:center;">
<col style="text-align:left;font-weight:bold;" width="1*">
<col span="5" width="1*" style="padding:0.5em 1em;" align="char" char=".">
<tr>
   <th>
   <th>Money
   <th>Transfer to shift or department of choice
   <th>Time off with pay
   <th>Additional coffee break
   <th>Special recognition
   <th>
<tr><td rowspan="11" style="vertical-align:bottom;">Total
                <td>76   <td>52   <td>37   <td>19   <td>11
<tr>            <td>70   <td>52   <td>26   <td>21   <td>15
<tr>            <td>59   <td>43   <td>28   <td>16   <td>23
<tr>            <td>77   <td>48   <td>38   <td>23   <td>15
<tr>            <td>59   <td>43   <td>25   <td>23   <td>25
<tr>            <td>69   <td>56   <td>30   <td>23   <td>18
<tr>            <td>80   <td>52   <td>28   <td>14   <td>20
<tr>            <td>78   <td>53   <td>25   <td>15   <td>18
<tr>            <td>61   <td>58   <td>26   <td>13   <td>20
<tr>            <td>66   <td>50   <td>25   <td>16   <td>16
<tr>            <td>695  <td>507  <td>288  <td>183  <td>181  <td>1854
<tr><td>Mean    <td>69.5 <td>50.7 <td>28.8 <td>18.3 <td>18.1 <td>37.08
<tr><td>Variance<td>65.17<td>24.23<td>23.73<td>15.79<td>16.99<td>438.65
</table>

<h2>Partitioning of the sum of squares</h2>

In analyzing the total variation using the well-known variance formula,

<p align="center">
<table class="eqnserif">
<tr>
 <td style="padding:0;margin:0;">
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;font-size:90%;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;"><i>k</i><br>
     <span style="font-size:200%">&sum;</span><br><i>j</i> = 1
        <td align="center"
    style="font-size:80%;padding:0;vertical-align:bottom;"><i>n<sub>j</sub></i><br>
        <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
      <td style="padding:0;">(<i>y<sub>ij</sub></i> &minus;
               <i>y</i><sub>..</sub>)<sup>2</sup>
   </table>
<tr><td><i>s</i><sup>2</sup> =
   <td>&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;
<tr><td><td><i>n</i> &minus; 1
</table>

<p>
the variation is found to be distributed between treatments, called the
treatment sum of the squares (SSTR), and within treatments, called the
error sum of squares (SSE).  The total variation is now expressed as:

<p align="center">
<table class="eqnserif">
<tr>
   <td style="font-size:80%;padding:0;vertical-align:bottom;"><i>k</i><br>
      <span style="font-size:200%;">&sum;</span><br>
       <i>j</i> = 1
   <td align="center"
     style="font-size:80%;padding:0;vertical-align:bottom;"><i>n<sub>j</sub></i><br>
      <span style="font-size:200%">&sum;</span><br>
      <i>i</i> = 1
   <td>(<i>y<sub>ij</sub></i> &minus;
               <i>y</i><sub>..</sub>)<sup>2</sup> =
   <td align="center"
     style="font-size:80%;padding:0;vertical-align:bottom;"><i>k</i><br>
         <span style="font-size:200%">&sum;</span><br>
         <i>j</i> = 1
   <td><i>n<sub>j</sub></i>(<i>y<sub>.j</sub></i> &minus;
               <i>y</i><sub>..</sub></i>)<sup>2</sup> +
   <td align="center"
     style="font-size:80%;padding:0;vertical-align:bottom;"><i>k</i><br>
      <span style="font-size:200%">&sum;</span><br>
       <i>j</i> = 1
   <td align="center"
      style="font-size:80%;padding:0;vertical-align:bottom;"><i>n<sub>j</sub></i><br>
      <span style="font-size:200%">&sum;</span><br>
       <i>i</i> = 1
   <td>(<i>y<sub>ij</sub></i> &minus;
               <i>y<sub>.j</sub></i>)<sup>2</sup>
</table>

<p>
or SST = SSTR + SSE.  The following practical equations may be used
to calculate these sums:

<p align="center">
<table class="eqnserif">
<tr><td>SST =
 <td align="center"
   style="font-size:80%;padding:0;vertical-align:bottom;"><i>k</i><br>
      <span style="font-size:200%">&sum;</span><br><i>j</i> = 1
 <td align="center"
    style="font-size:80%;padding:0;vertical-align:bottom;"><i>n<sub>j</sub></i><br>
     <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
 <td><i>y<sub>ij</sub></i><sup>2</sup> &minus; <i>C</i>
</table>

<p>
where

<p align="center">
<table class="eqnserif">
<tr><td>
 <td style="vertical-align:bottom;"><i>T</i><sub>..</sub><sup>2</sup>
 <td style="vertical-align:bottom;padding:0;text-align:right;">
   <span style="font-size:400%;">(</span>
   <table class="eqnserif" style="display:inline;margin-bottom:0;">
    <tr><td align="center"
       style="font-size:80%;padding:0;vertical-align:bottom;"><i>k</i><br>
         <span style="font-size:200%">&sum;</span><br>
         <i>j</i> = 1
        <td align="center"
           style="font-size:80%;vertical-align:bottom;padding:0;">
          <i>n<sub>j</sub></i><br>
         <span style="font-size:200%">&sum;</span><br>
          <i>i</i> = 1
       <td><i>y<sub>ij</sub></i>
   </table>
   <span style="font-size:400%;">)
           <sup style="font-size:33%;position:relative;top:-1em;left:-1em;">2</sup></span>
 <td style="text-align:center;vertical-align:bottom;">(grand total)<sup>2</sup>
<tr>
 <td><i>C</i>
 <td>&nbsp;= &#151;&#151;&#151;&#151;
 <td>&nbsp;=&nbsp;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;
 <td>&nbsp;= &#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;
<tr>
 <td>
 <td style="text-align:center;vertical-align:top;"> <i>n</i>
 <td style="text-align:center;vertical-align:top;"> <i>n</i>
 <td style="text-align:center;vertical-align:top;">total number of observations
</table>
<p>
and
<p align="center">
<table class="eqnserif">
<tr>
 <td><i>n</i> =
 <td>
   <table class="eqnserif">
    <tr><td align="center" style="font-size:80%;"><i>k</i><br>
         <span style="font-size:200%">&sum;</span><br>
         <i>j</i> = 1
        <td align="center" style="font-size:80%;">
          <i>n<sub>j</sub></i><br>
   </table>
</table>

<p>
In addition,

<p align="center">
<table class="eqnserif">
<tr>
 <td rowspan="2">SSTR =
 <td rowspan="2" style="font-size:80%;"><i>k</i><br>
         <span style="font-size:200%">&sum;</span><br>
         <i>j</i> = 1
 <td style="border-bottom:1px solid black;"><i>T<sub>.j</sub></i><sup>2</sup>
 <td rowspan="2">&nbsp;&minus; <i>C</i>
 <td rowspan="2">= (sum of all squared treatment totals divided by
   corresponding group size) &minus; <i>C</i>
<tr><td><i>n<sub>j</sub></i>
</table>

<p>SSE is automatically calculated as the difference between SST and SSTR, namely<br>
SSE = SST &minus; SSTR.  Calculations of SSE can be done directly, but are tedious.
<p>From the example above, the following are computed:

<table class="eqnserif" style="margin-left:30%;margin-top:1em;">
<tr>
 <td rowspan="2"><i>C</i> =
 <td style="border-bottom:1px solid black;"><i>T</i><sub>..</sub><sup>2</sup>
 <td rowspan="2"> =
 <td style="border-bottom:1px solid black;">(1854)<sup>2</sup>
 <td rowspan="2"> = 68746.32
<tr><td><i>n</i>
 <td>50
</table>

<table class="eqnserif" style="margin-left:30%;margin-top:1em;">
<tr><td>SST =
 <td>
  <table class="eqnserif">
    <tr><td style="font-size:80%;">5<br>
         <span style="font-size:200%">&sum;</span><br>
         <i>j</i> = 1
    <td style="font-size:80%;"><i>n<sub>j</sub></i><br>
         <span style="font-size:200%">&sum;</span><br>
         <i>i</i> = 1
    <td><i>y<sub>ij</sub></i><sup>2</sup>
  </table>
<tr><td style="text-align:right;"> =
   <td colspan="3">76<sup>2</sup> + 70<sup>2</sup> + &hellip; +
      16<sup>2</sup> &minus; 68746.32 = 21493.68
</table>

<table class="eqnserif" style="margin-left:30%;margin-top:1em;">
<col style="text-align:right;">
<tr><td rowspan="2">SSTR =
 <td rowspan="2" style="font-size:80%;">5<br>
         <span style="font-size:200%">&sum;</span><br>
         <i>j</i> = 1
 <td style="border-bottom:1px solid black;"><i>T<sub>.j</sub></i><sup>2</sup>
 <td rowspan="2"> &minus; <i>C</i> =
 <td style="border-bottom:1px solid black;">695<sup>2</sup>
 <td rowspan="2"> +
 <td style="border-bottom:1px solid black;">507<sup>2</sup>
 <td rowspan="2"> + &hellip; +
 <td style="border-bottom:1px solid black;">181<sup>2</sup>
 <td rowspan="2"> &minus; 68746.32
<tr><td><i>n<sub>j</sub></i>
  <td>10<td>10<td>10
<tr><td style="text-align:right;">=
   <td style="text-align:left;" colspan="4">20180.48
</table>

<h2>Partitioning of the degrees of freedom</h2>

Dividing the sum of squares by the appropriate degrees of freedom
yields a variance.  If we divide each of these sums of squares by
its appropriate degree of freedom, we shall obtain a variance which
is called the <i>mean square</i> (MS).  The degrees of freedom for
the set are the number of measurements
(<i>n</i>) less 1, or <i>n</i>&nbsp;&minus;&nbsp;1, and this quantity
is called the <i>total degrees of freedom</i>.  Dividing the total
sum of squares by its total degrees of freedom gives the <i>total
mean square</i> (MST) or, SST/(<i>n</i>&nbsp;&minus;&nbsp;1) = MST.

<p>
We also divide the total treatment sum of squares (SSTR) by the
total treatment degrees of freedom (<i>k</i> treatments &minus; 1).
This gives us the <i>treatment mean square</i>, or
SSTR/(<i>k</i>&nbsp;&minus;&nbsp;1) = MSTR.

<p>To obtain the <i>error degrees of freedom</i>, this is the difference
between the total degrees of freedom and the treatment degrees of freedom,
or (<i>n</i>&nbsp;&minus;&nbsp;1)&nbsp;&minus;&nbsp;(<i>k</i>&nbsp;&minus;&nbsp;1)
= <i>n</i>&nbsp;&minus;&nbsp;<i>k</i>.

The <i>error mean square</i> (MSE) is thus calculated:
SSE/(<i>n</i>&nbsp;&minus;&nbsp;<i>k</i>) = MSE.

<h2>The <i>F</i> test</h2>

The purpose of all the calculations to this point is summarized.
ANOVA aims to test the null hypothesis (<i>H</i><sub>0</sub>)
whether all population (treatment) means of several populations
are equal whereas the alternative states that not all means are
equal.

<p class="indent">
  <i>H</i><sub>0</sub>: &mu;<sub>0</sub> = &mu;<sub>1</sub> = ... = &mu;<sub>k</sub><br>
  <i>H</i><sub>1</sub>: Not all &mu;<sub>j</sub> are equal

<p>
The test statistic is an <i>F</i> test, or <i>variance ratio</i> (VR),
VR = MSTR/MSE.
<p>
The use of this statistic requires numerous rigorous assumptions:
<ol>
<li>Each set of <i>n<sub>j</sub></i> measurements is an independent
simple random sample of measurements from a population of measurements,
and each population is identified according to treatment designation.
<li>Each population of measurements within a treatment group must be
normally distributed with a mean within the treatment.
<li>Each population of measurements must have the same variance.
</ol>

There are several ways one can control unequal variances or
rejection of ANOVA as a means of analysis.  One might keep the
sample sizes equal for all treatments, for example.

<p>
Now consider the use of the variance ratio (VR).  If all the assumptions
underlying ANOVA, are met, MSTR and MSE are both independent estimates
of &sigma;<sup>2</sup>, and we expect both estimates to be near
equal to each other when <i>H</i><sub>0</sub> is true.
Now if  <i>H</i><sub>0</sub> is true (all treatment means equal),
and populations are indeed normally distributed with homogeneous
variances and independent error terms, the VR is distributed as
<i>F</i> with &nu;<sub>1</sub> and &nu;<sub>2</sub> degrees of freedom.
The <i>F</i> distribution is used to make inferences specifically
regarding the ratio of variances.
The value for &nu;<sub>1</sub> is the degrees of freedom associated
with the calculation of SSTR or with <i>k</i> treatments.
It is &nu;<sub>1</sub> = <i>k</i>&nbsp;&minus;&nbsp;1.  The value for
&nu;<sub>2</sub> is the degrees of freedom associated with SSE.
It is &nu;<sub>2</sub> = <i>n</i>&nbsp;&minus;&nbsp;<i>k</i>.

<p>
>From the employee-reward example above, and using a level of
significance &alpha; = 0.01, <i>k</i> = 5, <i>n</i> = 50,
we find a critical value for <i>F</i> to be 3.78
(based on interpolation in the table).
MSTR = 5045.12 and MSE = 29.18, if we apply the calculations.
This means VR = 172.90.

<p>
Since 172.90 &gt;&gt; 3.78, <i>H</i><sub>0</sub> is rejected and
we conclude that not all treatments are equal.  Hence different
awards have different effects.  We find that on closer inspection,
that <i>p</i> &lt; 0.005.

<p>
A proper ANOVA table will list the following as headings:
<ol>
<li>sources of variation (being either treatment or error)
<li>sums of the squares of the sources of variation (SS)
<li>degrees of freedom (df)
<li>means of the sum of squares (MS = SS/df)
<li>the variance ratio (VR = MSTR / MSE)
</ol>

Thus we construct the table:

<table>
<caption>SUMMARY</caption>
<tr><th>Groups<th>Count<th>Sum<th>Mean<th>Variance
<tr><td>Money<td>10<td>695<td>69.5<td>65.167
<tr><td>Transfer<td>10<td>507<td>50.7<td>24.233
<tr><td>Time Off<td>10<td>288<td>28.8<td>23.733
<tr><td>Coffee Break<td>10<td>183<td>18.3<td>15.789
<tr><td>Recognition<td>10<td>181<td>18.1<td>16.989
</table>

<table>
<caption>ANOVA</caption>
<tr><th>Source of Variation<th>SS<th>df<th>MS<th>F<th><i>p</i><th>F crit
<tr><td>Between Groups<td>20180.48<td>4<td>5045.12<td>172.883
   <td>1.07&times;10<sup>&minus;26<td>2.579
<tr><td>Within Groups<td>1313.2<td>45<td>29.182
<tr><td>Total<td>21493.68<td>49
</table>


<h2>Testing Normality</h2>
An assumption of ANOVA is that the data is sampled <i>at random</i>
from a normally distributed population, and that the data itself shows this.
<h3>Symmetry and Kurtosis</h3>
The first moment of the mean is &sum;(<i>X<sub>i</sub></i> &minus;
&mu;)/<i>N</i> is zero, since &sum;(<i>X<sub>i</sub></i> &minus; &mu;) = 0.
The second moment of the mean is &sum;(<i>X<sub>i</sub></i> &minus;
&mu;)<sup>2</sup>/<i>N</i>, and is the variance &sigma;<sup>2</sup>.
The third moment of the mean &sum;(<i>X<sub>i</sub></i> &minus;
&mu;)<sup>3</sup>/<i>N</i> is descriptive of the symmetry.  The statistic
<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>k</i><sub>3</sub> =
  <td style="border-bottom:1px solid black;"><i>n</i>
  &sum;(<i>X<sub>i</sub></i> &minus; <span
   class="overbar"><i>X</i></span>)<sup>3</sup>
<tr><td>(<i>n</i> &minus; 1)(<i>n</i> &minus; 2)
</table>
<p>
A computational form for this equation is:
<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>k</i><sub>3</sub> =
  <td style="border-bottom:1px solid black;"><i>n</i>
   &sum;<i>X<sub>i</sub></i><sup>3</sup> &minus; 3 &sum; <i>X<sub>i</sub></i>
   &sum; <i>X<sub>i</sub></i><sup>2</sup> + 2(&sum; <i>X<sub>i</sub></i>)<sup>3</sup>/
  <i>n</i>
<tr><td>(<i>n</i> &minus; 1)(<i>n</i> &minus; 2)
</table>
<p>
A dimensionless value called &gamma;<sub>1</sub> is a population parameter
indicating symmetry that is related to <i>k</i><sub>3</sub> through
its sample estimator <i>g</i><sub>1</sub>:
<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>g</i><sub>1</sub> =
  <td style="border-bottom:1px solid black;"><i>k</i><sub>3</sub>
  <td rowspan="2">=
  <td style="border-bottom:1px solid black;"><i>k</i><sub>3</sub>
<tr><td><i>s</i><sup>2</sup>
  <td>&radic;<span class="overbar">(<i>s</i></span><sup>2</sup>)<sup>3</sup>
</table>
<p>
The fourth moment of the mean is a measure of <i>kurtosis</i>:
<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>k</i><sub>4</sub> =
  <td style="border-bottom:1px solid black;">
  &sum; (<i>X<sub>i</sub></i> &minus; <span class="overbar"><i>X</i></span>)<sup>4</sup>
    <i>n</i>(<i>n</i> + 1)/(<i>n</i> &minus; 1) &minus;
    3[&sum; (<i>X<sub>i</sub></i> &minus;
    <span class="overbar"><i>X</i></span>)<sup>2</sup>]<sup>2</sup>
<tr><td>(<i>n</i> &minus; 2)(<i>n</i> &minus; 3)
</table>
<p>
A computational form for this equation is:
<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>k</i><sub>4</sub> =
  <td style="border-bottom:1px solid black;">
  (<i>n</i><sup>3</sup> + <i>n</i><sup>2</sup>) &sum; <i>X</i> <sup>4</sup>
   &minus; 4(<i>n</i><sup>2</sup> + <i>n</i>) &sum; <i>X</i> <sup>3</sup>
   &sum; <i>X</i> &minus; 3(<i>n</i><sup>2</sup> &minus;
   <i>n</i>)(&sum; <i>X</i> <sup>2</sup>)<sup>2</sup> + 12<i>n</i>
   &sum; <i>X</i> <sup>2</sup> (&sum;<i>X</i>)<sup>2</sup> &minus;
   6(&sum; <i>X</i>)<sup>4</sup>
<tr><td><i>n</i>(<i>n</i> &minus; 1)(<i>n</i> &minus; 2)(<i>n</i> &minus; 3)
</table>
<p>
<i>g</i><sub>2</sub> is an estimator of the population parameter
&gamma;<sub>2</sub>, and represents the dimensionless parameter:
<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>g</i><sub>2</sub> =
  <td style="border-bottom:1px solid black;"><i>k</i><sub>4</sub>
<tr><td><i>s</i><sup>4</sup>
</table>
<p>
and the denominator is the square of the variance
[(<i>s</i><sup>2</sup>)<sup>2</sup>].
<p>
When &gamma;<sub>1</sub> = 0, the distribution is considered to be
symmetrical, whereas a &gamma;<sub>1</sub> &lt; 0 is said to be skewed
left, and &gamma;<sub>1</sub> &gt; 0 is thought skewed right.
When &gamma;<sub>2</sub> is not significantly different from zero, the
curve is considered mesokurtic, and one that is &gt; 0 is leptokurtic
(very high peak), and one &lt; 0 is quite platykurtic (flat peak).
<p>
The parameter &radic;<span class="overbar">&beta;</span><sub>1</sub>
is considered to be identical to &gamma;<sub>1</sub>, and that of
&beta;<sub>2</sub> = &gamma;<sub>2</sub> + 3.  The parameters
&radic;<span class="overbar"><i>b</i></span><sub>1</sub> and
<i>b</i><sub>2</sub> are sample estimators of the population parameters:
<p align="center">
<table class="eqnserif">
<tr><td rowspan="2">&radic;<span class="overbar"><i>b</i></span><sub>1</sub> =
  <td style="border-bottom:1px solid black;">
   (<i>n</i> &minus; 1)<i>g</i><sub>1</sub>
<tr><td>&radic;<span class="overbar"><i>n</i>(<i>n</i> &minus; 1)</span>
</table>
<p align="center">
<table class="eqnserif" >
<tr><td rowspan="2"><i>b</i><sub>2</sub> =
  <td style="border-bottom:1px solid black;">
  (<i>n</i> &minus; 2)(<i>n</i> &minus; 3)<i>g</i><sub>2</sub>
  <td rowspan="2" style="padding:0.5em;">+
  <td style="border-bottom:1px solid black;">
    3(<i>n</i> &minus; 1)
<tr><td>(<i>n</i> + 1)(<i>n</i> &minus; 1)
 <td><i>n</i> + 1
</table>
<p>
For <i>n</i> &lt; 9, a table <i>g</i><sub>1</sub>(&alpha;, <i>n</i>)
provides probabilities for testing <i>H</i><sub>0</sub>:
&radic;<span class="overbar"><i>b</i></span><sub>1</sub> = 0.
Alternatively, the probability can be computed from the standard normal
distribution <i>Z</i> by deriving from the following sequence of operations,
and also computing &radic;<span class="overbar"><i>b</i></span><sub>1</sub>
from <i>g</i><sub>1</sub>:

<p align="center">
<table class="eqnserif" >
<tr><td rowspan="2"><i>A</i> =
   &radic;<span class="overbar"><i>b</i></span><sub>1</sub>
  <span style="font-size:200%;">&radic;</span>
  <td style="border-bottom:1px solid black;border-top:1px solid black;">
  (<i>n</i> + 1)(<i>n</i> + 3)
<tr><td>6(<i>n</i> &minus; 2)
</table>

<table class="eqnserif" >
<tr><td rowspan="2"><i>B</i> =
  <td style="border-bottom:1px solid black;">
   3(<i>n</i><sup>2</sup> + 27<i>n</i> &minus; 70)(<i>n</i> + 1)(<i>n</i> + 3)
<tr><td>(<i>n</i> &minus; 2)(<i>n</i> + 5)(<i>n</i> + 7)(<i>n</i> + 9)
</table>

<p class="eqnserif">
   <i>C</i> = &radic;<span class="overbar">2(<i>B</i> &minus; 1)</span>
     &minus; 1
<p class="eqnserif">
   <i>D</i> = &radic;<span class="overbar"><i>C</i></span>

<table class="eqnserif">
<tr><td rowspan="2"><i>E</i> =
  <td style="border-bottom:1px solid black;">1
<tr><td>&radic;<span class="overbar">ln <i>D</i></span>
</table>

<table class="eqnserif">
<tr><td rowspan="3"><i>F</i> =
  <td colspan="2" style="border-bottom:1px solid black;"><i>A</i>
<tr><td rowspan="2"><span style="font-size:200%;">&radic;</span>
   <td style="border-bottom:1px solid black;border-top:1px solid black;">2
<tr><td><i>C</i> &minus; 1
</table>
<p>
<i>Z</i><sub><i>g</i><sub>1</sub></sub> is then determined:
<p align="center">
 <i>Z</i><sub><i>g</i><sub>1</sub></sub> = <i>E</i> ln (<i>F</i> +
  &radic;<span style="border-top:1px solid black;"><i>F</i><sup>2</sup> + 1</span> )
<p>
Suppose that for a set of <i>n</i> = 70 values that <i>g</i><sub>1</sub>
= &minus;0.3452.  From a table of critical values, one can learn that
<i>P</i>(<i>g</i><sub>1</sub> &ge; 0.3452) is &gt; 0.20.  Since
<i>P</i> &gt; 0.05, <i>H</i><sub>0</sub> is not rejected.  The exact probability
can be learned by computing the sequence:
&radic;<span class="overbar"><i>b</i></span><sub>1</sub> =
&minus;0.337758, <i>A</i> = &minus;1.203833, <i>B</i> = 3.368090,
<i>C</i> = 1.176277, <i>D</i> = 1.084563, <i>E</i> = 3.509806,
<i>F</i> = &minus;0.357395, <i>Z</i><sub><i>g</i><sub>1</sub></sub> =
&minus;1.2291.
<p>
One calculates <i>P</i>(|<i>Z</i>| &ge; 1.23) =
<i>P</i>(<i>Z</i> &le; &minus;1.23) + <i>P</i>(<i>Z</i> &ge; 1.23) =
0.1093 + 0.1093 = 0.2186.  Since the null hypothesis is not rejected,
the data is considered to show symmetry.
<p>
<b>Testing kurtosis</b>.  After calculating <i>g</i><sub>2</sub>
(the sample estimator for parameter &gamma;<sub>2</sub>), one can
test the hypothesis <i>H</i><sub>0</sub>: &gamma;<sub>2</sub> = 0
or equivalently <i>H</i><sub>0</sub>: &beta;<sub>2</sub> = 3.  A table
of critical values is often available.  Alternatively probabilities can
be obtained from the standard normal <i>Z</i> distribution after computing
the following:

<p align="center">
<table class="eqnserif" >
<tr><td rowspan="2"><i>G</i> =
  <td style="border-bottom:1px solid black;">
  24<i>n</i>(<i>n</i> &minus; 2)(<i>n</i> &minus; 3)
<tr><td>(<i>n</i> + 1)<sup>2</sup>(<i>n</i> + 3)(<i>n</i> + 5)
</table>

<table class="eqnserif" >
<tr><td rowspan="2"><i>H</i> =
  <td style="border-bottom:1px solid black;">
   (<i>n</i> &minus; 2)(<i>n</i> &minus; 3)|<i>g</i><sub>2</sub>|
<tr><td>(<i>n</i> + 1)(<i>n</i> &minus; 1)&radic;<span
    class="overbar">G</span>
</table>

<table class="eqnserif" >
<tr><td rowspan="2"><i>J</i> =
  <td style="border-bottom:1px solid black;">
   6(<i>n</i><sup>2</sup> &minus; 5<i>n</i> + 2)
  <td rowspan="2"><span style="font-size:200%;">&radic;</span>
  <td style="border-bottom:1px solid black;border-top:1px solid black;">
   6(<i>n</i> + 3)(<i>n</i> + 5)
<tr><td>(<i>n</i> + 7)(<i>n</i> + 9)
  <td><i>n</i>(<i>n</i> &minus; 2)(<i>n</i> &minus; 3)
</table>

<table class="eqnserif" >
<tr><td rowspan="2"><i>K</i> = 6 +
  <td style="border-bottom:1px solid black;">8
  <td rowspan="2"><span style="font-size:300%;">[</span>
  <td style="border-bottom:1px solid black;">2
  <td rowspan="2">+
  <td rowspan="2"><span style="font-size:250%;">&radic;</span>
  <td rowspan="2">
   <table class="eqnserif" style="border-top:1px solid black;font-size:85%;">
   <tr><td rowspan="2" >1 +
     <td style="border-bottom:1px solid black;">4
   <tr><td><i>J</i> <sup>2</sup>
   </table>
  <td rowspan="2"><span style="font-size:300%;">]</span>
<tr><td><i>J</i>  <td><i>J</i>
</table>

<table class="eqnserif" >
<tr><td rowspan="2"><i>L</i> =
 <td style="border-bottom:1px solid black;">
  <table class="eqnserif" style="font-size:85%;">
  <tr><td rowspan="2">1 &minus;
    <td style="border-bottom:1px solid black;">2
  <tr><td><i>K</i>
  </table>
<tr><td>
  <table class="eqnserif" style="font-size:83%;">
  <tr><td rowspan="2">1 + <i>H</i>
    <span style="font-size:250%;">&radic;</span>
   <td style="border-top:1px solid black;border-bottom:1px solid black;">2
  <tr><td><i>K</i> &minus; 4
  </table>
</table>
<p>
From all this, <i>Z</i><sub><i>g</i><sub>2</sub></sub> is calculated:
<p align="center">
<table class="eqnserif" >
<tr><td rowspan="2"><i>Z</i><sub><i>g</i><sub>2</sub> =
 <td style="border-bottom:1px solid black;">
  <table class="eqnserif" style="font-size:85%;">
  <tr><td rowspan="2">1 &minus;
   <td style="border-bottom:1px solid black;">2
   <td rowspan="2">&minus; <sup>3</sup>&radic;<span class="overbar"><i>L</i></span>
  <tr><td>9<i>K</i>
  </table>
<tr><td>
  <table class="eqnserif" style="font-size:83%;">
  <tr><td rowspan="2"><span style="font-size:250%;">&radic;</span>
   <td style="border-top:1px solid black;border-bottom:1px solid black;">2
  <tr><td>9<i>K</i>
  </table>
</table>

<p>
Suppose that for a set of <i>n</i> = 70 values that <i>g</i><sub>2</sub>
= &minus;0.7183.  From a table of critical values, one can learn that
<i>P</i>(<i>g</i><sub>1</sub> &ge; 0.7183) is &gt; 0.20.  Since
<i>P</i> &gt; 0.05, <i>H</i><sub>0</sub> is not rejected.  The exact probability
can be learned by computing the sequence:
<i>G</i> = 0.277327, <i>H</i> = 1.268487,
<i>J</i> = 1.440994, <i>K</i> = 23.202508, <i>L</i> = 0.648374,
<i>Z</i><sub><i>g</i><sub>2</sub></sub> = 1.2763.
<p>
<i>P</i>(|<i>Z</i>| &ge; 1.28) =
<i>P</i>(<i>Z</i> &le; &minus;1.28) + <i>P</i>(<i>Z</i> &ge; 1.28) =
0.1003 + 0.1003 = 0.2006.  Since the null hypothesis is not rejected,
the data is considered to show mesokurtosis.

<h3>D'Agostino-Pearson Test</h3>
<p>
The null hypothesis of population normality is tested with the
D'Agostino-Pearson <i>K</i><sup>2</sup> statistic:
<p align="center" class="eqnserif" style="font-size:130%;">
        <i>K</i><sup>2</sup> =
<i>Z</i><sub><i>g</i><sub>1</sub></sub><sup style="margin-left:-0.5em;">2</sup> +
<i>Z</i><sub><i>g</i><sub>2</sub></sub><sup style="margin-left:-0.5em;">2</sup>
<p>
One then only needs to obtain a probability from <i>K</i> <sup>2</sup>
using the &chi;<sup>2</sup> distribution using &nu; = 2.


<h2>Homogeneity of Variance</h2>
ANOVA and many other tests that compare differences between means
require or assume equality of group variances.  There are tests
for

<h3>Levene's Test</h3>
The null hypothesis (<i>H</i><sub>0</sub>) is that all variances
of the datasets are equal:  &sigma;<sub>1</sub> =
&sigma;<sub>2</sub> = ... = &sigma;<sub><i>k</i></sub>, and the
alternative hypothesis (<i>H</i><sub>a</sub>) is that for at least one
pair (<i>i,j</i>), the variances are not equal:
&sigma;<sub><i>i</i></sub> &ne; &sigma;<sub><i>j</i></sub>.
<p>
The Levene statistic is:
<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>W</i> =
 <td>(<i>N</i> &minus; <i>k</i>) &sum;<sup><i>k</i></sup><sub
   style="margin-left:-0.5em;"><i>i</i>=1</sub>
  <i>N<sub>i</sub></i> (<i><span class="overbar">Z</span><sub>i</sub></i>
 &minus; <i><span class="overbar">Z</span><sub>..</sub></i>)<sup>2</sup>
<tr><td style="border-top:1px solid black;">
 (<i>k</i> &minus; <i>1</i>) &sum;<sup><i>k</i></sup><sub
   style="margin-left:-0.5em;"><i>i</i>=1</sub>
  &sum;<sup><i>N<sub>i</sub></i></sup><sub
    style="margin-left:-0.5em;"><i>j</i>=1</sub>
  (<i>Z<sub>ij</sub></i> &minus; <i><span
   class="overbar">Z</span><sub>i.</sub></i>)<sup>2</sup>
</table>
<p>
<i>Z<sub>ij</sub></i> can have one of three possible values:
<ol>
<li><i>Z<sub>ij</sub></i> = |<i>Y<sub>ij</sub></i> &minus;
  <span class="overbar"><i>Y</i></span><sub><i>i.</i></sub>|<br>
  where <span class="overbar"><i>Y</i></span><sub><i>i.</i></sub> is the mean
  of the <i>i</i>th subgroup
<li><i>Z<sub>ij</sub></i> = |<i>Y<sub>ij</sub></i> &minus;
  <i>Y</i><sup style="vertical-align:1.3em;margin-left:-0.5em;">~</sup><sub><i>i.</i></sub>|
<br>where <i>Y</i><sup style="vertical-align:1.3em;margin-left:-0.5em;">~</sup><sub><i>i.</i></sub>
  is the median of the <i>i</i>th subgroup
<li><i>Z<sub>ij</sub></i> = |<i>Y<sub>ij</sub></i> &minus;
  <span class="overbar"><i>Y</i></span><sub><i>i.</i></sub>&prime;|
<br>where <span class="overbar"><i>Y</i></span><sub><i>i.</i></sub>&prime;
 is the 10% trimmed mean of the <i>i</i>th subgroup
</ol>
<p>
The hypothesis that the variances are equal is rejected if:
<p align="center" class="eqnserif">
 <i>W</i> > <i>F</i><sub>(&alpha;,<i>k</i>&minus;1,<i>N&minus;k</i>)</sub>
<p>
for significance level &alpha;, <i>N &minus; k</i> denominator degrees
of freedom, and <i>k</i> &minus; 1 numerator degrees of freedom.
<p>
Here's a typical example of how to calculate Levene's, in which the
absolute deviations are typically determined:

<table class="data" style="font-size:90%;">
<tr><th><th colspan="4"><i>x<sub>i</sub></i><th>
  <th colspan="4"><i>z</i> = |<i>x<sub>i</sub></i> &minus;
   <i><span class="overbar">x</span><sub>i</sub></i>)|<th><th>
  <th colspan="4">(<i>z</i> - <span class="overbar">z</span>)<sup>2</sup>
<tr><th><th>#1<th>#2<th>#3<th>#4<th><th> #1  <th>  #2  <th>   #3 <th> #4   <th><th><th>  #1  <th>  #2  <th>  #3  <th>  #4
<tr><td><td> 3<td>18<td>24<td>12<td><td>8.071<td> 2.600<td> 5.909<td> 7.500<td><td><td>25.000<td>49.000<td> 0.138<td> 1.960
<tr><td><td>13<td> 6<td>14<td>30<td><td>1.929<td> 9.400<td> 4.091<td>10.500<td><td><td> 1.306<td> 0.040<td> 2.092<td>19.360
<tr><td><td>13<td>21<td>21<td>27<td><td>1.929<td> 5.600<td> 2.909<td> 7.500<td><td><td> 1.306<td>16.000<td> 6.907<td> 1.960
<tr><td><td> 8<td>34<td> 5<td>20<td><td>3.071<td>18.600<td>13.091<td> 0.500<td><td><td> 0.000<td>81.000<td>57.059<td>31.360
<tr><td><td>11<td>26<td>17<td>17<td><td>0.071<td>10.600<td> 1.091<td> 2.500<td><td><td> 9.000<td> 1.000<td>19.769<td>12.960
<tr><td><td> 9<td>11<td>17<td>23<td><td>2.071<td> 4.400<td> 1.091<td> 3.500<td><td><td> 1.000<td>27.040<td>19.769<td> 6.760
<tr><td><td>12<td> 2<td>23<td>13<td><td>0.929<td>13.400<td> 4.909<td> 6.500<td><td><td> 4.592<td>14.440<td> 0.395<td> 0.160
<tr><td><td> 7<td> 5<td>19<td>28<td><td>4.071<td>10.400<td> 0.909<td> 8.500<td><td><td> 1.000<td> 0.640<td>21.419<td> 5.760
<tr><td><td>16<td> 5<td> 7<td>12<td><td>4.929<td>10.400<td>11.091<td> 7.500<td><td><td> 3.449<td> 0.640<td>30.844<td> 1.960
<tr><td><td>15<td>26<td>27<td>13<td><td>3.929<td>10.600<td> 8.909<td> 6.500<td><td><td> 0.735<td> 1.000<td>11.370<td> 0.160
<tr><td><td>18<td>  <td>25<td>  <td><td>6.929<td>      <td> 6.909<td>      <td><td><td>14.878<td>      <td> 1.882<td>
<tr><td><td>12<td>  <td>  <td>  <td><td>0.929<td colspan="3">              <td><td><td> 4.592<td colspan="3">
<tr><td><td> 8<td>  <td>  <td>  <td><td>3.071<td colspan="3">              <td><td><td> 0.000<td colspan="3">
<tr><td><td>10<td>  <td>  <td>  <td><td>1.071<td colspan="3">              <td><td><td> 4.000<td colspan="3">
<tr style="font-weight:bold;"><td>count
        <td>14<td>10<td>11<td>10<td><td>3.071<td> 9.600<td> 5.537<td> 6.100<td><td>sum<td>70.857<td>190.800<td>171.644<td>82.400
<tr style="font-weight:bold;"><td>mean
   <td>11.071<td>15.400<td>18.091<td>19.500<td colspan="5" style="text-align:right;">grand
   mean<td>5.798<td colspan="5">
<tr style="font-weight:bold;"><td colspan="6" style="text-align:right;"><i>N<sub>i</sub></i>(<span class="overbar"><i>z</i></span><sub><i>i.</i></sub> &minus;
  <span class="overbar"><i>z</i></span><sub>..</sub>)<sup>2</sup>
   <td>104.077<td>144.553<td>0.748<td>0.912<td colspan="7">
<tr style="color:red;font-weight:bold;">
  <td style="color:black;">variance
  <td style="color:black;"> 15.610
  <td style="color:black;">123.600
  <td style="color:black;"> 50.890
  <td style="color:black;"> 50.500
  <td colspan="4" style="text-align:right;">sum<td>250.291
   <td colspan="5" style="text-align:right;">sum<td>515.701
</table>

<table>
<tr><th>       <th>  SS   <th>df<th>  MS  <th><i>F</i><th><i>p</i>
<tr><td>between<td>250.291<td> 3<td>83.430<td> 6.633  <td> 0.0009
<tr><td>within <td>515.701<td>41<td>12.578<td colspan="2">
<tr><td>total  <td>765.992<td>44<td colspan="3">
</table>
<p>
Note that <i>F</i> is the <i>W</i> statistic in the table.  Although
it seems intuitive in the example that the variances are unequal,
this particular test is most objective, and the inequality of variances
is shown significant.

<h3>Bartlett's Test</h3>
This also tests for whether equal variances can be assumed.  However
this test, unlike Levene's test, is more sensitive to examination
of data that is not normally distributed, while Levene's is not so
sensitive.  If Bartlett's test shows unequal variances, it may be
useful to apply a normality test.
<p>
The null and alternative hypotheses are the same as for Levene's test.
The test statistic is calculated as follows:

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>T</i> =
 <td>(<i>N</i> &minus; <i>k</i>) ln <i>s<sub>p</sub></i><sup>2</sup>
  &minus; &sum;<sup><i>k</i></sup><sub
   style="margin-left:-0.5em;"><i>i</i>=1</sub>
  (<i>N<sub>i</sub></i> &minus; 1) ln <i>s<sub>i</sub></i><sup>2</sup>
<tr><td style="border-top:1px solid black;">1 + (1/(3(<i>k</i> &minus; 1)))
 ((&sum;<sup><i>k</i></sup><sub style="margin-left:-0.5em;"><i>i</i>=1</sub>
  1/(<i>N<sub>i</sub></i> &minus; 1)) &minus; 1/(<i>N</i> &minus; <i>k</i>))
</table>
<p>
where <i>s<sub>i</sub></i><sup>2</sup> is the variance of the <i>i</i>th
group, <i>N</i> is the sum of all group sample sizes, <i>N<sub>i</sub></i>
is the size of the <i>i</i>th group, <i>k</i> is the number of groups,
and <i>s<sub>p</sub></i><sup>2</sup> is a pooled variance, which is
a weighted average of the group variances defined as follows:
<p align="center" class="eqnserif">
<table class="eqnserif">
<tr><td rowspan="2"><i>s<sub>p</sub></i><sup>2</sup> =
 <td style="border-bottom:1px solid black;">
  &sum;<sup><i>k</i></sup><sub
   style="margin-left:-0.5em;"><i>i</i>=1</sub>
  (<i>N<sub>i</sub></i> &minus; 1) <i>s<sub>i</sub></i><sup>2</sup>
 <td rowspan="2"> =
  <td style="border-bottom:1px solid black;">&sum; <i>SS<sub>i</sub></i>
<tr><td><i>N</i> &minus; <i>k</i>
 <td>&sum; <i>&nu;<sub>i</sub></i>
</table>
<p>
Note that <i>SS<sub>i</sub></i> is the sum of squared deviations for
each group, and the numerator is the sum of all those, while <i>&nu;<sub>i</sub></i>
is the degrees of freedom for each group, and the denominator is the sum
of those.  Both formulae are equivalent.
<p>
If the condition
<p align="center" class="eqnserif">
   <i>T</i> > &chi;<sup>2</sup><sub>(&alpha;,<i>k</i>&minus;1)</sub>
<p>
then the null hypothesis is rejected and it is concluded that at least one
of the variances does not fit the equality assumption.
<p>
&chi;<sup>2</sup><sub>(&alpha;,<i>k</i>&minus;1)</sub> is the upper critical
point of the chi-square distribution with <i>k</i> &minus; 1 degrees of
freedom.  Note that the &chi;<sup>2</sup><sub>(1&minus;&alpha;)</sub> refers
to the lower critical point of a chi-square distribution.
<p>
The data from Levene's test is used.
<p align="center">
<table class="data" id="bartletts">
<col style="text-align:right;">
<col span="4" style="text-align:center;">
<tr><th>Group<th>#1    <th>#2    <th>#3    <th>#4
<tr><td>     <td>3     <td>18    <td>24    <td>12
<tr><td>     <td>13    <td>6     <td>14    <td>30
<tr><td>     <td>13    <td>21    <td>21    <td>27
<tr><td>     <td>8     <td>34    <td>5     <td>20
<tr><td>     <td>11    <td>26    <td>17    <td>17
<tr><td>     <td>9     <td>11    <td>17    <td>23
<tr><td>     <td>12    <td>2     <td>23    <td>13
<tr><td>     <td>7     <td>5     <td>19    <td>28
<tr><td>     <td>16    <td>5     <td>7     <td>12
<tr><td>     <td>15    <td>26    <td>27    <td>13
<tr><td>     <td>18    <td>      <td>25    <td>
<tr><td>     <td>12    <td>      <td>      <td>
<tr><td>     <td>8     <td>      <td>      <td>
<tr><td>     <td>10    <td>      <td>      <td>
<tr><td colspan="5" style="line-height:0.3em;background:none;">&nbsp;
<tr><th>SS<sub><i>i</i></sub>
             <td>202.9 <td>1112.4<td>508.9<td>454.5
<tr><th>df: <i>&nu;<sub>i</sub></i>
             <td>13    <td>9     <td>10   <td>9
<tr><th>variance: <i>s<sub>i</sub></i><sup>2</sup>
             <td>15.6  <td>123.6 <td>50.9 <td>50.5
<tr><th><i>&nu;<sub>i</sub></i> ln(<i>s<sub>i</sub></i><sup>2</sup>)
             <td>35.72 <td>43.35 <td>39.30<td>35.30
<tr><th>1 / <i>&nu;<sub>i</sub></i>
             <td>0.0769<td>0.1111<td>0.1000<td>0.1111
<tr><th><i>s<sub>p</sub></i><sup>2</sup>
             <td>55.579
<tr><th>ln <i>s<sub>p</sub></i><sup>2</sup> <td>4.018
<tr><th>B  <td>11.059
<tr><th>C  <td>1.0416
<tr><th>T  <td>10.617
<tr><th>&chi;<sup>2</sup><sub>(0.05, 3)</sub>  <td>7.815
</table>
<p>
The following steps can be taken to calculate the <i>T</i> statistic:
<ol>
<li>Compute the variances of each group and calculate the product of their
natural logarithms and the degrees of freedom (the sum of these products
is the weighted sum of the logarithm of the variances).
<li>Compute the weighted average variance (<i>s<sub>p</sub></i><sup>2</sup>)
and also its logarithm.
<li>The difference between the product of the summed degrees of freedom
and the logarithm of the weighted average variance, and the weighted sum
of the logarithm of the variances is calculated (<i>T</i> statistic
numerator).
<li>This is then divided by a &#147;correction factor,&#148; (<i>C</i>)
</ol>
Once the <i>T</i> statistic is calculated, it is compared to the
&chi;<sup>2</sup> critical value for the degrees of freedom (groups compared
less 1) and the level of significance.  In the example, the null hypothesis
that the variances are homogeneous is rejected.

<h3>Hartley's Test</h3>
Hartley suggested a &#147;quick and dirty&#148; test called the
<b><i>F</i><sub>max</sub> test</b>.  Find the smallest variance
(<i>s</i><sup>2</sup><sub style="margin-left:-0.5em;">min</sub>)
and the largest variance
(<i>s</i><sup>2</sup><sub style="margin-left:-0.5em;">max</sub>)
of the set of variances being compared in the datasets.  Compute the ratio
<i>s</i><sup>2</sup><sub
style="margin-left:-0.5em;">min</sub>/<i>s</i><sup>2</sup><sub
style="margin-left:-0.5em;">max</sub>.
A table of values representing the cumulative probability distribution
of <i>F</i><sub>max &alpha; [<i>a</i>, <i>n</i> &minus; 1]</sub> is then
consulted, where &alpha; is the level of significance, <i>a</i> is the
count of all variances of the dataset, and <i>n</i> &minus; 1 is the degrees
of freedom of the groups.  If the degrees of freedom is not the same for
each group, then take the lesser of the degrees of freedom of the variances
<i>s</i><sup>2</sup><sub style="margin-left:-0.5em;">min</sub> or
<i>s</i><sup>2</sup><sub style="margin-left:-0.5em;">max</sub>.

<p align="center">
<table>
<caption>Critical Values for Hartley's Test</caption>
<col style="width:6em;">
<tr><th rowspan="3" style="vertical-align:bottom;">degrees of freedom
   <th colspan="11">Number of compared variances
<tr>      <th>2   <th>3   <th>4   <th>5   <th>6   <th>7   <th>8   <th>9   <th>10  <th>11  <th>12
<tr><td colspan="10" style="background:none;color:blue;text-align:left;">
   <b>Values for &alpha; = 0.05</b>
<tr><th> 5<td>7.15<td>10.8<td>13.7<td>16.3<td>18.7<td>20.8<td>22.9<td>24.7<td>26.5<td>28.2<td>29.9
<tr><th> 6<td>5.82<td>8.38<td>10.4<td>12.1<td>13.7<td>15.0<td>16.3<td>17.5<td>18.6<td>19.7<td>20.7
<tr><th> 7<td>4.99<td>6.94<td>8.44<td>9.70<td>10.8<td>11.8<td>12.7<td>13.5<td>14.3<td>15.1<td>15.8
<tr><th> 8<td>4.43<td>6.00<td>7.18<td>8.12<td>9.03<td>9.78<td>10.5<td>11.1<td>11.7<td>12.2<td>12.7
<tr><th> 9<td>4.03<td>5.34<td>6.31<td>7.11<td>7.80<td>8.41<td>8.95<td>9.45<td>9.91<td>10.3<td>10.7
<tr><th>10<td>3.72<td>4.85<td>5.67<td>6.34<td>6.92<td>7.42<td>7.87<td>8.28<td>8.66<td>9.01<td>9.34
<tr><th>12<td>3.28<td>4.16<td>4.79<td>5.30<td>5.72<td>6.09<td>6.42<td>6.72<td>7.00<td>7.25<td>7.48
<tr><th>15<td>2.86<td>3.54<td>4.01<td>4.37<td>4.68<td>4.95<td>5.19<td>5.40<td>5.59<td>5.77<td>5.93
<tr><th>20<td>2.46<td>2.95<td>3.29<td>3.54<td>3.76<td>3.94<td>4.10<td>4.24<td>4.37<td>4.49<td>4.59
<tr><th>30<td>2.07<td>2.40<td>2.61<td>2.78<td>2.91<td>3.02<td>3.12<td>3.21<td>3.29<td>3.36<td>3.39
<tr><th>60<td>1.67<td>1.85<td>1.96<td>2.04<td>2.11<td>2.17<td>2.22<td>2.26<td>2.30<td>2.33<td>2.36
<tr><th> <td colspan="10" style="background:none;color:blue;text-align:left;">
   <b>Values for &alpha; = 0.01</b>
<tr><th> 5<td>14.9<td>22  <td>28  <td>33  <td>38  <td>42  <td>46  <td>50  <td>54  <td>57  <td>60
<tr><th> 6<td>11.1<td>15.5<td>19.1<td>22  <td>25  <td>27  <td>30  <td>32  <td>34  <td>36  <td>37
<tr><th> 7<td>8.89<td>12.1<td>14.5<td>16.5<td>18.4<td>20  <td>22  <td>23  <td>24  <td>26  <td>27
<tr><th> 8<td>7.50<td> 9.9<td>11.7<td>13.2<td>14.5<td>15.8<td>16.9<td>17.9<td>18.9<td>19.8<td>21
<tr><th> 9<td>6.54<td> 8.5<td> 9.9<td>11.1<td>12.1<td>13.1<td>13.9<td>14.7<td>15.3<td>16.0<td>16.6
<tr><th>10<td>5.85<td> 7.4<td> 8.6<td> 9.6<td>10.4<td>11.1<td>11.8<td>12.4<td>12.9<td>13.4<td>13.9
<tr><th>12<td>4.91<td> 6.1<td> 6.9<td> 7.6<td> 8.2<td> 8.7<td> 9.1<td> 9.5<td> 9.9<td>10.2<td>10.6
<tr><th>15<td>4.07<td> 4.9<td> 5.5<td> 6.0<td> 6.4<td> 6.7<td> 7.1<td> 7.3<td> 7.5<td> 7.8<td> 8.0
<tr><th>20<td>3.32<td> 3.8<td> 4.3<td> 4.6<td> 4.9<td> 5.1<td> 5.3<td> 5.5<td> 5.6<td> 5.8<td> 5.9
<tr><th>30<td>2.63<td> 3.0<td> 3.3<td> 3.4<td> 3.6<td> 3.7<td> 3.8<td> 3.9<td> 4.0<td> 4.1<td> 4.2
<tr><th>60<td>1.96<td> 2.2<td> 2.3<td> 2.4<td> 2.4<td> 2.5<td> 2.5<td> 2.6<td> 2.6<td> 2.7<td> 2.7
</table>
<p>

Taking the example in the section discussing Bartlett's Test,
<i>s</i><sup>2</sup><sub style="margin-left:-0.5em;">min</sub> = 15.6 and
<i>s</i><sup>2</sup><sub style="margin-left:-0.5em;">max</sub> = 123.6, and
thus ratio is 123.6 / 15.6 = 7.92.
The degrees of freedom were not equal in all groups.  The df for the
<i>min</i> group is 13 and for the <i>max</i> group is 9, so 9 will be
used for the <i>F</i><sub>max</sub> test.  There are 4 groups.
Thus, at &alpha; = 0.05, <i>F</i><sub>max 0.05 [4, 9]</sub> = 6.31.
Since <i>s</i><sup>2</sup><sub
style="margin-left:-0.5em;">min</sub>/<i>s</i><sup>2</sup><sub
style="margin-left:-0.5em;">max</sub> &gt; <i>F</i><sub>max 0.05 [4, 9]</sub>,
the variances are not homogeneous.

<h3>The <i>log-anova</i> or Scheff&eacute;-Box Test</h3>
Within each of the groups (samples) of the dataset, the data are organized
into subgroups (subsamples) randomly.  The variance of each of the subgroups
is then calculated, then transformed into its natural logarithm.  ANOVA
is then done on these values:  the variance between subgroups is compared to
variance within the subgroups for significance.
<p>
The number of subsamples in each group <i>k<sub>i</sub></i> should approximate
&radic;<i><span class="overbar">n</span><sub>i</sub></i>,
the square root of the count of data in each group.
<p>
The data from the Levene's/Bartlett's examples is used.
<p align="center">
<table class="data" id="bartletts">
<col style="text-align:right;">
<col span="4" style="text-align:center;">
<tr><th>Group (<i>g</i> = 4)<th>#1    <th>#2    <th>#3    <th>#4
<tr><td>            <td>7     <td>26    <td>23    <td>17
<tr><td>            <td>12    <td>5     <td>14    <td>28
<tr><td>            <td>18    <td>21    <td>27    <td>20
<tr><td>            <td>10    <td>6     <td>      <td>12
<tr><td>df          <td>3     <td>3     <td>2     <td>3
<tr><td>ln <i>s</i> <sup>2</sup>
                    <td>3.072 <td>4.721 <td>3.792 <td>3.805
<tr>
<tr><td>            <td>8     <td>5     <td>17    <td>27
<tr><td>            <td>3     <td>34    <td>7     <td>13
<tr><td>            <td>15    <td>2     <td>25    <td>23
<tr><td>            <td>13    <td>      <td>5     <td>
<tr><td>            <td>11    <td>      <td>      <td>
<tr><td>df          <td>4     <td>2     <td>3     <td>2
<tr><td>ln <i>s</i> <sup>2</sup>
                    <td>3.091 <td>5.744 <td>4.458 <td>3.951
<tr>
<tr><td>            <td>16    <td>11    <td>19    <td>30
<tr><td>            <td>8     <td>26    <td>17    <td>12
<tr><td>            <td>9     <td>18    <td>24    <td>13
<tr><td>            <td>13    <td>      <td>21    <td>
<tr><td>            <td>12    <td>      <td>      <td>
<tr><td>df          <td>4     <td>2     <td>3     <td>2
<tr><td>ln <i>s</i> <sup>2</sup>
                    <td>2.332 <td>4.031 <td>2.188 <td>4.628
<tr><td colspan="5" style="background-color:silver;line-height:0.5em;">&nbsp;
<tr><td><i>n<sub>i</sub></i>
                    <td>14    <td>10    <td>11    <td>10
<tr><td><i>k<sub>i</sub></i>
                    <td>3     <td>3     <td>3     <td>3
</table>

<p align="center">
<table class="eqnserif" style="font-size:90%;">
<tr><td rowspan="2"><i>MS</i><sub>between</sub> =
 <td style="border-bottom:1px solid black;">&sum;<sup
 style="vertical-align:1.5em;margin-left:-1em;"><i>g</i></sup>
 (<i>n<sub>i</sub></i> &minus; <i>k<sub>i</sub></i>)
 (<i><span class="overbar">Z</span><sub>i</sub></i> &minus; <i>M</i>)<sup>2</sup>
<tr><td><i>g</i> &minus; 1
</table>

<table class="eqnserif" style="font-size:90%;">
<tr><td rowspan="2"><i>MS</i><sub>within</sub> =
 <td style="border-bottom:1px solid black;">&sum;<sup
 style="vertical-align:1.5em;margin-left:-1em;"><i>g</i></sup>
 &sum;<sup style="vertical-align:1.5em;margin-left:-1em;"><i>n<sub>i</sub></i></sup>
 <i>&nu;<sub>ij</sub></i> (<i>Z<sub>ij</sub></i> &minus;
    <i><span class="overbar">Z</span><sub>i</sub></i>)<sup>2</sup>
<tr><td>&sum;<sup style="vertical-align:1.5em;margin-left:-1em;"><i>g</i></sup>
 (<i>k<sub>i</sub></i> &minus; 1)
</table>
<p>
where <i>Z<sub>ij</sub></i> = ln <i>s</i><sup>2</sup> of every subsample
in the corresponding group and

<p align="center">
<table class="eqnserif" style="font-size:90%;">
<tr><td rowspan="2"><i><span class="overbar">Z</span><sub>i</sub></i> =
 <td style="border-bottom:1px solid black;">&sum;<sup
 style="vertical-align:1.5em;margin-left:-1em;"><i>k<sub>i</sub></i></sup>
  <i>&nu;<sub>ij</sub>Z<sub>ij</sub>
 <td rowspan="2" style="width:5em;">
 <td rowspan="2"><i>M</i> =
 <td style="border-bottom:1px solid black;">&sum;<sup
 style="vertical-align:1.5em;margin-left:-1em;"><i>g</i></sup>
 &sum;<sup style="vertical-align:1.5em;margin-left:-1em;"><i>k<sub>i</sub></i></sup>
 <i>&nu;<sub>ij</sub> Z<sub>ij</sub></i>
<tr><td>&sum;<sup style="vertical-align:1.5em;margin-left:-1em;"><i>k<sub>i</sub></i></sup>
  <i>&nu;<sub>ij
 <td>&sum;<sup style="vertical-align:1.5em;margin-left:-1em;"><i>g</i></sup>
 &sum;<sup style="vertical-align:1.5em;margin-left:-1em;"><i>k<sub>i</sub></i></sup>
 <i>&nu;<sub>ij</sub></i>
</table>
<p>
For the example,
<p align="center">
<table class="eqnserif" style="font-size:90%;">
<tr><td rowspan="2"><i>M</i>&nbsp;=
 <td style="border-bottom:1px solid black;">(3)(3.072) +
  (4)(3.091) + (4)(2.332) + (3)(4.721) + (2)(5.744) + (2)(4.031) +
  (2)(3.792) + (3)(4.458) + (3)(2.188) + (3)(3.805) + (2)(3.951) + (2)(4.628)
  <td rowspan="2">=&nbsp;3.658
<tr><td>3 + 4 + 4 + 3 + 2 + 2 + 2 + 3 + 3 + 3 + 2 + 2
</table>

<p align="center">
<table class="eqnserif" style="font-size:90%;">
<tr><td rowspan="2"><i><span class="overbar">Z</span></i><sub>1</sub>&nbsp;=
 <td style="border-bottom:1px solid black;">(3)(3.072) +
  (4)(3.091) + (4)(2.332)
  <td rowspan="2" style="width:5em;">=&nbsp;2.810
  <td rowspan="2"><i><span class="overbar">Z</span></i><sub>2</sub>&nbsp;=
  <td style="border-bottom:1px solid black;">(3)(4.721) +
  (2)(5.744) + (2)(4.031)
  <td rowspan="2">=&nbsp;4.816
<tr><td>3 + 4 + 4  <td>3 + 2 + 2

<tr><td rowspan="2"><i><span class="overbar">Z</span></i><sub>3</sub>&nbsp;=
 <td style="border-bottom:1px solid black;">(2)(3.792) + (3)(4.458) + (3)(2.188)
  <td rowspan="2" style="width:5em;">=&nbsp;3.440
  <td rowspan="2"><i><span class="overbar">Z</span></i><sub>4</sub>&nbsp;=
  <td style="border-bottom:1px solid black;">(3)(3.805) + (2)(3.951) + (2)(4.628)
  <td rowspan="2">=&nbsp;4.082
<tr><td>2 + 3 + 3  <td>3 + 2 + 2
</table>

<p align="center">
<table class="eqnserif" style="font-size:90%;">
<tr><td rowspan="2"><i>MS<sub>between</sub>&nbsp;=
 <td style="border-bottom:1px solid black;">
   (14&nbsp;&minus;&nbsp;3)(2.809&nbsp;&minus;&nbsp;3.658)<sup>2</sup> +
   (10&nbsp;&minus;&nbsp;3)(4.816&nbsp;&minus;&nbsp;3.658)<sup>2</sup> +
   (14&nbsp;&minus;&nbsp;3)(3.440&nbsp;&minus;&nbsp;3.658)<sup>2</sup> +
   (14&nbsp;&minus;&nbsp;3)(4.082&nbsp;&minus;&nbsp;3.658)<sup>2</sup>
 <td rowspan="2">=&nbsp;6.315
<tr><td>4 &minus; 1
</table>

<p align="center">
<table class="eqnserif" style="font-size:90%;">
<tr><td rowspan="2"><i>MS<sub>within</sub>&nbsp;=
 <td style="border-bottom:1px solid black;">
   (3)(3.072&nbsp;&minus;&nbsp;2.810)<sup>2</sup> +
   (4)(3.091&nbsp;&minus;&nbsp;2.810)<sup>2</sup> +
   (4)(2.332&nbsp;&minus;&nbsp;2.810)<sup>2</sup> +
   (3)(4.721&nbsp;&minus;&nbsp;4.816)<sup>2</sup> +
   (2)(5.744&nbsp;&minus;&nbsp;4.816)<sup>2</sup> +
   (2)(4.031&nbsp;&minus;&nbsp;4.816)<sup>2</sup> +
   (2)(3.792&nbsp;&minus;&nbsp;3.440)<sup>2</sup> +
   (3)(4.458&nbsp;&minus;&nbsp;3.440)<sup>2</sup> +
   (3)(2.188&nbsp;&minus;&nbsp;3.440)<sup>2</sup> +
   (3)(3.805&nbsp;&minus;&nbsp;4.082)<sup>2</sup> +
   (2)(3.951&nbsp;&minus;&nbsp;4.082)<sup>2</sup> +
   (2)(4.628&nbsp;&minus;&nbsp;4.082)<sup>2</sup>
 <td rowspan="2">=&nbsp;1.667
<tr><td>(3 &minus; 1) + (3 &minus; 1) + (3 &minus; 1) + (3 &minus; 1)
</table>

The <i>F<sub>s</sub></i> ratio = 6.315 / 1.667 = 3.788.  The critical
value <i>F</i><sub>0.05[3,]</sub> =

<h2>Comparison among treatment means (A Priori and Post-Hoc Testing)</h2>

Having determined that not all treatment means are equal (i.e.,
rejecting <i>H</i><sub>0</sub>), it is of interest to assess
where differences occur.  It is natural to make <i>pairwise</i>
comparisons among the groups to see for these differences
(what accounts for the rejection).  Generally hypothesis testing
regarding two means can be done, or confidence intervals constructed.
In some cases, the experimenter may make comparisons among certain
pairs of groups regardless of whether VR was significant;
these are called <i>planned</i> or <i>a priori</i> comparisons.
<p>
On the other hand, if VR was found significant, the experimenter
may attempt to ascertain which differences accounted for the
significance;  since he is unlikely to be interested if
<i>H</i><sub>0</sub> was accepted and only interested when
<i>H</i><sub>0</sub> is rejected, these comparisons are called
<i>a posteriori</i> or <i>post hoc</i> comparisons.

<h3>A Priori Testing</h3>
This testing makes use of a modified <i>t</i> test, and is permissible
if there are only 1 or 2 comparisons.  Moreover, the use of the
<i>t</i> test should have been planned before and should not be
an after-thought.
<p>
In doing the <i>t</i>, instead of using the group variances, use
the mean square error (MSE) term calculated from the ANOVA table, which is
the sum of the squares of the within-group error divided by the
degrees of freedom (SSE/df).  This is a better estimate of the
variance.  However, this can only be used when the homogeneity of
variance of the groups is assumed.
<p>
The formula is written thusly:
<p align="center">
<table align="center" class="eqnserif">
<tr><td rowspan="2" style="vertical-align:top;"><br><i>t</i> =
 <td style="border-bottom:1px solid black;">
   <span class="overbar"><i>x</i></span><sub>1</sub> &minus;
     <span class="overbar"><i>x</i></span><sub>2</sub>
<tr>
 <td>
  <table align="center" class="eqnserif" style="font-size:80%;">
  <tr><td rowspan="2"><span style="font-size:200%;">&radic;</span>
   <td style="border-bottom:1px solid black;"><span
         style="text-decoration:overline;">2<i>MSE</i></span>
  <tr><td><i>n</i>
  </table>
</table>
<p>
Note that this requires that groups be of equal size to apply this.
<p>
When the assumption of homogeneity of variance can not be made,
the individual variances must be use to evaluate a pooled estimate
of the error.
<p align="center">
<table align="center" class="eqnserif">
<tr><td rowspan="2" style="vertical-align:top;"><br><i>t</i> =
 <td style="border-bottom:1px solid black;">
   <span class="overbar"><i>x</i></span><sub>1</sub> &minus;
     <span class="overbar"><i>x</i></span><sub>2</sub>
<tr>
 <td>
  <table align="center" class="eqnserif" style="font-size:80%;">
  <tr><td rowspan="2" style="font-size:200%;">&radic;
   <td style="border-bottom:1px solid black;border-top:1px solid black;">
     <i>s</i><sub>1</sub><sup>2</sup> + <i>s</i><sub>2</sub><sup>2</sup>
  <tr><td><i>n</i>
  </table>
</table>
<p>
Again the groups compared must be of equal size.
<p>
When groups are of unequal size and it cannot be the case that variances
are homogeneous (equal), then the individual variances are again employed
and one corrects for the degrees of freedom using the Welch-Satterhtwaite
approach, or one can use the Box procedure, testing a range of the
degrees of freedom from <i>n</i><sub>L</sub> &minus; 1 to
<i>n</i><sub>L</sub> + <i>n</i><sub>B</sub> &minus; 2.

<h4>Linear Contrasts</h4>
Although this test can be used as a post-hoc test, it is more common
to anticipate what groups (their means) are expected to be different,
and to plan for this expectation <i>a priori</i>.
This test assumes that data in each treatment group is normally distributed,
that variances are equal, and that sampling is random and data are
independent.  Samples sizes should also be identical for each of the
<i>g</i> populations.  Hence <i>n</i><sub>1</sub> = <i>n</i><sub>2</sub> =
... = <i>n<sub>g</sub></i> = <i>m</i>.
<p>
There is a value <i>c</i> which represents a <i>contrast of means</i>
and can be defined as:

<table class="eqnserif" align="center">
<tr>
 <td><i>c</i> =
 <td style="font-size:80%;"><i>g</i><br>
   <span style="font-size:200%">&sum;</span><br>
      <i>i</i> = 1
 <td><i>a<sub>i</sub><span style="text-decoration:overline;">x</span><sub>i</sub></i>
</table>

where <i>a<sub>i</sub></i> is a constant representing contrast coefficient
or weight such that

<p align="center">
<table class="eqnserif" style="display:inline;" align="center">
<tr>
 <td style="font-size:80%;"><i>g</i><br>
   <span style="font-size:200%">&sum;</span><br>
      <i>i</i> = 1
 <td><i>a<sub>i</sub></i> = 0.
</table>
<p>

Note that the contrast value is a weighted sum of the treatment means.
Differences between means can be represented as contrasts such that <span
style="text-decoration:overline;"><i>x</i></span><sub>1</sub> &minus; <span
style="text-decoration:overline;"><i>x</i></span><sub>2</sub>, with
<i>a</i><sub>1</sub> = 1, <i>a</i><sub>2</sub> = &minus;1,
<i>a</i><sub>3</sub> = <i>a</i><sub>4</sub> = ... = <i>a<sub>g</sub></i> = 0.
<p>
Other contrasts can take different forms:  <span
style="text-decoration:overline;"><i>x</i></span><sub>1</sub> + <span
style="text-decoration:overline;"><i>x</i></span><sub>2</sub> &minus; 2<span
style="text-decoration:overline;"><i>x</i></span><sub>3</sub> or
<span style="text-decoration:overline;"><i>x</i></span><sub>1</sub> + <span
style="text-decoration:overline;"><i>x</i></span><sub>2</sub> &minus; <span
style="text-decoration:overline;"><i>x</i></span><sub>3</sub> &minus; <span
style="text-decoration:overline;"><i>x</i></span><sub>4</sub>.
<p>
The important thing to do is to make sure that any contrast chosen has
its coefficients summing to zero.  The means or groups of means should
weighted reasonably to compare each other.
<p>
When the sizes of all groups are equal, the sum of squares is
evaluated:

<p align="center">
<table class="eqnserif" align="center">
<tr>
 <td rowspan="2"><i>SS</i> =
 <td style="border-bottom:1px solid black;"><i>n c</i><sup>2</sup>
<tr>
 <td>&sum; <i>a<sub>i</sub><sup>2</sup>
</table>
<p>
When sample sizes are unequal, the sum of squares must be evaluated
to account for different sizes:
<p align="center">
<table align="center" class="eqnserif">
<tr><td rowspan="2" style="vertical-align:top;"><br><i>SS</i> =
 <td style="border-bottom:1px solid black;"> <i>c</i><sup>2</sup>
<tr>
 <td>
  <table align="center" class="eqnserif" style="font-size:80%;">
  <tr><td rowspan="2" style="font-size:200%;">&sum;
   <td style="border-bottom:1px solid black;"><i>a<sub>i</sub></i><sup>2</sup>
  <tr><td><i>n<sub>i</sub></i>
  </table>
</table>
<p>
The work incentive example will serve well.
<span class="overbar"><i>x</i></span><sub>money</sub> had the greatest
mean at 69.5, followed by
<span class="overbar"><i>x</i></span><sub>transfer</sub> = 50.7, and
<span class="overbar"><i>x</i></span><sub>time off</sub> = 28.8.
The means <span class="overbar"><i>x</i></span><sub>coffee break</sub>
= 18.3 and <span class="overbar"><i>x</i></span><sub>recognition</sub>
= 18.1 are both the smallest and are the least different from one
another.
<p>
Setting up possible contrasts is a matter of thinking logically
about what means are expected to be significantly different, either
before or after the calculation of the means.  The only limitation
the sum of the coefficients must add up to zero.
<p>
The table below shows sets of arbitrarily assigned coefficients,
some that make sense and others less so.

<p align="center">
<table class="data">
<col span="10" style="width:5em;">
<col span="3" style="width:3.5em;">
<tr><th colspan="2">Money<th colspan="2">Transfer
  <th colspan="2">Time Off<th colspan="2">Coffee Break
  <th colspan="2">Recognition
  <th colspan="3">
<tr><th><i>a</i><sub>1</sub><th><i>a</i><sub>1</sub>(69.5)
    <th><i>a</i><sub>2</sub><th><i>a</i><sub>2</sub>(50.7)
    <th><i>a</i><sub>3</sub><th><i>a</i><sub>3</sub>(28.8)
    <th><i>a</i><sub>4</sub><th><i>a</i><sub>4</sub>(18.3)
    <th><i>a</i><sub>5</sub><th><i>a</i><sub>5</sub>(18.1)
    <th><i>c</i><th>&sum;<i>a<sub>i</sub></i><sup>2</sup><th>SS
<tr><td>3          <td>208.5
    <td>2          <td>101.4
    <td>0          <td>0
    <td>&minus;2.5 <td>&minus;45.75
    <td>&minus;2.5 <td>&minus;45.25
    <td>
</table>

<p>
Now note that the sum of squares of the contrasts equal the between-group
sum of squares:
<p align="center" class="eqnserif">
   <i>SS</i><sub>between</sub> = <i>SS</i><sub>c1</sub> + <i>SS</i><sub>c2</sub>
<p align="center" class="eqnserif">
    11.67 = 10.42 + 1.25
<p>
The variance that is attributable to contrast is:
<table align="center" class="eqnserif" style="display:inline;">
<tr><td style="border-bottom:1px solid black;"><i>SS</i><sub>ci</sub>
<tr><td><i>SS</i><sub>total</sub>
</table>
<p>
The mean square of the contrast (<i>MS</i><sub>cont</sub>) is the
<i>SS<sub>ci</sub></i> divided by df.  All linear contrasts have df = 1
however.  The <i>F</i> ratio with 1 df in the numerator and the error df
in the denominator:
<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>F</i><sub>(1, df<sub>error</sub>)</sub> =
 <td style="border-bottom:1px solid black;"> <i>MS</i><sub>cont</sub>
<tr>
 <td><i>MS</i><sub>error</sub>
</table>

<h4>Orthogonal Contrasts</h4>

Contrasts that are independent of one another are called <b>orthogonal</b>.
<i>SS</i><sub>total</sub> will represent the sum of squares of the complete
set of orthogonal contrasts.
<p>
Thus multiple contrasts are represented:

<p align="center">
<table class="eqnserif">
<tr>
 <td><i>c<sub>j</sub></i> =
 <td style="font-size:80%;"><i>g</i><br>
   <span style="font-size:200%">&sum;</span><br>
      <i>i</i> = 1
 <td><i>a<sub>ji</sub><span style="text-decoration:overline;">x</span><sub>i</sub></i>
</table>
<p>
and any two contrasts

<p align="center">
<table class="eqnserif">
 <tr><td>
  <table class="eqnserif" style="font-size:80%;">
   <tr>
   <td><i>c</i><sub>1</sub> =
   <td style="font-size:80%;"><i>g</i><br>
     <span style="font-size:200%">&sum;</span><br>
       <i>i</i> = 1
   <td><i>a</i><sub>1<i>i</i></sub><i><span style="text-decoration:overline;">x</span><sub>i</sub></i>
  </table>
  <td style="padding:1em;">and
  <td>
  <table class="eqnserif" style="font-size:80%;">
   <tr>
   <td><i>c</i><sub>2</sub> =
   <td style="font-size:80%;"><i>g</i><br>
     <span style="font-size:200%">&sum;</span><br>
       <i>i</i> = 1
   <td><i>a</i><sub>2<i>i</i></sub><i><span style="text-decoration:overline;">x</span><sub>i</sub></i>
  </table>
</table>
<p>
are <i><b>orthogonal</b></i> if

<table class="eqnserif" style="display:inline;vertical-align:middle;margin:2em;">
<tr>
 <td style="font-size:80%;"><i>g</i><br>
   <span style="font-size:200%">&sum;</span><br>
      <i>i</i> = 1
 <td><i>a</i><sub>1<i>i</i></sub> = 0
</table>

and

<table class="eqnserif" style="display:inline;vertical-align:middle;margin:2em;">
<tr>
 <td style="font-size:80%;"><i>g</i><br>
   <span style="font-size:200%">&sum;</span><br>
      <i>i</i> = 1
 <td><i>a</i><sub>2<i>i</i></sub> = 0.
</table>
<p>
and also that

<table class="eqnserif" style="display:inline;" align="center">
<tr>
 <td style="font-size:80%;"><i>g</i><br>
   <span style="font-size:200%">&sum;</span><br>
      <i>i</i> = 1
 <td><i>a</i><sub>1<i>i</i></sub><i>a</i><sub>2<i>i</i></sub> = 0.
</table>

<p>
The contrasts <span
style="text-decoration:overline;"><i>x</i></span><sub>1</sub> &minus; <span
style="text-decoration:overline;"><i>x</i></span><sub>2</sub> and <span
style="text-decoration:overline;"><i>x</i></span><sub>1</sub> + <span
style="text-decoration:overline;"><i>x</i></span><sub>2</sub> &minus; 2<span
style="text-decoration:overline;"><i>x</i></span><sub>3</sub> are
orthogonal because all these conditions are true.
<p>
The parameter <i>SS</i><sub>between</sub> will be the total sum of
squares of all contrasts.  The number of comparisons will represent
the degrees of freedom for the between-groups contrasts.
<p>
In developing contrasts, do it in a tree-like fashion.  Suppose
6 groups.  Compare groups 1 and 2 against 3, 4, and 5.  Then compare
1 against 2.  Then compare group 3 against 4 and 5.  Finally compare
4 against 5.
<p>
There are only 1 of many possibilities.  Compute an <i>F</i>.
Check to ensure that:
<p align="center" class="eqnserif">
<span style="font-size:150%;">&sigma;</span>
 <i>SS</i><sub>contrasts</sub> = <i>SS</i><sub>between</sub>

<h3>Post Hoc Methods</h3>
Post hoc analysis involves a variety of procedures falling into
to different classes.  Sequential vs. simultaneous, weighted vs.
unweighted, and step-up vs. step-down.  The modified Bonferroni
procedures used in ANOVA are designed to perform other statistical
evaluations such as correlations and chi-square tests.
<p>
Heterogeneity of variances, i.e., unequal variances in compared
datasets, can complicate the evaluation, particularly when the
sample sizes are small.  Levene's homogeneity test can be used as
a guide but is less reliable with smaller sample sizes, and thus
Type I errors are more likely.


<h4>Linear Trend Analysis</h4>
If the differences between the means of the effects of each
treatment group and the treatment itself show a linear relationship,
then the best analysis is a linear regression.  The treatment itself
must be a variable on a continuous scale (a numeric value).
<p>
To determine if there are significant differences in effects
between treatment groups, determine the confidence interval limits
for the predicted effect (<i>y<sub>pred</sub></i> on the regression
line) at each treatment value (<i>x<sub>i</sub></i>), or do a <i>t</i>
test to compute the probability that two points on the regression line
differ.

<h4>Studentized Range and Studentized Maximum Modulus</h4>
A set of values {<i>x<sub>i</sub></i>} is independent and normally
distributed with mean &mu; and standard deviation &sigma;.
<i>s<sub>m</sub></i> is an estimate of &sigma; with <i>m</i> degrees
of freedom, is independent of {<i>x<sub>i</sub></i>}, and with
<i>ms<sub>m</sub></i><sup>2</sup> / &sigma;<sup>2</sup> ~
&chi;<sup>2</sup>.

<p align="center">
<table class="eqnserif">
<tr>
 <td rowspan="2"><i>S<sub>r,m</sub></i> =
 <td style="border-bottom:1px solid black;">
   max(<i>x</i><sub>1</sub>, ..., <i>x<sub>r</sub></i>) &minus;
   min(<i>x</i><sub>1</sub>, ..., <i>x<sub>r</sub></i>)
<tr>
 <td><i>s<sub>m</sub></i>
</table>
<p>
is the <b>Studentized range</b>.  The critical point (upper &epsilon;)
in this distribution is <i>S<sub>&epsilon;,r,m</sub></i>.   The quantity

<p align="center">
<table class="eqnserif">
<tr>
 <td rowspan="2"><i>M<sub>r,m</sub></i> =
 <td style="border-bottom:1px solid black;">
   max(|<i>x</i><sub>1</sub>|, ..., |<i>x<sub>r</sub></i>|)
<tr>
 <td><i>s<sub>m</sub></i>
</table>
<p>
is the <b>Studentized maximum modulus</b>, and its upper critical
point is also <i>M<sub>&epsilon;,r,m</sub></i>

<h4>Familywise Error and the Bonferroni Correction (Dunn's Test)</h4>
This error represents the probability of a Type I error produced
from a series of a sequence of performed comparisons.
With the greater number of comparisons, there is a chance that they
Type I error increases.  The quantity (1&nbsp;&minus;&nbsp;&alpha;) represents the
chance of <em>not</em> making a Type I error.  If <i>k</i> possible
comparisons will be made, the probability of avoiding Type I error
completely is thus (1&nbsp;&minus;&nbsp;&alpha;)<sup><i>k</i></sup>.  Of course,
the probability of making a Type I error is (1&nbsp;&minus;&nbsp;<i>the probability
of not making an error</i>), or:
<p align="center" class="eqnserif">
&alpha;<sub>FW</sub> &le; 1 &minus; (1 &minus;
&alpha;<sub>PC</sub>)<sup><i>k</i></sup>
<p>
Here &alpha;<sub>FW</sub> is called the <i>familywise error rate</i>,
also known as <b>alpha inflation</b> or <b>cumulative Type I error</b>.
&alpha;<sub>PC</sub> is the rate for the individual test
(<i>per comparison</i>).  Suppose <i>k</i> = 3 comparisons to be made
for three groups, and the normal standard &alpha; = 0.05 applies
to the per-comparison (&alpha;<sub>PC</sub>).  This means that the chance
of committing Type I error when all comparisons are made increases to
&alpha;<sub>FW</sub> = 0.143.
<p>
On the other hand, to keep &alpha;<sub>FW</sub> = 0.05 for all comparisons
to be made, the equation is solved for &alpha;<sub>PC</sub> to determine
and make adjustments for the per-comparison test level:
<p align="center" class="eqnserif">
&alpha;<sub>PC</sub> = 1 &minus; exp[ ln(1&minus;&alpha;<sub>FW</sub>)/<i>k</i> ]
<p>
For <i>k</i> = 3 comparisons, that means &alpha;<sub>PC</sub> = 0.0170.
This means determining a critical value <i>t</i> for &alpha;=0.017
and the degrees of freedom.  A good approximation actually ends up being
the original error divided by the number of comparisons:  &alpha;/<i>k</i>
= 0.05/3 = 0.0167.
<p>
The problem is, critical values are
not determined for interpolated values of &alpha;.  What is generally done
instead is to calculate the probability from the <i>F</i> or <i>t</i>,
and then compare it to &alpha;.
<p>
The test of Bonferroni generates very wide confidence intervals and
is said to have a low power in detecting differences.  Nonetheless
it is widely used because its approach is to compare the data as a whole
rather than allowing for bias in deciding what means are interesting to
compare.  It is like Tukey&#146;s and Student-Neuman-Keuls in this
regard, but it is a better method when the number of treatment groups
compared is small.  Prefer Tukey&#146;s or S-N-K when there are five
or more treatment groups.

<h4>Sidak Test</h4>
Like Bonferroni, this assumes that all possible mean pairs are
compared and thus corrects for alpha as follows:

<p align="center" class="eqnserif">
   &alpha;&prime; = 1 &minus; (1 &minus; &alpha;)<sup>1/<i>m</i></sup>
<p>
where <i>m</i> is the number of comparisons to be made.  As an
example, for three comparisons at &alpha; = 0.05, &alpha;&prime;
becomes 0.01695.  Compare this to Bonferroni which is
&alpha;&prime; = 0.05/3 = 0.01667.  Sidak has slightly more power
than Bonferroni at &alpha; = 0.05, but is identical at &alpha; = 0.01


<h4>Hochberg&#146;s Sequential Test</h4>

<h4>Keppel&#146;s Modified Bonferroni</h4>
This corrects the &alpha; level for the number of groups in the ANOVA
as well as the number of comparisons to be made.

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2">&alpha;&prime; =
 <td style="border-bottom:1px solid black;">&alpha; &times; df
<tr>
 <td><i>c</i>
</table>
<p>
where &alpha;&prime; is the new level of significance to be used
in the comparisons, &alpha; is the usual significance level (0.05),
<i>c</i> is the number of mean comparisons to be made, and <b>df</b>
is the degrees of freedom between groups in the ANOVA table.
<p>
Note that this modification increases the value of the probability
which is determined for significance.  The use of a single &alpha;
level for all comparisons however is subject to the criticism that
it does not have a balance to avoid Type I and Type II errors.

<h4>Tukey&#146;s HSD test</h4>

Tukey&#146;s HSD test is often called the <i>HSD</i>
(<i>honestly signficant difference</i>) test or <i>w procedure</i>.
This is a procedure for all possible pairwise comparisons between means
and is widely used; indeed the test assumes all possible pairwise
comparisons will be made and its power relies on this.  Tukey&#146;s
method can be more conservative than
other post hoc tests, and so it will often show no difference between
means where other post hoc tests will show a difference.  Tukey&#146;s
should be used when the number of treatment groups being compared is
large (5 or more).
<p>
A single value (HSD) against which all differences are compared is
calculated:

<p align="center">
<table align="center" class="eqnserif" style="font-size:110%">
<tr><td rowspan="2">HSD = <i>q</i><sub>&alpha;,<i>k</i>,<i>n&minus;k</i></sub>
  <td rowspan="2" style="font-size:200%;">&radic;
  <td style="border-bottom:1px solid black;text-decoration:overline;">MSE
<tr>
 <td><i>n<sub>j</sub></i>
</table>

<p>
The value <i>q</i> is taken from a table called the percentage
points for the Studentized range.  Its value depends on the level
of significance &alpha;, the number of treatments, and the error
degrees of freedom.  Any difference between a pair of means in
the data which exceeds HSD is significant at level &alpha;.
<p>
Tukey&#146;s assumes that the sample sizes of all treatment groups are
equal.  When dealing with unequal sample sizes, the
<a href="#tukeykramer">Kramer modification</a>
of the Tukey test is used.

<p>
Consider the employee reward example.  The level of significance
is &alpha; = 0.05.  A table is constructed showing the means, computed
differences in comparisons, and whether the difference is significant.
<p align="center">
<table style="width:30em;">
<caption>Multiple Comparison by Tukey's HSD</caption>
<col span="2" width="33%">
<tr><th colspan="2">Comparison <th rowspan="2">Mean Difference
   <th rowspan="2">Significant?
<tr><th>Group 1<th>Group 2
<tr><td colspan="4" style="color:green;">
 <i>q</i><sub>0.05,5,45</sub> = 4.02 (interpolated from tables)<br>
 HSD = (4.02)(&radic;<span class="overbar">29.18/10</span>) = 6.87
<tr><td rowspan="4">Money vs.<td>Transfer<td>69.5&minus;50.7=18.8<td>yes
<tr><td>Time Off<td>69.5&minus;28.8=40.7<td>yes
<tr><td>Coffee Break<td>69.5&minus;18.3=51.2<td>yes
<tr><td>Recognition<td>69.5&minus;18.1=51.4<td>yes
<tr><td rowspan="3">Transfer vs.<td>Time Off<td>50.7&minus;28.8=21.9<td>yes
<tr><td>Coffee Break<td>50.7&minus;18.3=32.4<td>yes
<tr><td>Recognition<td>50.7&minus;18.1=32.6<td>yes
<tr><td rowspan="2">Time Off vs.<td>Coffee Break<td>28.8&minus;18.3=10.5<td>yes
<tr><td>Recognition<td>28.8&minus;18.1=10.7<td>yes
<tr><td>Coffee Break vs.<td>Recognition<td>18.3&minus;18.1=0.2<td>no
</table>
<p>
Based on comparison to the HSD value, <em>all</em> but one of the pairwise
comparisons of the treatment means (that is, all differences) are
significantly different.
<p>
Tukey&#146;s value can be used to construct confidence
intervals for the differences in means:

<p class="indent">
(mean of treatment 1 &minus; mean of treatment 2) &plusmn; HSD

<p>
where the confidence interval 100(1 &minus; &alpha;)% applies to
the particular <i>q</i> value recorded in the table and used to
calculate HSD.  Looking at the employee-rewards example again,
it is seen that the confidence interval between the reward being
given time off with pay vs. the reward for an additional coffee
break is:

<p class="indent">
      (28.8 &minus; 18.3) &plusmn; 6.87  = 10.5 &plusmn; 6.87

<p>
It is only possible to be 95% confident that the true difference
in the two treatment means is somewhere between 3.63 and 17.37.

<h4><a name="tukeykramer">Tukey-Kramer</a></h4>
While the Tukey&#146;s test essentially demands equal samples sizes,
the Tukey-Kramer test allows for different sizes, although
variance homogeneity is assumed.

<p align="center">
<table class="eqnserif">
<tr><td><td><td colspan="6" style="border-bottom:1px solid black;">&nbsp;
<tr><td rowspan="2">HSD = <i>q</i><sub>&alpha;,<i>k</i>,<i>n&minus;k</i></sub>
  <td rowspan="2" style="font-size:200%;">&radic;
  <td style="border-bottom:1px solid black;">MSE
  <td rowspan="2" style="font-size:175%;">(
  <td style="border-bottom:1px solid black;">1
  <td rowspan="2">+
  <td style="border-bottom:1px solid black;">1
  <td rowspan="2" style="font-size:175%;">)
<tr>
 <td>2
 <td><i>n</i><sub>1</sub>
 <td><i>n</i><sub>2</sub>
</table>
<p>
The table below shows bone mineral density (BMD) data obtained from the
trochanter portion of the femur of pediatric healthy (Control) and
epileptic patients taking either one of two anti-convulsant
(anti-seizure) drugs:  carbamezepine (CMZ) or valproate.

<p align="center">
<table style="font:normal 83% Helvetica,Arial,sans-serif;text-align:center;">
<col><col span="4" style="width:5em;">
<tr><th><th>CMZ   <th>Valproate   <th colspan="2">Control
<tr><td rowspan="17">
    <td> 0.5209 <td> 0.5220 <td> 0.5700 <td> 0.6050
<tr><td> 0.5280 <td> 0.5040 <td> 0.5590 <td> 0.5340
<tr><td> 0.5224 <td> 0.5377 <td> 0.6450 <td> 0.5850
<tr><td> 0.5249 <td> 0.6308 <td> 0.4700 <td> 0.5900
<tr><td> 0.6022 <td> 0.4726 <td> 0.5030 <td> 0.7370
<tr><td> 0.6303 <td> 0.6362 <td> 0.5340 <td> 0.6460
<tr><td> 0.4770 <td> 0.6192 <td> 0.5270 <td> 0.7870
<tr><td> 0.5141 <td> 0.6247 <td> 0.6080 <td> 0.6970
<tr><td> 0.5659 <td> 0.5280 <td> 0.5990 <td> 0.7000
<tr><td> 0.6229 <td> 0.5020 <td> 0.5810 <td> 0.7650
<tr><td> 0.5054 <td> 0.5828 <td> 0.5570 <td> 0.7990
<tr><td> 0.5971 <td> 0.4330 <td> 0.5820 <td> 0.7560
<tr><td> 0.6378 <td> 0.6153 <td> 0.6300 <td> 0.6530
<tr><td> 0.6883 <td> 0.5659 <td> 0.7140 <td> 0.5680
<tr><td> 0.6796 <td> 0.6150 <td> 0.6260 <td>
<tr><td> 0.6055 <td> 0.6419 <td> 0.7380 <td>
<tr><td> 0.7043 <td>        <td> 0.8880 <td>
<tr><td><b><i>count</i></b>
       <td><b>17</b> <td><b>16</b> <td colspan="2"><b>31</b>
<tr style="background-color:#fff8f8;">
    <td><b><i>mean</i></b>
       <td><b>0.5839</b><td><b>0.5644</b> <td colspan="2"><b>0.6372</b>
</table>
<p>
The ANOVA table appears as follows:

<table style="font:normal 100% Helvetica,Arial,sans-serif;">
<tr><th>Source of<br>Variation<th>SS<th>df <th>MS    <th><i>F</i><th><i>p</i>
<tr><td>Treatments <td>0.066000     <td>2  <td>0.033000 <td>4.618<td>0.0136
<tr><td>Error      <td>0.435863     <td>61 <td>0.007145
<tr><td>Total      <td>0.501864     <td>63
</table>

The multiple comparison table looks as below, with <sub>3</sub><i>C</i><sub>2</sub>
= 3 comparisons.  The comparisons are done with the means ordered
from largest to smallest in Group 1, smallest to largest in Group 2.
<p align="center">
<table style="font:normal 100% Helvetica,Arial,sans-serif;">
<caption>Multiple Comparison by Tukey-Kramer</caption>
<col span="2" style="width:8em;">
<col style="width:14em;">
<col style="width:8em;">
<col style="width:7em;text-align:center;">
<tr><th colspan="2">Comparison <th rowspan="2">Mean Difference
   <th rowspan="2">Difference for Significance
   <th rowspan="2">Significant?
<tr><th>Group 1<th>Group 2
<tr><td colspan="5" style="color:green;">
 <i>q</i><sub>0.05,61,3</sub> = 3.399 (interpolated from tables)<br>
 <table class="eqnserif">
 <tr><td rowspan="2">
   Difference for Significance = <span style="font-size:200%;">&radic;</span>
  <td style="border-bottom:1px solid black;border-top:1px solid green;">2
    &times; MS<sub>error</sub>
  <tr><td>(1 / <i>n</i><sub>1</sub> + 1 / <i>n</i><sub>2</sub>)
 </table>
<tr><td rowspan="2" style="text-align:right;">Control  vs.<td>Valproate
   <td style="text-align:center;">0.6372 &minus; 0.5644 = 0.0727 <td>0.0625<td>yes
<tr><td>CMZ<td style="text-align:center;">0.6372 &minus; 0.5839 = 0.0533
   <td>0.0613<td>no
<tr><td style="text-align:right;">CMZ vs.<td>Control
   <td style="text-align:center;">0.5839 &minus; 0.5644 = 0.01947 <td>0.0708<td>no
</table>


<h4>Scheffe&#146s Method</h4>
The Scheffe test should <em>not</em> be used unless all means are compared.
<ol>
<li>Set up a two-column table that shows all possible comparisons
for differences of the mean, <i>x</i><sub>1</sub> &minus;
<i>x</i><sub>2</sub>.
There number of two-group comparisons when there are <i>k</i> groups
will, of course, be <i><sub>k</sub>C</i><sub>2</sub>, which simplifies
to <i>k</i>(</i>k</i> &minus; 1)/2.
Hence if there are 3 groups, there will be 3(3&minus;1)/2 = 3 comparisons.
If there are four groups there will be 4(4&minus;1)/2 = 6 comparisons.
<li>For each mean difference, compute the confidence inteval for the
difference:
<p align="center">
<table class="eqnserif">
<tr><td rowspan="2">
   <span class="overbar"><i>x</i></span><sub>1</sub> &minus;
    <span class="overbar"><i>x</i></span><sub>2</sub> &plusmn;
   &radic;<span style="border-top:1px solid black;">(<i>k</i> &minus; 1)
   <i>F</i><sub><i>k</i>&minus;1,<i>n&minus;k</i></sub>
    (<i>MS</i><sub>error</sub>)</span>
  <span style="font-size:200%;">&radic;</span>
 <td style="border-top:1px solid black;">1
 <td style="border-top:1px solid black;" rowspan="2">+
 <td style="border-top:1px solid black;">1
<tr><td style="border-top:1px solid black;"><i>n</i><sub>1</sub>
  <td style="border-top:1px solid black;"><i>n</i><sub>2</sub>
</table>
</ol>
<p>
The test for significance is whether the confidence interval spans to
include zero.  Calculate the low and high limits of the interval.  If
the low limit is &le;0 <em>and</em> the high limit is &ge;0, then
the mean differences are not significant.
<p>
Taking the data from the work incentive example above, we construct
5(5&minus;1)/2 = 10 comparisons of the mean differences, and we compute
their confidence intervals, shown in the table below.

<table class="data" style="font:normal 90% Helvetica,Arial,sans-serif;">
<caption>Scheffe Test Group Comparisons</caption>
<col span="2">
<tr style="text-align:center;">
  <th colspan="3"><th colspan="3">Confidence Interval<th>
<tr>
  <th>Group 1 <th>Group 2        <th>Group Means Difference<th>error<span style="color:red;">*</span>
                                                                        <th>low <th>high <th>Significant?
<tr><td>Money   <td>Transfer       <td>69.5 &minus; 50.7 = 18.8 <td>7.759<td>11.0<td>26.56<td>yes
<tr><td>Money   <td>Time Off       <td>69.5 &minus; 28.8 = 40.7 <td>7.759<td>32.9<td>48.46<td>yes
<tr><td>Money   <td>Coffee break   <td>69.5 &minus; 18.3 = 51.2 <td>7.759<td>43.4<td>58.96<td>yes
<tr><td>Money   <td>Recognition    <td>69.5 &minus; 18.1 = 51.4 <td>7.759<td>43.6<td>59.16<td>yes
<tr><td>Transfer<td>Time Off       <td>50.7 &minus; 28.8 = 21.9 <td>7.759<td>14.1<td>29.66<td>yes
<tr><td>Transfer<td>Coffee Break   <td>50.7 &minus; 18.3 = 32.4 <td>7.759<td>24.6<td>40.16<td>yes
<tr><td>Transfer<td>Recognition    <td>50.7 &minus; 18.1 = 32.6 <td>7.759<td>24.8<td>40.36<td>yes
<tr><td>Time Off<td>Coffee Break   <td>28.8 &minus; 18.3 = 10.5 <td>7.759<td>2.74<td>18.26<td>yes
<tr><td>Time Off<td>Recognition    <td>28.8 &minus; 18.1 = 10.7 <td>7.759<td>2.94<td>18.46<td>yes
<tr><td>Coffee Break<td>Recognition<td>18.3 &minus; 18.1 = 0.2  <td>7.759<td>&minus;7.56<td>7.96<td>no
<tr><th colspan="7" style="text-align:left;font-size:75%;font-weight:normal;">
  <span style="color:red;">*</span>Because the sample sizes were equal,
   the error is the same for all groups
</table>

<!--
This method actually alters the critical value for the <i>F</i> test
used in ANOVA:
<p align="center" class="eqnserif">
<i>F</i>&prime;<sub>crit</sub> = (<i>c</i> &minus; 1) <i>F</i><sub>crit</sub>
<p>
<i>F</i><sub>crit</sub> is the critical <i>F</i> at the usual significance
level &alpha;, and <i>c</i> is the number of comparisons to be made
(not the number of groups necessarily).
<p>
A potential problem with this test is that it can be too severe,
greatly increasing the Type II error.

Scheffe&#146s Method is another way of contrasting means.  A drawback
is that it is fairly conservative, generating quite wide confidence
interval limits and therefore not detecting differences that other
methods would show.-->


<h4>Student-Newman-Keuls</h4>
This method is very much like Tukey&#146;s.  The Student-Newman-Keuls
method, also called the &#147;SNK test,&#148; gives identical results
to Tukey&#146;s when comparing the largest mean to the smallest.
With other comparisons however,  Tukey&#146;s HSD test is more
conservative and will show no difference,
while SNK will show a difference.  A major criticism is that SNK
shows differences which arguably do not exist.
<p>
Sample means are ranked and differences are determined.  The value
<i>q</i> is calculated as for Tukey&#146;s, however the critical value
is determined differently:  it is <i>q</i><sub>&alpha;,&nu;,<i>p</i></sub>
where <i>p</i> is the count of means in the range of means being
tested.  That is, if the first ranked mean is compared to the fifth
ranked mean, there are five means in the range, and thus <i>p</i> = 5.
When the second ranked mean is compared to the first ranked, there
are only two means in the range, and thus <i>p</i> = 2.  Note that
different critical values for each pairwise comparison is called a
<i>multiple range</i> test.

<p>
The employee reward example is evaluated using SNK:

<table class="data" style="margin-bottom:0;">
<caption>Student-Newman-Keuls Test of Ranked Means</caption>
<col><col span="5" style="width:6em;">
<tr><th>Rank <td>1    <td>2    <td>3    <td>4    <td>5
<tr><th>Group<td>Recognition<td>Coffee Break<td>Time Off<td>Transfer<td>Money
<tr><th>Mean <td>18.1 <td>18.3 <td>28.8 <td>50.7 <td>69.5
</table>

<table class="data" style="margin-top:0;">
<tr><td colspan="5" style="background:none;">&nbsp;
<tr style="text-align:center;">
  <th>Group Comparison <th>Difference <th><i>q</i> <th><i>p</i>
  <th><i>q</i><sub>0.05,50,<i>p</i></sub>  <th>Significant?
<tr><td>5 vs 1 <td>51.4
<tr><td>5 vs 2 <td>51.2
<tr><td>5 vs 3 <td>40.7
<tr><td>5 vs 4 <td>18.8
<tr><td>4 vs 1 <td>32.6
<tr><td>4 vs 2 <td>32.4
<tr><td>4 vs 3 <td>21.9
<tr><td>3 vs 1 <td>10.7
<tr><td>3 vs 2 <td>10.5
<tr><td>2 vs 1 <td>0.2
</table>



<h4>Dunnett&#146s Test</h4>
Dunnett&#146s test provides more power in comparing treated groups
against a single control group, as opposed to comparing treated
groups with each other.

<h4>Multiple Welch Test</h4>

<p align="center">
<table class="eqnserif">
<tr><td rowspan="3"><i>F</i>&prime; =
 <td colspan="2" style="border-bottom:1px solid black;">
	&sum;<sup style="vertical-align:1.5em;margin-left:-0.5em;"><i>k</i></sup><sub
   style="vertical-align:-1.5em;margin-left:-1em;"><i>i</i>=1</sub>
   <i>c<sub>i</sub></i>(<i><span class="overbar">X</span><sub>i</sub>
   &minus; <i><span class="overbar">X</span><sub>w</sub>)<sup>2</sup>
<tr><td rowspan="2">(<i>k</i> &minus; 1)<span style="font-size:200%;">[</span>
   1 +
 <td style="border-bottom:1px solid black;">2<i>A</i>(<i>k</i> &minus; 2)
 <td rowspan="2"><span style="font-size:200%;">]</span>
<tr><td><i>k</i><sup>2</sup> &minus; 1
</table>
<p>
where
<p align="center">
<table class="eqnserif" style="display:inline;vertical-align:bottom;">
<tr><td rowspan="2"><i>c<sub>i</sub></i> =
 <td style="border-bottom:1px solid black;"><i>n<sub>i</sub></i>
<tr><td><i>s<sub>i</sub></i><sup>2</sup>
</table>

<span class="eqnserif" style="margin-left:2em;">
 <i>C</i> = &sum;<sup style="vertical-align:1.5em;margin-left:-0.5em;"><i>k</i></sup><sub
   style="vertical-align:-1.5em;margin-left:-1em;"><i>i</i>=1</sub>
   <i>c<sub>i</sub></i></span>

<table class="eqnserif" style="display:inline;vertical-align:bottom;margin-left:3em;">
 <td rowspan="2"><i><span class="overbar">X</span><sub>w</sub></i> =
 <td style="border-bottom:1px solid black;">
   &sum;<sup style="vertical-align:1.5em;margin-left:-0.5em;"><i>k</i></sup><sub
   style="vertical-align:-1.5em;margin-left:-1em;"><i>i</i>=1</sub>
   <i>c<sub>i</sub><span class="overbar">X</span><sub>i</sub></i>
<tr><td><i>C</i>
</table>

<table class="eqnserif" style="display:inline;vertical-align:bottom;margin-left:3em;">
 <td rowspan="2"><i>A</i> = &sum;<sup style="vertical-align:1.5em;margin-left:-0.5em;"><i>k</i></sup><sub
   style="vertical-align:-1.5em;margin-left:-1em;"><i>i</i>=1</sub>
  <td style="border-bottom:1px solid black;">
   (1 &minus; <i>c<sub>i</sub>/C</i>)<sup>2</sup>
<tr><td>&nu;<sub><i>i</i></sub>
</table>
<p>
where &nu;<sub>i</sub> = <i>n<sub>i</sub></i> &minus; 1, and <i>F</i>&prime;
has degrees of freedom &nu;<sub>1</sub> = <i>k</i> &minus; 1 and
&nu;<sub>2</sub> = (<i>k</i><sup>2</sup> &minus; 1)/3<i>A</i> (round
to next lowest integer).

<h4>Games-Howell</h4>
An assumption of ANOVA is homogeneity of variance (equal variances)
among the compared datasets.  Severely unequal variances lead to
a very high chance of Type I error.   Type I error is also more likely
when sample sizes are small and there are moderate differences in the
variances between groups.  Games-Howell makes use of a Studentized
range statistic and is based on Welch&#146;s correction to <i>t</i>
and <b>df</b>.

<h4>Tamhane&#146;s T2</h4>
Used only when assumptions about equal variances cannot be made
and particularly when group sizes are also unequal.  An elaborate
set of steps first uses the Welch procedure for determining
degrees of freedom to be used in the standard error, using
Student's <i>t</i> to do this.  Sidak is used to determine the
&alpha; level.

<h4>Waller-Duncan <i>t</i> Test</h4>
This <i>t</i> test statistic is given as:
<p align="center" class="eqnserif">
<i>v<sub>i,j</sub></i> = |<i><span class="overbar">x</span><sub>i</sub></i>
&minus; <i><span class="overbar">x</span><sub>j</sub></i>| &ge;
<i>t</i><sub>B</sub>(<i>w,F,q,f</i>)<i>S</i>&radic;<span
class="overbar">2 / <i>n</i></span>
<p>
where <i>t</i><sub>B</sub>(<i>w,F,q,f</i>) is a Bayesian <i>t</i> value
dependent upon four parameters.  <i>w</i> is a measure of the relative
severity of a Type I error vs. a Type II error.  <i>F</i> is the statistic
from one-way ANOVA (<i>MS</i><sub>between</sub> / <i>MS</i><sub>within</sub>,
or <i>MS</i><sub>treatment</sub> / <i>MS</i><sub>error</sub>).  The
parameter <i>q</i> is the degrees of freedom reflecting the number of
compared datasets in the ANOVA, or <i>k</i> &minus; 1, and <i>f</i>
is the degrees of freedom for all data values in all datasets, or
<i>k</i>(<i>n</i> &minus; 1).  The value <i>S</i> is the square
root of the mean square error (<i>MS</i><sub>error</sub> =
<i>S</i><sup>2</sup>, thus <i>S</i> = &radic;<span
class="overbar"><i>MS</i></span><sub>error</sub>).
<p>
This test requires equal sample sizes.  Use the harmonic mean of the
sample sizes <i>n<sub>h</sub></i> for <i>n</i> when sample sizes are
unequal.

<p align="center">
<table align="center" class="eqnserif">
<tr><td rowspan="2"><i>n<sub>h</sub></i> =
 <td colspan="2" style="border-bottom:1px solid black;"><i>k</i>
<tr>
 <td style="text-align:center;">
   <table class="eqnserif" style="padding:0;margin:0;font-size:90%;" align="center">
    <tr><td
  style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;">
     <span style="font-size:200%">&sum;</span><br>
                               1&le;<i>i</i>&le;<i>k</i>
   </table>
 <td> <i>n<sub>i</sub></i><sup>&minus;1</sup>
</table>

<h3>Tests Assuming Equal Variances</h3>
Homogeneous subsets are constructed for tests that assume equal variances.
They have a range determined by the specific test used.  To construct
these subsets:
<ol>
<li>Rank the <i>k</i> means in ascending order.  The ordered means are
denoted <span class="overbar"><i>x</i></span><sub>1</sub>, ...,
<span class="overbar"><i>x</i></span><sub><i>k</i></sub>.
<li>The range value for the test, <i>R<sub>&epsilon;,k,f</sub></i>,
is then determined.
<li>If <span class="overbar"><i>x</i></span><sub><i>k</i></sub> &minus;
  <span class="overbar"><i>x</i></span><sub>1</sub> >
  <i>Q<sub>h</sub>R<sub>&epsilon;,k,f</sub></i>, then there is a
 <em>significant</em> range, and the ranges of two sets of
 <i>k</i> &minus; 1 means {<span class="overbar"><i>x</i></span><sub>1</sub>,...,<span class="overbar"><i>x</i></span><sub><i>k</i>&minus;1</sub>}
  and {<span class="overbar"><i>x</i></span><sub>2</sub>,...,<span class="overbar"><i>x</i></span><sub><i>k</i></sub>}
 are compared with <i>Q<sub>h</sub>R</i><sub>&epsilon;,<i>k</i>&minus;1,<i>f</i></sub>.
 So long as the range continues to be significant, ever smaller subsets
 are examined.  For some tests, <i>Q<sub>i,j</sub></i> is used instead
 of <i>Q<sub>h</sub></i>.
<li>Each time a range is found not significant, the means become a
  <i>homogeneous subset</i>.
</ol>
The range values for the various types of tests are indicated:
<p align="center">
<table>
<tr><th>Test<th>Range Value Condition
<tr><td>Student-Newman-Keuls
   <td><i>R<sub>&epsilon;,r,f</sub></i> = <i>S<sub>&epsilon;,r,f</sub></i>
<tr><td>Tukey&#146;s Honestly Significant Difference
  <td><i>R<sub>&epsilon;,r,f</sub></i> = <i>S<sub>&epsilon;,r,f</sub></i>
  <br>The confidence intervals of the mean differences are calculated
   using <i>Q<sub>i,j</sub></i> instead of <i>Q<sub>h</sub></i>.
<tr><td>Tukey&#146;s <i>b</i>
 <td><table class="eqnserif">
  <tr><td rowspan="2"><i>R<sub>&epsilon;,r,f</sub></i> =
    <td style="border-bottom:1px solid black;">
       <i>S<sub>&epsilon;,r,f</sub></i> + <i>R<sub>&epsilon;,k,f</sub></i>
  <tr>
   <td style="text-align:center;">2
  </table>
<tr><td>Duncan's Multiple Range Test
  <td><i>R<sub>&epsilon;,r,f</sub></i> = <i>S<sub>&alpha;<sub>r</sub>,r,f</sub></i>
  where <i>&alpha;<sub>r</sub></i> = 1 &minus; (1 &minus; &epsilon;)<sup><i>r</i>&minus;1</sup>
<tr><td>Scheff&eacute; Test
  <td><i>R<sub>&epsilon;,r,f</sub></i> = &radic;<span
  style="border-top:1px solid black;">2(<i>k</i> &minus;
     1)<i>F</i><sub>1&minus;&epsilon;</sub>(<i>k</i> &minus; 1, <i>f</i>)</span>
  <br>The confidence intervals of the mean differences are calculated
   using <i>Q<sub>i,j</sub></i> instead of <i>Q<sub>h</sub></i>.
<tr><td>Hochberg's GT2
  <td><i>R<sub>&epsilon;,r,f</sub></i> =
    <i>M<sub>&epsilon;,k*,f</sub></i>&radic;<span
     style="border-top:1px solid black;">2</span>
  <br>The confidence intervals of the mean differences are calculated
   using <i>Q<sub>i,j</sub></i> instead of <i>Q<sub>h</sub></i>.
<tr><td>Gabriel&#146;s Pairwise Comparisons Test
  <td>The test statistic and critical point are:<br>
   <table class="eqnserif">
   <tr><td rowspan="2">|<span class="overbar"><i>x</i></span><sub><i>i</i></sub>
    &minus; <span class="overbar"><i>x</i></span><sub><i>j</i></sub>|
    &ge; <i>s<sub>pp</sub></i><span style="font-size:200%;">(</span>
  <td style="border-bottom:1px solid black;text-align:center;">1
  <td rowspan="2">+
  <td style="border-bottom:1px solid black;text-align:center;">1
  <td rowspan="2"><span style="font-size:200%;">)</span> <i>M<sub>&epsilon;,k*,f</sub></i>
  <tr>
   <td>&radic;<span style="border-top:1px solid black;">2<i>n<sub>i</sub></i></span>
   <td>&radic;<span style="border-top:1px solid black;">2<i>n<sub>j</sub></i></span>
  </table>
   where <i>s<sub>pp</sub></i> is the square root of the mean square error (MSE)
  <br><i>n<sub>h</sub></i> is used instead of <i>n<sub>i</sub></i> and
   <i>n<sub>j</sub></i> for homogeneous subsets.  Confidence intervals
   of mean differences are calculated from this equation.
</table>
<p>
For the certain tests, namely the least signficant difference (LSD),
Bonferroni, and Sidak, only pairwise confidence intervals are provided.
The test statistic is:
<p align="center">
<span class="overbar"><i>x</i></span><sub><i>i</i></sub> &minus;
  <span class="overbar"><i>x</i></span><sub><i>j</i></sub> >
  <i>Q<sub>i,j</sub>R<sub>&epsilon;,k,f</sub></i>
<p>
For the LSD, <i>R<sub>&epsilon;,k,f</sub></i> = &radic;<span
style="border-top:1px solid black;">2<i>F</i><sub>1&minus;&alpha;</sub>(1,<i>f</i>)</span>.
For Bonferroni, <i>R<sub>&epsilon;,k,f</sub></i> = &radic;<span
style="border-top:1px solid black;">2<i>F</i><sub>1&minus;&alpha;&prime;</sub>(1,<i>f</i>)</span>,
where &alpha;&prime; = &epsilon; / <i>k</i>*.
For the Sidak <i>t</i> test, <i>R<sub>&epsilon;,k,f</sub></i> = &radic;<span
style="border-top:1px solid black;">2<i>F</i><sub>1&minus;&alpha;1,f</sub></span>,
where &alpha; = 1 &minus; (1 &minus; &epsilon;)<sup>2/<i>k</i>(<i>k</i>&minus;1)</sup>
<p>
Dunnett tests are mainly used to compare treatment groups to a control
group.  The confidence interval for the difference between control groups
and other groups are given below.
<p>
For Dunnett's two-tailed <i>t</i> test (assuming equal variances), the
CI is:
<p>
<table class="eqnserif">
<tr><td rowspan="2">&nu;<sub><i>i</i>,0</sub> =
  |<span class="overbar"><i>x</i></span><sub><i>i</i></sub> &minus;
   <span class="overbar"><i>x</i></span><sub>0</sub>| >
  <i>d<sup>&epsilon;</sup><sub>k,&nu;</sub> s<sub>dd</sub>
  <span style="font-size:200%;">&radic;</span>
 <td style="border-top:1px solid black;">1
 <td rowspan="2" style="border-top:1px solid black;">+
 <td style="border-top:1px solid black;">1
<tr>
 <td style="border-top:1px solid black;"><i>n</i><sub>0</sub>
 <td style="border-top:1px solid black;"><i>n<sub>i</sub></i>
</table>
<p>
<i>d<sup>&epsilon;</sup><sub>k,&nu;</sub></i> is the upper 100&epsilon;
percentage point in the distribution of
<p>
<i>T</i> = max<sub
style="vertical-align:-1em;right:-3em;">1&le;<i>i</i>&le;<i>k</i></sub>
{|<i>T<sub>i</sub></i>|} and

<table class="eqnserif" style="display:inline;vertical-align:middle;">
<tr><td rowspan="2"><i>T<sub>i</sub></i> =
 <td colspan="2" style="border-bottom:1px solid black;">
   (<span class="overbar"><i>x</i></span><sub><i>i</i></sub> &minus;
   <span class="overbar"><i>x</i></span><sub>0</sub>)
<tr><td style="vertical-align:middle;"><i>s<sub>dd</sub></i><span
   style="font-size:200%;">&radic;</span>
 <td style="border-top:1px solid black;">
  <table class="eqnserif" style="font-size:75%;">
  <tr><td>1<td rowspan="2">+<td>1
  <tr><td style="border-top:1px solid black;"><i>n</i><sub>0</sub>
    <td style="border-top:1px solid black;"><i>n<sub>i</sub></i>
  </table>
</table>

and <i>s</i><sup>2</sup><sub><i>dd</i></sub>

<h3>Which Post-Hoc Test?  Choices in Comparing Means</h3>
The choice in which post-hoc test to use will often be confusing since
many tests have criteria that are easily met.  The important elements to
consider in the ability to use a post-hoc test are:
<ul>
<li>What about variance homogeneity (group variance equality)?  If a
test was designed to handle it, it also might be inappropriate for
groups where the variance is very equal.
<li>What about identical sizes for all groups?  Many tests require this
and it is thus easy to exclude these from consideration.  They may also
have a variant to weight size into pairwise or groupwise comparisons.
<li>Normality is usually assumed for all parametric tests.  Nonparametric
tests should be used where it cannot be assumed or tests of normality
consistently fail, or where transformation to approximate normality are
questionable.
</ul>

<p>
Other (treatment) group comparisons are also possible and their
use depends on the desired inference.  Computation of confidence
intervals for mean difference comparisons using other methods
are shown (Tukey&#146;s formula is shown once again):


<table class="eqnserif" style="font-size:110%;margin-left:5em;margin-top:1em;">
<col style="width:10em;">
<tr>
 <td rowspan="2">Tukey:
 <td rowspan="2">
   <i><span style="text-decoration:overline;">y</span><sub>i</sub></i>
 <td rowspan="2">&nbsp;&minus;
   <i><span style="text-decoration:overline;">y</span><sub>j</sub></i>
 <td rowspan="2">&nbsp;&plusmn;
 <td style="border-bottom:1px solid black;"><i>Q</i>
 <td rowspan="2">&nbsp;<i>s</i>
 <td rowspan="2"> <span style="font-size:200%;">&radic;</span>
 <td rowspan="2" style="border-top:1px solid black;">
   <table class="eqnserif" style="font-size:110%">
   <tr><td style="border-bottom:1px solid black;">1
      <td rowspan ="2">+
       <td style="border-bottom:1px solid black;">1
   <tr><td><i>n<sub>i</sub></i>
     <td><i>n<sub>j</sub></i>
   </table>
<tr>
 <td>&radic;<span style="text-decoration:overline;">2</span>
</table>

<table class="eqnserif" style="font-size:110%;margin-left:5em;margin-top:1em;">
<col style="width:10em;">
<tr>
 <td rowspan="2">Fisher:
 <td rowspan="2">
   <i><span style="text-decoration:overline;">y</span><sub>i</sub></i>
 <td rowspan="2">&nbsp;&minus;
   <i><span style="text-decoration:overline;">y</span><sub>j</sub></i>
 <td rowspan="2">&nbsp;&plusmn;
 <td rowspan="2">&nbsp;<i>ts</i>
 <td rowspan="2"> <span style="font-size:200%;">&radic;</span>
 <td rowspan="2" style="border-top:1px solid black;">
   <table class="eqnserif" style="font-size:110%">
   <tr><td style="border-bottom:1px solid black;">1
      <td rowspan ="2">+
       <td style="border-bottom:1px solid black;">1
   <tr><td><i>n<sub>i</sub></i>
     <td><i>n<sub>j</sub></i>
   </table>
</table>

<table class="eqnserif" style="font-size:110%;margin-left:5em;margin-top:1em;">
<col style="width:10em;">
<tr>
 <td rowspan="2">Dunnett:
 <td rowspan="2">
   <i><span style="text-decoration:overline;">y</span><sub>i</sub></i>
 <td rowspan="2">&nbsp;&minus;
   <i><span style="text-decoration:overline;">y</span><sub>j</sub></i>
 <td rowspan="2">&nbsp;&plusmn;
 <td rowspan="2">&nbsp;|<i>d</i>|<i>s</i>
 <td rowspan="2"> <span style="font-size:200%;">&radic;</span>
 <td rowspan="2" style="border-top:1px solid black;">
   <table class="eqnserif" style="font-size:110%">
   <tr><td style="border-bottom:1px solid black;">1
      <td rowspan ="2">+
       <td style="border-bottom:1px solid black;">1
   <tr><td><i>n<sub>i</sub></i>
     <td><i>n<sub>j</sub></i>
   </table>
</table>

<table class="eqnserif" style="font-size:110%;margin-left:5em;margin-top:1em;">
<col style="width:10em;">
<tr>
 <td rowspan="2" style="vertical-align:top;">MCB:
 <td>lower endpoint:  smaller of zero or
   <i><span style="text-decoration:overline;">y</span></i>
     &minus; max
    <span style="font-size:75%;position:relative;left:-2em;top:1em;"><i>j</i>&ne;1</span>
   <i><span style="text-decoration:overline;">y</span><sub>j</sub></i>
    &minus; <i>ds</i>&radic;<span style="text-decoration:overline;">2/<i>n</i></span>
<tr>
 <td>upper endpoint:  larger of zero or
   <i><span style="text-decoration:overline;">y</span></i>
     &minus; max
    <span style="font-size:75%;position:relative;left:-2em;top:1em;"><i>j</i>&ne;1</span>
   <i><span style="text-decoration:overline;">y</span><sub>j</sub></i>
    &minus; <i>ds</i>&radic;<span style="text-decoration:overline;">2/<i>n</i></span>
</table>



<p>In these formulas,
<i><span class="overbar">y</span><sub>i</sub></i> is the sample mean
for group <i>i</i>, <i>s</i> is the pooled standard deviation, which is
the square root of MSE, and <i>n</i> is the error degrees of freedom.
Special to the Fisher formula, <i>t</i> is the upper
&alpha;/2 point of Student&#146;s <i>t</i> distribution with
&nu; degrees of freedom.  In the Dunnett formula,
|d| is the two-sided upper &alpha; equicoordinate point of a central
(<i>k</i>&nbsp;&minus;&nbsp;1) variate Student&#146;s <i>t</i> distribution.

<p>
The MCB formula applies only when group sizes are equal.  <i>d</i>
is the one-sided upper &alpha; equicoordinate point of a central
(<i>k</i>&nbsp;&minus;&nbsp;1) variate Student&#146;s <i>t</i> distribution
with correlation 0.5.  Suppose you chose the best to be the largest
mean, and you want the confidence interval for the <i>i</i>th mean
minus the largest of the others.  Then you would use this MCB.
When the best is the smallest of the group means, the formulas
are the same, except that max is replaced by min.

<p>
The critical value for Tukey, Dunnett, and MCB are found by
solving 1&nbsp;&minus;&nbsp;&alpha; = <i>F</i>(<i>x</i>),
where <i>x</i> is the critcal value on the <i>F</i> distribution.
The critical value for Fisher is found using an approximation
to the inverse of the Student <i>t</i> distribution.

<p>
Tukey&#146;s confidence intervals are wider and the hypothesis
testing less powerful for a given family error rate.  Dunnett and
MCB should be used in preference.  The MCB is superior to Dunnett
if you want to eliminate levels that are not the best and to
identify those that are the best or close to the best.  Choice
of Tukey vs. Fisher depends on which error rate, family or individual,
you wish to specify.

<p>
The individual group errors are exact in all cases here.  The errors
of the entire set of groups are exact where there are equal group sizes.
However if group sizes are unequal, the error for the entire set of
groups will be slightly smaller than the formula of Tukey, Fisher,
and MCB indicate, while the Dunnett formula will be exact for
unequal sample sizes.

<h1>Power of test and sample size</h1>

The power of a test is defined as the probability of rejecting
a false null hypothesis.  It is a consideration to be taken before
an experiment is to be conducted.  Often the usefulness of determining
the power of a test is to determine if a proposed number of test
subjects is greater than or less than that needed to achieve the
desired goal of the project.

<p>
Calculating power in ANOVA is more complicated than that for
hypothesis testing.  First it is necessary to calculate the quantity

<p align="center">
<table class="eqnserif" style="font-size:110%">
<tr><td rowspan="2">&phi; =
  <td style="border-bottom:1px solid black;font-size:200%;">&radic;
    <table class="eqnserif" style="display:inline;font-size:50%;">
    <tr><td style="border-top:1px solid black;padding-top:1ex;">
      <span style="font-size:150%;">&sum;</span><sup
         style="position:relative;top:-0.5em;"><i>k</i></sup><sub
      style="position:relative;left:-0.5em;"><i>j</i>=1</sub>
      &alpha;<sup>2</sup><sub><i>j</i></sub>/<i>k</i>
    </table>
<tr><td>&sigma;<sub><i>e</i></sub>/&radic;<i><span
    style="text-decoration:overline;">n</span><sub>j</sub></i>
</table>

<p>
<table class="eqnserif" style="display:inline;margin:0;">
<tr><td style="padding:0;margin:0;"><span style="font-size:150%;">&sum;</span><sup
     style="position:relative;top:-0.5em;"><i>k</i></sup><sub
   style="position:relative;left:-0.5em;"><i>j</i>=1</sub>
   &alpha;<sup>2</sup><sub><i>j</i></sub>/<i>k</i>
</table>
is the sum of the squared treatment effects, <i>&sigma;<sub>e</sub></i>
is the common standard deviation of the treatment populations,
<i>k</i> is the number of treatment populations represented in
the analysis, and <i>n<sub>j</sub></i> is the size of the sample
from the <i>j</i>th treatment population.  All treatment sample
sizes are assumed to be equal.  In addition to &phi;, the level
of significance &alpha; and the treatment (&nu;<sub>1</sub>)
and error (&nu;<sub>2</sub>) degrees of freedom must be known.

<p>
The true value of
<table class="eqnserif" style="display:inline;margin:0;">
<tr><td style="padding:0;margin:0;"><span style="font-size:150%;">&sum;</span><sup
     style="position:relative;top:-0.5em;"><i>k</i></sup><sub
   style="position:relative;left:-0.5em;"><i>j</i>=1</sub>
   &alpha;<sup>2</sup><sub><i>j</i></sub>/<i>k</i>
</table>
will in truth be unknown.  Rather this quantity can be estimated using
the equation

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2" style="font-size:80%;padding:0;vertical-align:bottom;"><i>k</i><br>
      <span style="font-size:200%">&sum;</span><br><i>j</i>=1
 <td rowspan="2">&alpha;<span
   style="position:relative;left:-1ex;">&circ;</span><sup>2</sup><span
   style="position:relative;left:-1ex;"><sub><i>j</i></sub></span> =
 <td style="border-bottom:1px solid black;"><i>k</i> &minus; 1
 <td rowspan="2" style="font-size:125%;">(MSTR &minus; MSE)
<tr><td><i>n<sub>j</sub></i>
</table>

<p>
For proof that this equation is an unbiased estimator,
see R. E. Kirk (<i>Experimental Design:  Procedures for the Behaviorial
Sciences</i>, (Belmont, CA: Brooks/Cole), 1968).
As for estimating &sigma;<sub>e</sub>s we can use
&radic;<span class="overbar">MSE</span>.

<p>
For an example, suppose 24 subjects are assigned to one of four
treatment groups in which different types of muscle tension on
anxiety are studied.  A standard one-way ANOVA table is constructed.

<p align="center">
<table align="center">
<col style="font:bold 90% Helvetica,Arial,sans-serif;">
<col span="4" style="font:normal 100% 'Arial Narrow',sans-serif;">
<tr>  <th>Source of
           variation  <th>SS     <th>df    <th>MS     <th> <i>F</i>
<tr>  <td> Treatment  <td> 230.4584 <td>  3  <td> 76.8195  <td> 5.88
<tr>  <td> Error      <td> 261.4999 <td> 20  <td> 13.0750
<tr>  <td> Total      <td> 491.9583 <td> 23
</table>

<p>
For <i>k</i> = 4, <i>n<sub>j</sub></i> = 6, MSTR = 76.8195,
and MSE = 13.0750, we calculate

<table class="eqnserif" style="display:inline;vertical-align:middle;">
<tr><td style="font-size:80%;padding:0;vertical-align:bottom;line-height:100%;"><i>k</i><br>
      <span style="font-size:200%">&sum;</span><br><i>j</i>=1
 <td rowspan="2" style="padding:0;">&alpha;<span
   style="position:relative;left:-1ex;">&circ;</span><sup>2</sup><span
   style="position:relative;left:-1ex;"><sub><i>j</i></sub></span>
</table>

to be 31.87225.  This value is now used to calculate &phi;, also using the
other quantities.  This value is now 1.91.

<p>
A special set of charts created by Pearson and Hartley in 1951
(<i>Biometrika</i> <b>38</b>: 112-130) is now used to find the
power associated with the <i>F</i> test.  Particular charts for
each &nu;<sub>1</sub> (the numerator degrees of freedom for VR)
have been created;  its particular chart, &nu;<sub>1</sub> = 3
in this case, should be located.  At the bottom axis of this
chart is the value for &phi;.  Also on the chart are a set of
hyperbolic (parabolic) lines which are grouped by significance
&alpha;.  Each line in the group represents a particular &nu;<sub>2</sub>,
or denominator degree of freedom.  From the bottom axis for
&phi; for 1.91 and using the significance &alpha; = 0.05 with
&nu;<sub>2</sub> = 20, we can determine that the power of a test
(1 &minus; &beta;) is about 0.80.

<p>
How does one increase the power of a test?
By either:
<ol>
<li>increasing the sample size, or
<li>selecting an alternative design that reduces the error variance,
which means reducing &sigma;<sub>e</sub> which in turn increases
&phi;
</ol>

<p>
Suppose we know what power we want for the test and we wish to
determine the sample size that will produce that power.  First
we must select a level of significance &alpha;, keeping in mind
that reduction of Type I error (the probability of rejecting the
true null hypothesis).  Then we select the power, or the probability
of rejecting a false null hypothesis (which we want).  Next we must
estimate the error variance, &sigma;<sub>e</sub>,
which of course is unknown, but may be estimated from previous work,
pilot experiments, or reasonable assumptions.  The number of treatment
groups gives us the necessary numerator degrees of freedom.  Then
we must calculate the minimum value of
&sigma; (&mu;<sub>.j</sub> &minus; &mu;)<sup>2</sup>, which is our
specification of the minimum differences among means we consider
important.

<p>
Suppose the researchers of the experiment above want at least
a power of 0.95 and will adjust the sample size to do it.
Also they have considered that any difference of treatment
means greater than 3 (&mu;<sub><i>.j</i></sub> &minus; &mu; &ge; 3)
worth detecting.  They have the previous work in hand.
Hence they know that &alpha; = 0.05, 1 &minus; &beta; = 0.95,
MSE = 13.0750, <i>k</i> = 4 and
&sigma; (&mu;<sub><i>.j</i></sub> &minus; &mu;)<sup>2</sup> =
3<sup>2</sup> + 3<sup>2</sup> + 3<sup>2</sup> + 3<sup>2</sup> = 36.
Using some of these values, we calculate

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2">&phi;&acute; =
  <td style="border-bottom:1px solid black;">&radic;<span
      class="overbar">36/4</span>
 <td rowspan="2"> = 0.83 &radic;<i><span
      class="overbar">n</span><sub>j</sub></i>
<tr><td>&radic;<span class="overbar"> 13.0750</span> /
   &radic;<i><span class="overbar">n</span><sub>j</sub></i>
</table>

<p>
It is necessary to solve for <i>n<sub>j</sub></i>.  When using the chart,
the solution is trial-and-error.  We now select the chart for which
&nu;<sub>1</sub> = 3.  One can find the value for which
1 &minus; &beta; = 0.95 and then proceed to select values for
&nu;<sub>2</sub> = <i>n<sub>j</sub></i> &minus; 1 and keeping in mind
that &phi;&prime; must be calculated for each change and the true power
calculated from &phi;&prime;.  When <i>n<sub>j</sub></i> = 7, this will
require (&nu;<sub>1</sub> + 1)(<i>n<sub>j</sub></i> &minus; 1) =

<h1>ANOVA of the The Randomized Complete Block Design</h1>

Experimenters can reduce the error variance and increase the power
of an analysis by using a design other than the completely randomized
design.  Suppose researchers want to know the effectiveness of four
different drugs in relieving depression.  The researchers want to
eliminate the effects of the duration of illness, and they
randomly select four patients from each of three types of
categories regarding duration of illness:  short, medium,
and long.  The patients are randomized for one of the four
treatments.

<p>Assumptions regarding the analysis will be that
<ol>
<li>Each observation is an independent random sample with size
    one from a population defined by a treatment-block combination
<li>The <i>kn</i> populations represented in the experiment are
    normally distrbuted
<li>Each of the kn populations has the same variance &sigma;<sup>2</sup>.
<li>Treatment and blocks do not interact, meaning that the combination
    of block effects and treatment effects should be ruled out as acting
    together to produce special effects.  In the example,
    this assumption is no particular combination of a drug and a
    duration-of-illness category produces an effect different from
    other combinations of drug and duration-of-illness categories.
</ol>

The null hypothesis is that the population means of each treatment
block are equal to one another, that is no differences in the effects
of the drugs.  The alternative is that not all treatment means are
equal.  The test statistic will be as before: the ratio of the
treatment mean square MSTR to the error mean square MSE.  When
the null hypothesis is true, the ratio follows the <i>F</i> distribution
with &nu;<sub>1</sub> = <i>k</i> &minus; 1 and
&nu;<sub>2</sub> = (<i>k</i> &minus; 1)(<i>n</i> &minus; 1) degrees of freedom.

<p>
The data from the example is presented in the table.

<table align="center">
<col><col span="6" style="text-align:center;">
<tr>
 <th rowspan="2">Duration<br> of Illness
 <th colspan="4">Drug
 <th rowspan="2">Total
 <th rowspan="2">Mean
<tr>
 <th> A
 <th> B
 <th> C
 <th> D
<tr><td>Short  <td>  12 <td>  11 <td>  16 <td> 15  <td> 54<td> 13.50
<tr><td>Medium <td>  10 <td>  13 <td>  15 <td> 17  <td> 55<td> 13.75
<tr><td>Long   <td>   8 <td>   8 <td>  10 <td> 10  <td> 36<td>  9.00
<tr><td>Total  <td>  30 <td>  32 <td>  41 <td> 42  <td>145<td>
<tr><td>Mean   <td>10.00<td>10.67<td>13.67<td>14.00<td>   <td> 12.08
</table>

<p>
First it is necessary to partition the total sum of the squares, and a
new parameter is introduced to account for the effect of the blocks:

<p class="indent">
  SST = SSBL + SSTR + SSE

<p>
Here SSBL is the sum of squares for the blocks, and SSTR and SSE are
as before.  The following formulae apply in making the calculation:

<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<col span="4" style="text-align:center;">
<tr>
 <td rowspan="2"><i>C</i> =
 <td style="border-bottom:1px solid black;"><i>T</i><sub>..</sub><sup>2</sup>
 <td rowspan="2"> =
 <td style="border-bottom:1px solid black;">(grand total)<sup>2</sup>
<tr>
 <td><i>kn</i>
 <td>total number of observations
</table>
<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<tr>
 <td>SST =
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;font-size:80%;">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;"><i>k</i><br>
     <span style="font-size:200%">&sum;</span><br><i>j</i> = 1
        <td align="center"
    style="font-size:80%;padding:0;vertical-align:bottom;"><i>n<sub>j</sub></i><br>
        <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
      <td style="padding:0;"><i>y<sub>ij</sub></i><sup>2</sup> &minus; <i>C</i>
   </table>
</table>

<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<col><col style="text-align:center;"><col><col style="text-align:center;">
<tr>
 <td>
 <td style="vertical-align:bottom;margin:0;padding:0;text-align:center;">
   <table class="eqnserif" style="padding:0;margin:0;font-size:80%;">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;"><i>n</i><br>
     <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
     <td style="padding:0;"><i>T<sub>i.</sub></i><sup>2</sup>
   </table>
 <td>
 <td style="vertical-align:bottom;margin:0;padding:0;">
    (sum of the squared block totals)
<tr>
 <td>SSBL =
 <td>&#151;&#151;&#151;&#151;&#151;
 <td>&minus; <i>C</i> =
 <td>&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;
 <td>&minus; <i>C</i>
<tr>
 <td>
 <td><i>k</i>
 <td>
 <td>number of treatments
</table>

<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<col><col style="text-align:center;"><col><col style="text-align:center;">
<tr>
 <td>
 <td style="vertical-align:bottom;margin:0;padding:0;text-align:center;">
   <table class="eqnserif" style="padding:0;margin:0;font-size:80%;">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;"><i>k</i><br>
     <span style="font-size:200%">&sum;</span><br><i>j</i> = 1
     <td style="padding:0;"><i>T<sub>.j</sub></i><sup>2</sup>
   </table>
 <td>
 <td style="vertical-align:bottom;margin:0;padding:0;text-align:center;">
   (sum of the squared treatment totals)
<tr>
 <td>SSTR =
 <td>&#151;&#151;&#151;&#151;&#151;
 <td>&minus; <i>C</i> =
 <td>&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;
 <td>&minus; <i>C</i>
<tr>
 <td>
 <td><i>n</i>
 <td>
 <td>number of blocks
</table>

<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<tr>
 <td>SSE =
 <td>SST &minus; SSBL &minus; SSTR
</table>

<p>
The ANOVA table for the example is constructed.

<table align="center">
<col style="font:bold 90% Helvetica,Arial,sans-serif;">
<col span="4" style="font:normal 100% 'Arial Narrow',sans-serif;">
<tr>  <th>Source of
           variation  <th>    SS    <th> df  <th>   MS     <th> VR
<tr>  <td> Treatment  <td>  37.5834 <td>  3  <td> 12.5278  <td> 7.394
<tr>  <td> Blocks     <td>  57.1667 <td>  2  <td> 28.5834
<tr>  <td> Error      <td>  10.1666 <td>  6  <td>  1.6944
<tr>  <td>            <td> 104.9167 <td> 11
</table>

<p>
Note that total degrees of freedom is <i>kn</i> &minus; 1 and the
error degrees of freedom are (<i>k</i> &minus; 1)(<i>n</i> &minus; 1)
for <i>k</i> treatments divided into <i>n</i> blocks.  The critical
value for <i>F</i> with &alpha; = 0.05 and 3 numerator and
6 denominator degrees of freedom is 4.76.  Since the variance ratio
is greater than the critical value, <i>H</i><sub>0</sub> is rejected
and we assume that there is a difference in the effects of drugs
after accounting for the effects of duration of illness.

<h2>Analysis of the Latin square design</h2>

The Latin square design goes one step further than the randomized
block by allowing elimination of two sources of variation in the
error variance.  Suppose an educator wants to know about the
learning in 8th grade students of a mathematical concept taught
five different ways.  He wonders however how much different
teachers contribute to the learning process, and the effect
of the time of day on the learning process.  For this to work,
each teacher will five different methods during various times
of the day.

<p>
In the table below, the columns represent the different times
of day and the rows the different teachers.  Each treatment is
designated by a letter <i>A</i> through <i>E</i>, and in this
experimental design, can appear only once in each row and column
of the matrix.  Next to each treatment are the scores (they may
be means of the entire group) of the class.  Note that treatments
are assignedly randomly to teacher and time of day, with the
limitation that a treatment can appear only once in each column
or row of the matrix.

<table>
<tr>
  <th>
  <th colspan="5">Time of Day
<tr>
  <th>Teacher
  <th> 8 a.m.
  <th>10 a.m.
  <th>Noon
  <th> 2 p.m.
  <th> 4 p.m.
  <th>Total
<tr><td> 1 <td> E, 80 <td> D, 90 <td> C, 66 <td> A, 64 <td> B, 58 <td> 358
<tr><td> 2 <td> B, 75 <td> E, 91 <td> A, 71 <td> D, 90 <td> C, 67 <td> 394
<tr><td> 3 <td> C, 75 <td> A, 79 <td> B, 74 <td> E, 78 <td> D, 80 <td> 386
<tr><td> 4 <td> D, 99 <td> C, 79 <td> E, 91 <td> B, 81 <td> A, 73 <td> 423
<tr><td> 5 <td> A, 62 <td> B, 85 <td> D, 96 <td> C, 61 <td> E, 85 <td> 389
<tr><td>Total<td> 391 <td>  424  <td>  398  <td>  374  <td>  363  <td>1950
<tr>
  <th>
  <th colspan="5">Treatment Totals and Means
<tr>
  <th>      <th>  A   <th>  B   <th>  C   <th>  D   <th>  E
<tr><td>Total <td> 349  <td> 373  <td> 348  <td> 455  <td> 425
<tr><td>Mean  <td> 69.8 <td> 74.6 <td> 69.6 <td> 91.0 <td> 85.0
</table>

<p>
The following assumptions must obtain when using the Latin square:

<ol>
<li>The observation <i>y</i><sub><i>ij</i>(<i>k</i>)</sub> must be
an independent random sample of size one drawn from a population that
is defined by the row-block-treatment combination <i>ij</i>(<i>k</i>).
In a given population, <i>r</i><sup>2</sup> populations will
be represented, where <i>r</i> happens to equal the number of
treatments, the number of columns, and the number of rows in
the data table, all at once.
<li> Each population in the distribution must be normally distributed.
<li> The variances of all represented populations must be equal.
<li> The rows, columns, and treatments must not interact.
</ol>

<p>
The total sum of squares SST is partitioned as
<p class="indent">
    SST = SSR + SSC + SSTR + SSE
<p>
in which SSR is the sum of the squares of the rows,
SSC are the sums of the squares of the columns, and
the others are previously defined.  Note that SSR and
SSC will remove the effects of the two sources of variation
from SSE.  The following formulae and the calculations for
each based on the table data will be as follows:

<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<tr>
 <td><i>T<sub>i..</sub></i> =
 <td> total of the <i>i</i>th row
<tr>
 <td><i>T<sub>.j.</sub></i> =
 <td> total of the <i>j</i>th column
<tr>
 <td><i>T<sub>..k</sub></i> =
 <td> total of the <i>k</i>th treatment
</table>
<table class="eqnserif" style="margin-left:15%;">
<col><col style="text-align:center;"><col style="text-align:left;">
<tr>
 <td rowspan="2"><i>C</i> =
 <td style="border-bottom:1px solid black;">
    <i>T</i><sub>...</sub><sup>2</sup>
 <td rowspan="2"> =
 <td style="border-bottom:1px solid black;">(grand total)<sup>2</sup>
<tr>
 <td><i>r</i><sup>2</sup><td>total number of observations
<tr>
 <td rowspan="2" style="text-align:right;"> =
 <td style="border-bottom:1px solid black;">1950<sup>2</sup>
 <td><td rowspan="2"> = 152,1000
<tr><td>5<sup>2</sup>
</table>

<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<tr>
 <td>SST =
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;"><i>r</i><br>
     <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
        <td align="center"
    style="font-size:80%;padding:0;vertical-align:bottom;"><i>c</i><br>
        <span style="font-size:200%">&sum;</span><br><i>j</i> = 1
      <td style="padding:0;"><i>y<sub>ij(k)</sub></i><sup>2</sup> &minus; <i>C</i>
   </table>
 <td> = (sum of all squared observations) &minus; <i>C</i>
<tr><td><td colspan="3"> = (80<sup>2</sup> + 90<sup>2</sup> + ... + 85<sup>2</sup>)
   &minus; 152,1000 = 3022
</table>

<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<col><col style="text-align:center;"><col><col style="text-align:center;">
<tr>
 <td>
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;"><i>r</i><br>
     <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
      <td style="padding:0;"><i>T<sub>i..</sub></i><sup>2</sup>
   </table>
 <td>
 <td style="vertical-align:bottom;">sum of squared row totals
<tr>
 <td>SSR =<td>&#151;&#151;&#151;&#151;
 <td> &minus; <i>C</i> =
 <td>&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;
 <td> &minus; <i>C</i>
<tr><td><td style="vertical-align:top;"><i>r</i><td>
   <td style="vertical-align:bottom;">number of columns
<tr><td>&nbsp;
<tr><td rowspan="2" style="text-align:right;">=
   <td style="border-bottom:1px solid black;" colspan="2">
   (358<sup>2</sup> + 394<sup>2</sup> + ... + 389<sup>2</sup>)
  <td rowspan="2" style="text-align:left;"> &minus; 152,1000 = 429.2
<tr><td colspan="2">5
</table>

<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<col><col style="text-align:center;"><col><col style="text-align:center;">
<tr>
 <td>
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;"><i>c</i><br>
     <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
      <td style="padding:0;"><i>T<sub>.j.</sub></i><sup>2</sup>
   </table>
 <td>
 <td style="vertical-align:bottom;">sum of squared column totals
<tr>
 <td>SSC =<td>&#151;&#151;&#151;&#151;
 <td> &minus; <i>C</i> =
 <td>&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;
 <td> &minus; <i>C</i>
<tr><td><td style="vertical-align:top;"><i>c</i><td>
   <td style="vertical-align:bottom;">number of rows
<tr><td>&nbsp;
<tr><td rowspan="2" style="text-align:right;">=
   <td style="border-bottom:1px solid black;" colspan="2">
   (391<sup>2</sup> + 424<sup>2</sup> + ... + 363<sup>2</sup>)
  <td rowspan="2" style="text-align:left;"> &minus; 152,1000 = 441.2
<tr><td colspan="2">5
</table>

<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<col><col style="text-align:center;"><col><col style="text-align:center;">
<tr>
 <td>
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;"><i>c</i><br>
     <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
      <td style="padding:0;"><i>T<sub>.j.</sub></i><sup>2</sup>
   </table>
 <td>
 <td style="vertical-align:bottom;">sum of squared treatment totals
<tr>
 <td>SSTR =<td>&#151;&#151;&#151;&#151;
 <td> &minus; <i>C</i> =
 <td>&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;&#151;
 <td> &minus; <i>C</i>
<tr><td><td style="vertical-align:top;"><i>r</i><td>
   <td style="vertical-align:bottom;">number of rows
<tr><td>&nbsp;
<tr><td rowspan="2" style="text-align:right;">=
   <td style="border-bottom:1px solid black;" colspan="2">
   (349<sup>2</sup> + 373<sup>2</sup> + ... + 425<sup>2</sup>)
  <td rowspan="2" style="text-align:left;"> &minus; 152,1000 = 1836.2
<tr><td colspan="2">5
</table>

<p class="eqnserif" style="margin-top:1em;margin-left:15%;">
SSE = SST &minus; SSR &minus; SSC &minus; SSTR = 314.8
<p>
The ANOVA table for this design is represented thusly:

<p align="center">
<table>
<tr><th>Source of variation    <th>SS	   <th>df  <th>MS     <th><i>F</i>
<tr><td>Rows                   <td>429.2   <td>4   <td>107.3
<tr><td>Columns                <td>441.2   <td>4   <td>110.3
<tr><td>Treatments             <td>1836.8  <td>4   <td>459.2  <td>17.51
<tr><td>Error                  <td>314.8   <td>12  <td>26.23
<tr><td>                       <td>3022.0  <td>24
</table>

<p>
Note that the degrees of freedom for both nuisance variables
(rows and columns variances) as well as for the treatment block
is <i>r</i>&nbsp;&minus;&nbsp;1.  The error degrees of freedom
will be (<i>r</i>&nbsp;&minus;&nbsp;1)(<i>r</i>&nbsp;&minus;&nbsp;2),
and the total degrees of freedom will be
<i>r</i><sub>2</sub>&nbsp;&minus;&nbsp;1.  When
<i>H</i><sub>0</sub> is true, the variance ratio of
MSTR/MSE will be distributed as <i>F</i> with
<i>r</i>&nbsp;&minus;&nbsp;1 and
(<i>r</i>&nbsp;&minus;&nbsp;1)(<i>r</i>&nbsp;&minus;&nbsp;2)
degrees of freedom.  In the present example, we find that,
when <i>H</i><sub>0</sub> is true, observing a VR = 17.51
or larger is less than 0.005, and so we reject
<i>H</i><sub>0</sub> at &alpha; = 0.005 and conclude there
are differences in teaching methods after accounting for
(eliminating) the effects of teachers and time of day.

<h1>Two-Way (Bivariate) ANOVA</h1>

With univariate ANOVA, treatments are applied that are compared to
one another at a single level.  Now treatments are applied at two levels
and the results of both the level of treatments and combination of
treatments are evaluated.

<p align="center">
<table>
<tr><th rowspan="2">Temp.       <th colspan="3">Oxygen
<tr>                            <th>2%   <th>6%   <th>10%
<tr><td rowspan="3">10&deg;     <td>13   <td>10   <td>15
<tr>                            <td>11   <td>4    <td>2
<tr>                            <td>3    <td>7    <td>7
<tr><td rowspan="3">16&deg;     <td>26   <td>15   <td>20
<tr>                            <td>19   <td>22   <td>24
<tr>                            <td>24   <td>18   <td>8
</table>
<p>
The following equations apply:

<table class="eqnserif" style="margin:1em 15%;">
<col style="width:4.5em;">
<col style="width:2em;">
<col style="width:3.5em;">
<tr>
 <td>SSR = <i>n<sub>k</sub>n<sub>c</sub></i>
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;">
      <i>n<sub>r</sub></i><br>
     <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
   </table>
  <td>(<span class="overbar"><i>x</i></span><sub><i>i</i></sub> &minus;
        <span class="overbar"><i>X</i></span>)<sup>2</sup>
  <td style="text-align:left;font-size:100%;padding-left:2em;">
    Find the sum of the
    difference between the mean of the treatment
    on the <i>i</i>th row and the grand mean
    <span class="overbar"><i>X</i></span> for the count of all rows
    <i>n<sub>r</sub></i>.  <i>n<sub>c</sub></i> is the number of
    column treatments, while <i>n<sub>k</sub></i> is the count of
    data within every cell (data from row-column combination)
</table>

<table class="eqnserif" style="margin:1em 15%;">
<col style="width:4.5em;">
<col style="width:2em;">
<col style="width:3.5em;">
<tr>
 <td>SSC = <i>n<sub>k</sub>n<sub>r</sub></i>
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;">
      <i>n<sub>c</sub></i><br>
     <span style="font-size:200%">&sum;</span><br><i>j</i> = 1
   </table>
  <td>(<span class="overbar"><i>x</i></span><sub><i>j</i></sub> &minus;
        <span class="overbar"><i>X</i></span>)<sup>2</sup>
  <td style="text-align:left;font-size:100%;padding-left:2em;">
  Find the sum of the difference between the mean of the treatment
    on the <i>j</i>th colunn and the grand mean
    <span class="overbar"><i>X</i></span> for the count of all columns
    <i>n<sub>c</sub></i>.
</table>

<table class="eqnserif" style="margin:1em 10%;">
<col style="width:4.5em;">
<col style="width:2em;">
<col style="width:2em;">
<col style="width:8em;">
<tr>
 <td>SSI = <i>n<sub>k</sub></i>
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;">
      <i>n<sub>r</sub></i><br>
     <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
   </table>
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;">
      <i>n<sub>c</sub></i><br>
     <span style="font-size:200%">&sum;</span><br><i>j</i> = 1
   </table>
 <td>(<span class="overbar"><i>x</i></span><sub><i>ij</i></sub> &minus;
      <span class="overbar"><i>x</i></span><sub><i>i</i></sub> &minus;
      <span class="overbar"><i>x</i></span><sub><i>j</i></sub> +
        <span class="overbar"><i>X</i></span>)<sup>2</sup>
  <td style="text-align:left;font-size:100%;padding-left:2em;">
   The sum of the square of the difference betweem the mean
   of the interaction and the grand mean less the difference
   between the grand mean and each treatment mean.
   Note that the term
   simplifies from [<span class="overbar"><i>x</i></span><sub><i>ij</i></sub>
     &minus; <span class="overbar"><i>X</i></span> &minus;
    (<span class="overbar"><i>x</i></span><sub><i>i</i></sub> &minus;
      <span class="overbar"><i>X</i></span>) &minus;
      (<span class="overbar"><i>x</i></span><sub><i>j</i></sub> &minus;
        <span class="overbar"><i>X</i></span>)]<sup>2</sup>
</table>

<table class="eqnserif" style="margin:1em 10%;">
<col style="width:4.5em;">
<col style="width:2em;">
<col style="width:2em;">
<col style="width:2em;">
<col style="width:5em;">
<tr>
 <td>SSE =
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;">
      <i>n<sub>r</sub></i><br>
     <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
   </table>
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;">
      <i>n<sub>c</sub></i><br>
     <span style="font-size:200%">&sum;</span><br><i>j</i> = 1
   </table>
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;">
      <i>n<sub>k</sub></i><br>
     <span style="font-size:200%">&sum;</span><br><i>k</i> = 1
   </table>
 <td>(<i>x<sub>ijk</sub></i> &minus;
      <span class="overbar"><i>x</i></span><sub><i>ij</i></sub>)<sup>2</sup>
  <td style="text-align:left;font-size:100%;padding-left:2em;">
   The sum of the square of the difference between each individual
   point and the mean of the interaction.
</table>


<table class="eqnserif" style="margin:1em 10%;">
<col style="width:4.5em;">
<col style="width:2em;">
<col style="width:2em;">
<col style="width:2em;">
<col style="width:5em;">
<tr>
 <td>SST =
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;">
      <i>n<sub>r</sub></i><br>
     <span style="font-size:200%">&sum;</span><br><i>i</i> = 1
   </table>
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;">
      <i>n<sub>c</sub></i><br>
     <span style="font-size:200%">&sum;</span><br><i>j</i> = 1
   </table>
 <td style="text-align:center;padding:0;margin:0;">
   <table class="eqnserif" style="padding:0;margin:0;" align="center">
    <tr><td
   style="text-align:center;font-size:80%;padding:0;vertical-align:bottom;">
      <i>n<sub>k</sub></i><br>
     <span style="font-size:200%">&sum;</span><br><i>k</i> = 1
   </table>
 <td>(<i>x<sub>ijk</sub></i> &minus;
      <span class="overbar"><i>X</i></span>)<sup>2</sup>
  <td style="text-align:left;font-size:100%;padding-left:2em;">
   The sum of the square of the difference between each individual
   point and the grand mean.
</table>

The row degrees of freedom will be <i>n<sub>r</sub></i> &minus; 1,
the column degrees of freedom <i>n<sub>c</sub></i> &minus; 1,
the interaction degrees of freedom being the product
(<i>n<sub>r</sub></i> &minus; 1)(<i>n<sub>c</sub></i> &minus; 1),
the error degrees of freedom
<i>n<sub>r</sub>n<sub>c</sub></i>(<i>N</i> &minus; 1),
and all sum of all these degrees of freedom is the total,
<i>N</i> &minus; 1.
<p>
Three variance ratios, <i>F</i><sub>R</sub>, <i>F</i><sub>C</sub>, and
<i>F</i><sub>I</sub>, are computed as MSR/MSE, MSC/MSE, and MSI/MSE,
resp.

<p>
Temperature has two groups, oxygen tension three.  The calculations
for ANOVA and the table are shown below:

<table class="data calctable">
<col><col><col>
<col span="3" style="width:3em;">
<col><col>
<col span="3" style="width:4em;">
<col>
<col span="3" style="width:4em;">
<tr><th style="background:none;">
   <th>A<th>B<th>C<th>D<th>E<th>F<th>G<th>H<th>I<th>J<th>K<th>L<th>M<th>N
<tr><th colspan="7"><th colspan="8" style="font:bold 83% Verdana,Arial,sans-serif;">
   Square of the Differences
<tr><th>1<th rowspan="2" colspan="2">Temp.<th colspan="3">Oxygen<th><th><th colspan="3">Oxygen
       <th><th colspan="3">Oxygen
<tr><th>2   <th>2%<th>6%<th>10%<th><th><th>2%<th>6%<th>10%<th><th>2%<th>6%<th>10%
<tr><th>3<td rowspan="3">
 10&deg;<td>     <td class="datum">13<td class="datum">10<td class="datum">15
                                   <td>  <td>T<td>0.6049<td>14.2716<td>1.4938<td>E<td>16<td>9<td>49
<tr><th>4<td>     <td class="datum">11<td class="datum">4 <td class="datum">2
                                   <td>  <td>T<td>7.7160<td>95.6049<td>138.72<td>E<td>4 <td>9<td>36
<tr><th>5<td>     <td class="datum">3 <td class="datum">7 <td class="datum">7
                                   <td>  <td>T<td>116.16<td>45.9383<td>45.938<td>E<td>36<td>0<td> 1
<tr><th>6<td><td>count<td>3 <td>3 <td>3 <td>9 <td colspan="8">
<tr><th>7<td><td>sum  <td>27<td>21<td>24<td>72<td>R<td>33.383<td colspan="6">
<tr><th>8<td><td>mean <td> 9<td> 7<td> 8<td> 8<td>I<td>1.4938<td>0.01234<td>1.2345
   <td colspan="4">
<tr><th>9<td rowspan="3">
 16&deg;<td>     <td class="datum">26<td class="datum">15<td class="datum">20
                                   <td>  <td>T<td>149.38<td>1.49383<td>38.716<td>E<td>9 <td>11.111<td>7.1111
<tr><th>10<td>     <td class="datum">19<td class="datum">22<td class="datum">24
                                   <td>  <td>T<td>27.272<td>67.6049<td>104.49<td>E<td>16<td>13.444<td>44.444
<tr><th>11<td>     <td class="datum">24<td class="datum">18<td class="datum">8
                                   <td>  <td>T<td>104.49<td>17.8272<td>33.383<td>E<td>1 <td>0.1111<td>87.111
<tr><th>12<td><td>count<td>3 <td>3 <td>3 <td>9 <td colspan="8">
<tr><th>13<td><td>sum  <td>69<td>55<td>52<td>176<td>R<td>33.383<td colspan="6">
<tr><th>14<td><td>mean <td>23<td>18.333<td>17.333<td>19.556<td>I<td>1.4938<td>0.01235<td>1.2346
   <td colspan="4">
<tr><td colspan="14" style="background:none;font-size:0.5em;">&nbsp;
<tr><th>15<td><td>count<td>6 <td>6 <td>6
<tr><th>16<td><td>sum  <td>96<td>76<td>76
<tr><th>17<td><td>mean <td>16<td>12.667<td>12.667<td><td>C<td>4.9383<td>1.23457<td>1.2346
<tr><td colspan="14" style="background:none;font-size:0.5em;">&nbsp;
<tr><th>18<td>Total<td>count<td>18
<tr><th>19<td><td>sum  <td>248
<tr><th>20<td><td>mean <td>13.7778
</table>

Because the table is complex, it has been organized in spreadsheet form,
and the formulas for determing spreadsheet cell values are given as
for Microsoft Excel.
<p>
It is important not to lose sight of the original data, which is shown
in a larger and blue-colored font.  These are the row-column combination
treatments, and each has three data values (<i>n<sub>k</sub></i> = 3).
The means of each of these cells is calculated in rows 8 and 14, columns
C through E, and these will correspond to
<span class="overbar"><i>x</i></span><sub><i>ij</i></sub> in the
computations.

<p align="center">
<table class="left simple">
<tr><td>C6=COUNT(C3:C5)   <td>D6=COUNT(D3:D5)    <td>E6=COUNT(E3:E5)
<tr><td>C7=SUM(C3:C5)     <td>D7=SUM(D3:D5)      <td>E7=SUM(E3:E5)
<tr><td>C8=AVERAGE(C3:C5) <td>D8=AVERAGE(D3:D5)  <td>E8=AVERAGE(E3:E5)
<tr><td>
<tr><td>C12=COUNT(C9:C11)   <td>D12=COUNT(D9:D11)    <td>E12=COUNT(E9:E11)
<tr><td>C13=SUM(C9:C11)     <td>D13=SUM(D9:D11)      <td>E13=SUM(E9:E11)
<tr><td>C14=AVERAGE(C9:C11) <td>D14=AVERAGE(D9:D11)  <td>E14=AVERAGE(E9:E11)
</table>
<p>
There are two row treatments (<i>n<sub>r</sub></i> = 2), corresponding
to the temperature.  The means of the row treatments requiring summing
all data within the row (given in cells F7 and F13 for the two treatments),
and dividing by the count of data (given in cells F6 and F12), to
come up with the means for the quantities
<span class="overbar"><i>x</i></span><sub><i>i</i></sub> (given in cells
F8 and F13).
<p align="center">
<table class="left simple">
<tr><td>F6=COUNT(C3:E5)   <td>F12=COUNT(C9:E11)
<tr><td>F7=SUM(C3:E5)     <td>F13=SUM(C9:E11)
<tr><td>F8=AVERAGE(C3:E5) <td>F14=AVERAGE(C9:E11)
</table>
<p>
The three column treatments (<i>n<sub>c</sub></i> = 3), correspond to
the oxygen tension.  One sums all the data corresponding to a single
column treatment within each row treatment, and divides by the number of
individual data points counted to arrive at the mean of each of the
column treatments, which are
The count, sums, and means of these columns are
<span class="overbar"><i>x</i></span><sub><i>j</i></sub> in the
computations. These are presented at the bottom of the columns,
in rows 15-17 of columns C-E.
<p align="center">
<table class="left simple">
<tr><td>C15=COUNT(C3:C5,C9:C11)  <td>D15=COUNT(D3:D5,D9:D11)  <td>E15=COUNT(E3:E5,E9:E11)
<tr><td>C16=SUM(C3:C5,C9:C11)    <td>D16=SUM(D3:D5,D9:D11)    <td>E16=SUM(E3:E5,E9:E11)
<tr><td>C17=AVERAGE(C3:C5,C9:C11)<td>D17=AVERAGE(D3:D5,D9:D11)<td>E17=AVERAGE(E3:E5,E9:E11)
</table>
<p>
A description of the <em>total</em> data is given in rows 18-20 of
column C.  The grand mean <span class="overbar">X</span> is given
in cell C20.
<p align="center">
<table class="left simple">
<tr><td>C18=COUNT(C3:E5,C9:E11)
<tr><td>C19=SUM(C3:E5,C9:E11)
<tr><td>C20=AVERAGE(C3:E5,C9:E11)
</table>
<p>
With the description of the data complete (all the important mean values
calculated), the next step in the process is the calculation of all
the sums of the squared differences of data from means and means from
grand means.  In two-way ANOVA, there are four components to the sums
of the squared differences (SSR, SSC, SSI, SSE), and these equal the total
(SST = SSR + SSC + SSI + SSE).  SST can be computed alone, and then each
of the components can be summed, which serves as a check that the computations
were correct.
<p>
The square of the differences between the individual data points and the grand mean
are to be calculated, the basis for the total sum of squares (SST).
The cells which correspond
to that calculation are labeled &#147;T&#148; in the worksheet:
<p align="center">
<table class="left simple">
<tr><td>H3=(C3-$C$20)^2    <td>I3=(D3-$C$20)^2    <td>J3=(E3-$C$20)^2
<tr><td>H4=(C4-$C$20)^2    <td>I4=(D4-$C$20)^2    <td>J4=(E4-$C$20)^2
<tr><td>H5=(C5-$C$20)^2    <td>I5=(D5-$C$20)^2    <td>J5=(E5-$C$20)^2
<tr><td>
<tr><td>H9=(C9-$C$20)^2    <td>I9=(D9-$C$20)^2    <td>J9=(E9-$C$20)^2
<tr><td>H10=(C10-$C$20)^2  <td>I10=(D10-$C$20)^2  <td>J10=(E10-$C$20)^2
<tr><td>H11=(C11-$C$20)^2  <td>I11=(D11-$C$20)^2  <td>J11=(E11-$C$20)^2
</table>
<p>
Note the use of the '$' symbol before a row and column designator:  with Excel
it makes it easier to use a forumla fill-down or fill-right such that the
absolute row-column specification does not change, while a relative specification
allows it to change.  Read the directions on the use of the spreadsheet
software, since this is a time-saving feature for numerous calculations.
<p>
The error sum of squares (SSE) are the difference between the individual
data points within a row-column treatment and their mean
(<span class="overbar"><i>x</i></span><sub><i>ij</i></sub>).  These are
labeled &#147;E&#148; in the worksheet:
<p align="center">
<table class="left simple">
<tr><td>L3=(C3-C$8)^2    <td>M3=(D3-D$8)^2    <td>N3=(E3-E$8)^2
<tr><td>L4=(C4-C$8)^2    <td>M4=(D4-D$8)^2    <td>N4=(E4-E$8)^2
<tr><td>L5=(C5-C$8)^2    <td>M5=(D5-D$8)^2    <td>N5=(E5-E$8)^2
<tr><td>
<tr><td>L9=(C9-C$14)^2    <td>M9=(D9-D$14)^2    <td>N9=(E9-E$14)^2
<tr><td>L10=(C10-C$14)^2  <td>M10=(D10-D$14)^2  <td>N10=(E10-E$14)^2
<tr><td>L11=(C11-C$14)^2  <td>M11=(D11-D$14)^2  <td>N11=(E11-E$14)^2
</table>
<p>
Most people generally think easily of the above squared differences.
The next three components are more arcane.  The row treatment
sum (SSR) requires computing the differences between each row
treatment mean for all row treatments <i>i</i> and the grand mean.
In the worksheet, these are given as &#147;R&#148;.  Since there are
only two row treatments, there should be only two differences with
&#147;R&#148; next to them.
<p align="center">
<table class="left simple">
<tr><td>H7=(F8-$C$20)^2
<tr><td>H13=(F14-C20)^2
</table>
<p>
The difference between the means of the column treatments and grand
mean are evaluated for the computation of the column treatment sum (SSC).
Since there are three column treatments, there should
be only 3 values.  In the worksheet, they are labeled &#147;C&#148;:
<p align="center">
<table class="left simple">
<tr><td>H17=(C17-$C$20)^2
<tr><td>I17=(D17-$C$20)^2
<tr><td>J17=(E17-$C$20)^2
</table>
<p>
Finally, the sum of the interaction between column and row treatments
(SSI) is calculated find the difference between the interaction
means (means of the row-column combinations) and the grand mean
plus the difference between the row and column treatment
means and the grand mean.  Follow the simplified computational formula.
Since there are 2 rows and 3 columns, there should be 6 values labeled
&#147;I&#148; in the worksheet:
<p align="center">
<table class="left simple">
<tr><td>H8=(C8-$F$8-C$17+$C$20)^2 <td>I8=(D8-$F$8-D$17+$C$20)^2
    <td>J8=(E8-$F$8-E$17+$C$20)^2
<tr><td>H14=(C14-$F$14-C$17+$C$20)^2 <td>I14=(D14-$F$14-D$17+$C$20)^2
    <td>J14=(E14-$F$14-E$17+$C$20)^2
</table>

<p>
With the elements of all sums of squares in place, it remains to
create cells (not shown in the table above) that represent the sums
of the squares for each component:

<p align="center">
<table class="left simple">
<tr><td>SSR <td>= 3 * 3 * SUM(H7, H13)     <td>= 600.89
<tr><td>SSC <td>= 3 * 2 * SUM(H17:J17)     <td>= 44.444
<tr><td>SSI <td>= 3 * SUM(H8:J8, H14:J14)  <td>= 16.444
<tr><td>SSE <td>= SUM(L3:N5, L9:N11)       <td>= 349.33
<tr><td colspan="2">SUM(SSR, SSC, SSI, SSE)<td>= 1011.1
<tr><td colspan="3">&nbsp;
<tr><td>SST <td>= SUM(H3:J5, H9:J11)       <td>= 1011.1
</table>
<p>
All that remains is to construct the ANOVA table:
<p align="center">
<table align="center">
<tr><th>Source of variation <th>df <th>SS     <th>MS     <th><i>F</i> <th><i>p</i>
<tr><td>Temp.               <td>1  <td>600.89 <td>600.89 <td>20.64    <td>&lt;&lt;0.0001
<tr><td>Oxygen              <td>2  <td>44.44  <td>22.22  <td>0.76     <td>0.487
<tr><td>Temp &times; Oxygen <td>2  <td>16.44  <td>8.22   <td>0.28     <td>0.759
<tr><td>Error               <td>12 <td>349.33 <td>29.11
<tr><td>Total               <td>17 <td>1011.11
</table>
<p>
We see from the data that effects of temperature appear to be
significant while effects of oxygen nor effects taken together.
<h2>Plotting Means</h2>
After finding a significant effect (and even if not finding), one can
plot the means of the row-column treatment/interactions.

<h1>Analysis of Covariance (ANCOVA)</h1>
The analysis of covariance combines the power of linear regression,
which shows the linear relationship between quantitative
variables <i>x</i> and <i>y</i>, amd single factor ANOVA, which tests
whether means are equal.  What ANCOVA does is test whether the slopes
of all fitted lines are equal.
<p>
Another description is quoted from a web source:
<blockquote>
Analysis of covariance is used to test the main and interaction
effects of categorical variables on a continuous dependent variable,
controlling for the effects of selected other continuous variables
which covary with the dependent.The control variable is called
the &#147;covariate.&#148;  There may be more than one covariate.
One may also perform planned comparison or post hoc comparisons
to see which values of a factor contribute most to the explanation
of the dependent.
</blockquote>
Covariates are typically control variables.  The error associated
with these variables is determined to see if the dependent variables
show any true effects.  The <i>F</i> test is the statistic of interest
in ANCOVA and will be used for each main and interaction effect.
<p>
ANCOVA results are used to produce <b>adjusted means</b>, which are
compared if the <i>F</i> test is significant.  The covariates can be
better understood if original and adjusted means are examined.
<p>
A <b>sphericity test</b> is done in the special case of repeated measures
ANCOVA;  the test is for homogeneity of variance, which is always the case
when there are two levels.  Sphericity is an assumption of ANCOVA, and when
sphericity is tested and fails, a correction such as Wilks Lambda,
Pillai's Trace, Hotelling-Lawley Trace, Roy's Greatest Root.
<p>

</body>
</html>

