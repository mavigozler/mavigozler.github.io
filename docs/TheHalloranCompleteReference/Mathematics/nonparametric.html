<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
        "http://www.w3.org/TR/REC-html40/loose.dtd">
<html lang="en">
<head>
<title>Nonparametric Statistics</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="Author" content="S. M. Halloran">
<meta name="Keywords" content="nonparametric, statistics, Mann-Whitney, Wilcoxon">
<link href="stats.css" rel="stylesheet">
<style>
   .smaller { font-size : smaller ; }
	.author, .pages, .pubyear { font : normal 100% serif; }
	.journal { font : oblique 100% serif; }
	.volnum { font : bold 100% serif; }
</style>
</head>

<body>
<p id="title">
Nonparametric Statistics
<p>
Data sets obtained concerning subjects from appropriate populations
where there is an interesting question being investigated should preferably
be characterized using <i>parametric</i> statistics.  The <i>mean</i>
and measure of the data set variation (typically the <i>standard
deviation</i>) are the most common parameters characterizing such data,
and the parametric tests for comparing means (e.g., Student's <i>t</i>) and
variation (e.g., chi square, F test) do a better job of finding differences
in data where they exist as opposed to the class of tests which are
nonparametric tests.  Where possible, use tests of parametric statistics
in preference to tests of nonparametric statistics when analyzing data.
<p>
So why use nonparametric statistics at all?
<p>
In order to use parametric statistics, one must meet the minimal
assumptions.  The first and foremost assumption required of all
parametric statistics is that the data set being characterized
be <i>normally distributed</i>.


<H1>Diagnostic Evaluation</H1>
Clinical laboratory (medical) tests are examples of tests performed which
try to determine if a particular clinical condition or disease exists.
<p>
Suppose 45 people are selected at random, and of those, 20 are found to
have a particular condition (it could be a disease).
A new test has been developed for the purpose of
detecting whether the condition is present in a person, and all 45 people are
given this test.  Of the 20 with the condition, the test identified 19 as
having the disease.  Of the other 25 without the condition, 3 were reported
by the test to have the condition.
<p>
How can the value of this test be characterized? One way to characterize
this test is given in the table below:

<div align="center">
<table>
<col style="text-align:left;">
<tr>
  <th rowspan="2">Test Result
  <th colspan="2">Condition (Disease)
<tr>
  <th>Yes
  <th>No
<tr>
  <td>Positive<td>19<br><span class="smaller">&#147;True Positives&#148;</span>
              <td> 1<br><span class="smaller">&#147;False Negatives&#148;</span>
<tr>
  <td>Negative<td> 3<br><span class="smaller">&#147;False Positives&#148;</span>
              <td>22<br><span class="smaller">&#147;True Negatives&#148;</span>
</TABLE>
</DIV>

Expressing the data in this way, a number of concepts which can be
given as parameters characterizing the value of the test can now be
explained below.

<H2>Sensitivity</H2>
The <em>sensitivity</em> of the test is its ability to detect
all those with the condition.  That is, the test above would be
&#147;100% sensitive&#148; if it had detected all 20 people who had
the conditon.  But since it detected in only 19, the test has a 19/20 or
95% sensitivity.
<pRE>
                              number of those with the condition
                                      detected by the test
                sensitivity = ---------------------------------
                              number of those with the conditon
</PRE>
Or,
<pRE>

                                       true positives
                sensitivity = ---------------------------------
                               true positives + false negatives
</PRE>
Note that the sum of the true positives and false negatives is the same
as saying &#147;all those who should test positive.&#148;

<H2>Specificity</H2>
The <em>specificity</em> of the test is its ability to discriminate
among those who do not have the condition.  That is, the test above would be
&#147;100% specific&#148; if it had ruled out, or reported as
&#147;negative&#148, all 25 people who did not have the condition.
The test showed 3 people as having the condition when they really did not,
so the test has a 22/25 or 88% specificity.
<pRE>
                              number of those not having the condition
                                  which the test correctly identifies
                specificity = ----------------------------------------
                              number of those not having the conditon
</PRE>
Or,
<pRE>

                                       true negatives
                specificity = ---------------------------------
                               true negatives + false positives
</PRE>
Note that the sum of the true negatives and false positives is the same
as saying &#147;all those who should test negative.&#148;
<p>
Needless to say, it would be excellent if a test were 100% sensitive
and 100% specific, but it is a rare test that has this ability to
be so perfect.  Those who develop or use tests need to strike a balance
of some sort, which depends on the goal they have.  If there is a desire
for a test to true to catch all people with a condition and then worry
later about ruling out those who do not really have it, then a test
is developed to be very sensitive and not very specific.  But sometimes
there is a need to develop a test that is highly specific as well, because
news that a test is positive (especially if the condition under
study is potentially fatal) could be psychologically devastating, so much
so that the stress of knowing about the condition might kill or debilitate
rather than the condition itself.

<H2>Positive Predictive Value</H2>
This is the probability that a positive test result is really a true
positive.  This is expressed in the following formula:
<pRE>
                            number of those testing positive
            positive           and having the condition
           predictive = ----------------------------------------
             value          number of those testing positive
</PRE>
Or,
<pRE>
            positive                true positives
           predictive = ----------------------------------------
             value          true positives + false positives
</PRE>
For this test in the example above, the PPV is 19/(19 + 3) = 86.3%.
This means that for every 100 positive tests performed, between 86 and 87
will be truly positive, or rather, the positive tests truly identify the
presence of a condition (disease).
<p>
Another parameter, which is the <b>false-positive rate</b> is the
number of false positives found among all testing positive, or
3/(19 + 3) = 13.7%, which is 1 &#150; PPV.

<H2>Negative Predictive Value</H2>
This is the probability that a negative test result is really a true
negative.  This is expressed in the following formula:
<pRE>
                            number of those testing negative
            negative           and NOT having the condition
           predictive = ----------------------------------------
             value          number of those testing negative
</PRE>
Or,
<pRE>
            negative                true negatives
           predictive = ----------------------------------------
             value          true negatives + false negatives
</PRE>
For this test in the example above, the NPV is 22/(22 + 1) = 95.7%.
This means that for every 100 negative tests performed, about 96
will be truly negative, or rather, the negative tests truly identify the
absence of a condition.
<p>
Another parameter, which is the <b>false-negative rate</b> is the
number of false negatives found among all those testing negative, or
1/(22 + 1) = 4.3%, which is 1 &#150; NPV.

<H2>Likelihood</H2>
What is the <i>likelihood</i> of having the condition
when a test result is positive?  That is, how many more times is it
probable that the disease is present when the test is positive than
when it is negative?
<p>
Taken is parts, what is the probability of testing positive and having
the condition?  This is the <i>sensitivity</i> of the test, or
the true positives divided by all who should test positive (true positives
plus false negatives).  What is the probability of testing positive
and <strong>not</strong> having the condition?  This is the false positives
divided by all those show should test negative (true negatives plus
false positives), which happens to be 1 &#150; specificity.
<p>
The likelihood ratio is the probability of testing positive and
actually having the condition divided by the probability of testing positive
and <strong>not</strong> having the condition.  Expressed as a formula
as already described above, this is:
<pRE>
                        true positives
                --------------------------------
                true positives + false negatives         sensitivity
 likelihood = -------------------------------------  = ---------------
                       false positives                 1 - specificity
                --------------------------------
                true negatives + false positives
</PRE>


<h1>Nonparametric Statistics</h1>

Estimation and hypothesis tests conducted on population parameters
are called parametric tests.  Inferential procedures not concerned
with population parameters (e.g., chi-square test of independence)
are nonparametric procedures.  Many statistical procedures have
been developed which do not assume populations are normally
distributed.  This is unlike the use of the <i>t</i> test,
analysis of variance, and inferential procedures regarding
regression and correlation analysis.  Procedure which are
nonparametric and do not depend on the functional form of
the parent population distribution are called
<i>distribution-free</i>.

<p>
What are the advantages and disadvantages of using nonparametric
statistics?

<h3>Advantages</h3>

<ol>
<li>Usually based on minimum of assumptions, reducing the
  chance of being improperly used.
<li>Arithmetic computations are quick and dirty, often saving time.
<li>Procedures are easily understood by novices using statistics.
<li>Can be used on a wider array of data.
</ol>

<h3>Disadvantages</h3>

<ol>
<li>Often used in cases where use of parametric procedures is
  not only possible but appropriate.
<li>Some calculations are tedious and laborious (especially
  true of interval-estimation procedures).
</ol>

<p>
When should you use nonparametric procedures?
<ol>
<li>When hypothesis testing does not involve a population parameter.
<li>When the data consists of objects such as frequency counts or
  ranks rather than measured data such as height, weight, test scores.
<li>When assumptions necessary for the valid use of parametric
  procedures are not met.  The discernment of whether all assumptions
  are met for a procedure is the hardest thing to determine.
<li>When results are needed in a hurry.  These tests are often
  easy to apply.
</ol>

<h2>Types of Measurements</h2>

<h3>Nominal scale</h3>
A very weak (the weakest of the four here) measurement.
This involves distinguishing objects by an arbitrarily
assigned name, which is really to class things into a group.
Assignments are often very subjective.  Examples include
classifying mental illness into categories of schizophrenia,
manic-depressive psychosis, psychoneurosis, etc.  Numerals
may be used as well;  a good example is the assignment of
numbers to members of an athletic team.

<h3>Ordinal scale</h3>
When an object has <i>more</i> or <i>less</i> of some
characteristic of interest, we relate that object on an
ordinal scale.  Objects may be ordered or <i>ranked</i>
relative to this characteristic.  Examples include ranking
families by socioeconomic status, students on exam scores.
The order does not really measure how much objects differ
from one another, so we do not know how far apart the first
was from the second, the second from the third, as so on.

<h3>Interval scale</h3>
As discussed in the ordinal scale, this scale allows us
to determine the magnitude of difference between objects
or events rather than just their rank.

<h3>Ratio scale</h3>
This is the strongest of the four measurement types shown here.
Use of this type of value allows us to determine difference or
fold-difference between values, as for the interval scale,
but this scale requires a true zero or reference point as well.
The zero point is not a value of zero, but the absence of the
character.  For example, 0&deg; F does not indicate the absence of
temperature, but a weight value of 0 indicates the absence of weight.

<h2>Binomial Test</h2>

Based on a binomial distribution and therefore does testing
on population proportions.  Good examples where this test may
apply are testing a null hypothesis about proportion of
college students with right-wing attitudes, proportion of
high school students who smoke.  The key word in these studies
is <i>proportion</i>, a <i>fraction</i> of a population.
Both one- and two-sided hypothesis tests apply.

<p align="center">
<table  class="eqnserif">
<col style="width:2em;">
<col style="width:5em;">
<col style="width:5em;">
<col style="width:5em;">
<tr>
 <td>(a) <td><i>H</i><sub>0</sub>: <i>p</i> = <i>p</i><sub>0</sub>
  <td><i>H</i><sub>1</sub>:  <i>p</i> &ne; <i>p</i><sub>0</sub>
  <td>(two-sided)
<tr>
 <td>(b) <td><i>H</i><sub>0</sub>: <i>p</i> &ge; <i>p</i><sub>0</sub>
 <td><i>H</i><sub>1</sub>:  <i>p</i> &lt; <i>p</i><sub>0</sub>
 <td>(one-sided)
<tr>
 <td>(c) <td><i>H</i><sub>0</sub>: <i>p</i> &le; <i>p</i><sub>0</sub>
 <td><i>H</i><sub>1</sub>:  <i>p</i> &gt; <i>p</i><sub>0</sub>
 <td> (one-sided)
</table>

<p>
To apply the test, the following assumptions must be met:

<ol>
<li>The observations for all <i>n</i> must be classified as
  either <i>having</i> or <i>not having</i> the characteristic.
<li>The <i>n</i> observations must be independent.
<li>The probability <i>p</i> of an observation having or not
  having a characteristic remains constant during the sampling procedure.
</ol>

<p>
Note that the last assumption usually means you have to have a large
population from which you are sampling so the probability does not
change signficantly.

<p>
If a high school physical education director wants to conclude that
less than 40% of athletes participate in sports because of health
and physical fitness, he needs to set up a test with
<i>H</i><sub>0</sub>: <i>p</i>&nbsp;&ge;&nbsp;0.40 and
<i>H</i><sub>1</sub>1:  <i>p</i>&nbsp;&lt;&nbsp;0.40.
He thinks &alpha; = 0.05 is acceptable.  He now interviews
16 student athletes and found only two (in his judgment)
participated for health reasons.  What is the probability
that 2 or fewer athletes drawn from <i>n</i> = 16 athletes
and assuming the probability is 0.40?  If we consult a table
for that purpose, the probability is 0.0183, and since
0.0183&nbsp;&lt;&nbsp;0.05, we reject <i>H</i><sub>0</sub> and
accept the alternative that fewer than 40% of students
participate in sports for health reasons.

<p>
Note that when <i>n</i> is large and neither <i>p</i> or
1 &minus; <i>p</i> is close to zero or one that the binomial
sample is approximately normally distributed.  This is true
when <i>np</i> and <i>n</i>(1 &minus; <i>p</i>) are both greater
than 5.

<h2>One-Sample Runs Test</h2>

A <i>run</i> can be defined as a sequence of like observations
preceded and followed by a different type of observation or by
none at all.  Suppose that on a Monday morning, male and female
applicants for drivers&#146; licenses present themselves to the
issuer of licenses in this order:

<p>
FF M FF M FF MM F MMM FFFF

<p>
Nine runs are found in this sequence (separated by spaces for
clarity).  The null hypothesis will be that the sequence is
random.  In truth if there are too few or too many runs,
the sequence is not likely to be considered random.  Look at
an example of too few runs:

<p>
FFFFFFFFFFF MMMMMMM

<p>
Here is the same 18 females and males, but in only two runs.
This is clearly not random.

<p>
FF M FF M FF M F M F M F M F M F

<p>
Here, there is a question about the randomness as well, for the
alternating nature indicates that too many runs is something
controlled.

<p>
The only assumption regarding the valid use of this test is
that each observation be classifiable as belonging to one of
two types of observation.

<p>
The test is performed as follows:

<ol>
<li>Designate observations of one type as <i>n</i><sub>1</sub>
and the other type as <i>n</i><sub>2</sub>.  The total sample
size is <i>n = n</i><sub>1</sub> + <i>n</i><sub>2</sub>.
<li>Determine the number of runs <i>r</i>.
<li>Now consult the appropriate table of testing the randomness
of runs using the appropriate level of significance.  The table
in the book presents the significance only for a level of
significance &alpha; = 0.05.
</ol>

In the example above, <i>n</i><sub>1</sub> = number of females = 11,
<i>n</i><sub>2</sub> = number of males = 7, and <i>r</i> = 9.
The critical values from the runs table are 5 and 14, meaning
that <i>r</i> must be 5 &lt; <i>r</i> &lt; 14.
Since 5 &lt; 9 &lt; 14, <i>H</i><sub>0</sub> can not be rejected
and we accept the finding that the sequence is random.

<p>
The table is for small sample sizes of <i>n</i><sub>1</sub> or
<i>n</i><sub>2</sub>.  When <i>either n</i><sub>1</sub> or
<i>n</i><sub>2</sub> are larger than 20, the table can not
be consulted.  Instead signficance can be determined as
approximately normally distributed.

<p align="center">
<table  class="eqnserif">
<tr>
 <td rowspan="2"><i>z</i> = <td>
  <table  class="eqnserif">
   <tr><td rowspan="2"><i>r</i> &minus; <span style="font-size:200%;">[</span>
    <td>2<i>n</i><sub>1</sub><i>n</i><sub>2</sub>
    <td rowspan="2">+ 1<span style="font-size:200%;">]</span>
   <tr><td style="border-top:1px solid black;">
      <i>n</i><sub>1</sub> + <i>n</i><sub>2</sub>
  </table>
<tr><td style="border-top:1px solid black;">
  <table  class="eqnserif">
  <tr><td rowspan="2"><span style="font-size:200%;">&radic;</span>
   <td style="border-top:1px solid black;">2<i>n</i><sub>1</sub><i>n</i><sub>2</sub>
    (2<i>n</i><sub>1</sub><i>n</i><sub>2</sub> &minus; <i>n</i><sub>1</sub>
       &minus; <i>n</i><sub>2</sub>)
  <tr><td style="border-top:1px solid black;">(<i>n</i><sub>1</sub> +
    <i>n</i><sub>2</sub>)<sup>2</sup>
    (<i>n</i><sub>1</sub> + <i>n</i><sub>2</sub> &minus; 1)
  </table>
</table>

<p>
It should be noted that

<table style="margin-left:5%;margin-top:1em;" class="eqnserif">
<tr><td style="border-bottom:1px solid black;">
      2<i>n</i><sub>1</sub><i>n</i><sub>2</sub>
<td rowspan="2">+ 1
<tr><td>
  <i>n</i><sub>1</sub> + <i>n</i><sub>2</sub>
</table>

<p>
is the mean and

<p>
<table style="margin-left:5%;margin-top:1em;" class="eqnserif">
<tr><td rowspan="2" style="font-size:200%;">&radic;
  <td style="border-top:1px solid black;">
      2<i>n</i><sub>1</sub><i>n</i><sub>2</sub>(2<i>n</i><sub>1</sub><i>n</i><sub>2</sub>
    &minus; <i>n</i><sub>1</sub> &minus; <i>n</i><sub>2</sub>)
<tr><td style="border-top:1px solid black;">
  (<i>n</i><sub>1</sub> + <i>n</i><sub>2</sub>)<sup>2</sup>(<i>n</i><sub>1</sub>
   + <i>n</i><sub>2</sub> &minus; 1)
</table>

<p>
is the standard deviation of the sampling distribution of <i>r</i>.

<p>
For an example, the win-loss record of a basketball team having
a 60 game schedule is:

<p>
WWWWWW L WWWWWW L WWWWWWW LLL WW LLLL W LLLL W LLLL WW LL WWWW L WWWW L WWWWWW

<p>
The null <i>H</i><sub>0</sub> hypothesis as always should be that
the sequence is random, while the alternative <i>H</i><sub>1</sub>
will be that it is not random.  The number of wins is 39,
the number of losses is 21, and the number of runs was 19.
We apply the equation:

<p align="center">
<table class="eqnserif">
<tr>
 <td rowspan="2"><i>z</i> = <td>
  <table class="eqnserif">
   <tr><td rowspan="2">19 &minus; <span style="font-size:200%;">[</span>
    <td>2(39)(21)
    <td rowspan="2">+ 1<span style="font-size:200%;">]</span>
   <tr><td style="border-top:1px solid black;">39 + 21
  </table>
 <td rowspan="2"> = &minus;2.67
<tr><td style="border-top:1px solid black;">
  <table class="eqnserif">
  <tr><td rowspan="2"><span style="font-size:200%;">&radic;</span>
   <td style="border-top:1px solid black;">2(39)(21)[2(39)(21) &minus; 39
       &minus; 21]
  <tr><td style="border-top:1px solid black;">(39 + 21)<sup>2</sup>
    (39 + 21 &minus; 1)
  </table>
</table>

<p>
Since &minus;2.67 &lt; &minus;1.96, <i>H</i><sub>0</sub> is rejected
and the sequence is not random.

<h2>Spearman Rank Correlation Coefficient</h2>

This test generally used when we cannot use the normal correlation
methods because certain assumptions regarding parametric correlation
are not met.  For a case where we observe a seeming correlation between
two variables, <i>X</i> and <i>Y</i>, but where we can not use standard
correlation analysis, the sample values X and Y are assigned ranks, if
the original data is itself not already ranked.  The null hypothesis
and its alternative must be expressed in this fashion:

<p align="center">
<table class="eqnserif">
<col style="width:2em;">
<col style="text-align:left;">
<tr>
 <td rowspan="2">(a)
 <td><i>H</i><sub>0</sub>:  <i>X</i> and <i>Y</i> are mutually independent
<tr><td><i>H</i><sub>1</sub>:  Either large values of <i>X</i> tend
   to be paired with large values of <i>Y</i>, or large values of
   <i>X</i> tend to be paired with small values of <i>Y</i>.
<tr>
 <td rowspan="2">(b)
 <td><i>H</i><sub>0</sub>: <i>X</i> and <i>Y</i> are mutually
   independent
<tr><td><i>H</i><sub>1</sub>:  Large  values of <i>X</i> tend to
  be paired with large values of <i>Y</i>
<tr>
 <td rowspan="2">(c)
 <td><i>H</i><sub>0</sub>: <i>X</i> and <i>Y</i> are
  mutually independent
<tr><td><i>H</i><sub>1</sub>:  Large values of <i>X</i> tend to be
  paired with small values of <i>Y</i>.
</table>

<p>
The first of these is two-sided, the others one-sided.  Note that
in order to use this test, X and Y must meet the test of independence
and both must be continuous.
<p>
<b>Method</b>.
(1) To use the test, the pairs of <i>X</i> and <i>Y</i> must be
assigned ranks of 1 to the smallest to <i>n</i> for the largest.
Ranking is done within the variable&#146;s class.  (2) For each pair,
the difference in rank is calculated:  <i>d</i><sub>i</sub> =
rank of <i>X</i><sub>i</sub> &minus; rank of <i>Y</i><sub>i</sub>.
(3) The square of each <i>d</i><sub>i</sub> is taken and
&sum; <i>d</i><sub>i</sub><sup>2</sup> is calculated.
(4) From the sum of the squares of the differences, the
test statistic is calculated:

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>r<sub>S</sub></i> = 1 &minus;
  <td style="border-bottom:1px solid black;">6&sum;<i>d<sub>i</sub></i><sup>2</sup>
<tr><td><i>n</i>(<i>n</i><sup>2</sup> &minus; 1)
</table>


<p>
(5) For <i>n</i> between 4 and 30, the computed <i>r</i><sub>S</sub>
is compared to a critical value <i>r</i><sub>S</sub><sup>*</sup> given in
an appropriate table.  When a two-sided test is done, the table
allows one to determine the level of significance and to reject
<i>H</i><sub>0</sub> when <i>r</i><sub>S</sub> &gt;
<i>r</i><sub>S</sub><sup>*</sup> or <i>r</i><sub>S</sub> &lt;
&minus;<i>r</i><sub>S</sub><sup>*</sup>. <i>r</i><sub>S</sub><sup>*</sup>
in the table must be found keeping in mind that &alpha;/2 should be used.
In case of hypothesis (b), reject <i>H</i><sub>0</sub> when
<i>r</i><sub>S</sub> &gt; <i>r</i><sub>S</sub><sup>*</sup> for
&alpha; and <i>n</i>.  For (c) reject <i>H</i><sub>0</sub> when
<i>r</i><sub>S</sub> &lt;  &minus;<i>r</i><sub>S</sub><sup>*</sup>.

<p>
(6) When <i>n</i> &ge; 10, the test statistic

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>t</i> = <i>r</i><sub>S</sub>
   <span style="font-size:200%;">&radic;</span>
 <td style="border-top:1px solid black;"><i>n</i> &minus; 2
<tr><td style="border-top:1px solid black;">1 &minus; <i>r</i><sub>S</sub><sup>2</sup>
</table>

<p>
can be used with Student&#146;s <i>t</i> distribution and using
<i>n</i> &minus; 2 degrees of freedom.  The test statistic

<p align="center">
   <i>z</i> = <i>r</i><sub>S</sub>&radic;<span class="overbar"><i>n</i> &minus; 1</span>

<p>
and can be compared for significance with appropriate values on
the standard normal distribution; approximation is good for values
as small as <i>n</i> = 10.

<p>
When observations of <i>X</i> and <i>Y</i> are tied&#151;for this
test can only be done on untied observations&#151;if the number
of ranks are small, one can assign the mean of the ranks for
which they are tied.  If the number of ties is large relative
to <i>n</i>, one can compute

<p align="center">
<table class="eqnserif">
<tr><td rowspan="2"><i>T</i> =
 <td style="border-bottom:1px solid black;"><i>t</i><sup>3</sup> &minus; <i>t</i>
<tr><td>12
</table>

<p>
Next compute <i>r</i><sub>S</sub>:

<p align="center">
<table class="eqnserif">
<tr>
 <td rowspan="2"><i>r</i><sub>S</sub> =
 <td style="border-bottom:1px solid black;" colspan="2">
   <span style="font-size:150%;">&sum;</span><i>x</i><sup>2</sup> +
   <span style="font-size:150%;">&sum;</span><i>y</i><sup>2</sup> &minus;
   <span style="font-size:150%;">&sum;</span><i>d<sub>i</sub></i><sup>2</sup>
<tr><td>2<span style="font-size:150%;text-align:right;">&radic;</span>
   <td style="border-top:1px solid black;margin:0;">
   <span style="font-size:150%;">&sum;</span><i>x</i><sup>2</sup><span
    style="font-size:150%;">&sum;</span><i>y</i><sup>2</sup>
</table>

<p>
where:

<p>
<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<col><col style="text-align:center;">
<tr>
 <td rowspan="2">&sum; <i>x</i><sup>2</sup> =
 <td  style="border-bottom:1px solid black;">
     <i>n</i><sup>3</sup> &minus; <i>n</i>
 <td rowspan="2">&minus; &sum; <i>T<sub>x</sub></i>
 <td>
<tr><td>12
</table>
<table class="eqnserif" style="margin-left:15%;">
<col><col style="text-align:center;">
<tr>
 <td rowspan="2">&sum; <i>y</i><sup>2</sup> =
 <td  style="border-bottom:1px solid black;">
     <i>n</i><sup>3</sup> &minus; <i>n</i>
 <td rowspan="2">&minus; &sum; <i>T<sub>y</sub></i>
 <td>
<tr><td>12
</table>
<table class="eqnserif" style="margin-left:15%;">
<tr>
 <td>&sum; <i>T<sub>x</sub></i> =
 <td colspan="3">sum of values of <i>T</i> for the tied ranks occurring
   in <i>X</i>
<tr>
 <td>&sum; <i>T<sub>y</sub></i> =
 <td colspan="3">sum of values of <i>T</i> for the tied ranks occurring
   in <i>Y</i>
</table>
<p>
Note that using the <i>r</i><sub>S</sub> equation used only for
a large number of ties when the number of ties is small will
not differ much from the computation of the <i>r</i><sub>S</sub>
statistic when ties are small.

<p>
Consider the following example:  A biologist has 10 aminals in
captivity and wants to compare an activity index and data she
has on metabolic rates.  The hypotheses are:

<table class="eqnserif">
<tr>
  <td><i>H</i><sub>0</sub>:
  <td>activity index and metabolic rates are independent
<tr>
  <td><i>H</i><sub>1</sub>:
  <td>metabolic rates increase as activity index increases
</table>

<p>
The data and ranking of data are given in the tables below:

<p align="center">
<table>
<tr>
 <th>Animal
 <th>Activity Index, <i>X</i>
 <th>metabolic Rate, <i>Y</i>
 <th>Rank of <i>X</i>
 <th>Rank of <i>Y</i>
 <th><i>d<sub>i</sub></i>
 <th><i>d<sub>i</sub></i><sup>2</sup>
<tr><td>1  <td>50   <td>0.16  <td>1   <td>1   <td>0        <td>0
<tr><td>2  <td>300  <td>0.20  <td>7   <td>6   <td>1        <td>1
<tr><td>3  <td>225  <td>0.19  <td>5   <td>4.5 <td>0.5      <td>0.25
<tr><td>4  <td>600  <td>0.25  <td>10  <td>10  <td>0        <td>0
<tr><td>5  <td>450  <td>0.23  <td>8   <td>8   <td>0        <td>0
<tr><td>6  <td>275  <td>0.19  <td>6   <td>4.5 <td>1.5      <td>2.25
<tr><td>7  <td>200  <td>0.21  <td>4   <td>7   <td>&minus;3 <td>9
<tr><td>8  <td>150  <td>0.18  <td>3   <td>3   <td>0        <td>0
<tr><td>9  <td>500  <td>0.24  <td>9   <td>9   <td>0        <td>0
<tr><td>10 <td>100  <td>0.17  <td>2   <td>2   <td>0        <td>0
<tr>
 <td colspan="6">&nbsp;
 <td>11.50 = &sum; <i>d</i><sub>i</sub><sup>2</sup>
</table>

<p>
Note that animals 3 and 6 were tied with respect to metabolic rate,
so the mean of 4 and 5 was used.

<p align="center">
<table class="eqnserif">
<tr>
 <td rowspan="2"><i>r</i><sub>S</sub> = 1 &minus;
 <td style="border-bottom:1px solid black;">
   6(11.50)
 <td rowspan="2"> = 0.93
<tr><td>10(10<sup>2</sup> &minus; 1)
</table>

<p>
The table is consulted for <i>n</i> = 10 and &alpha; = 0.05,
and the critical value (<i>r</i><sub>S</sub><sup>*</sup> for
a one-sided test is 0.5515.  Since 0.93 &gt; 0.5515,
<i>H</i><sub>0</sub> is rejected and we accept
<i>H</i><sub>1</sub>, that is, there is a correlation
between metabolic rate and activity index.

<p>
We consider a case with several ties in the data.
Twenty new employees take an aptitude test regarding
their ability to do a job, and six months later, their
supervisors were asked to rate the employeesï¿½ performance.
The hypotheses are:

<p>
<i>H</i><sub>0</sub>:  Aptitude scores and supervisor ratings are independent (do not correlate)
<br>
<i>H</i><sub>1</sub>:  High aptitude scores get high supervisor ratings

<table align="center">
<col span="6" style="width:5em;">
<tr>
 <th>Employee
 <th>Aptitude Score, <i>X</b></i>
 <th>Rank of X
 <th>Supervisor&#146;s rating, Y
 <th>Rank of <i>Y</i>
 <th><i>d<sub>i</sub></i>
 <th><i>d<sub>i</sub></i><sup>2</sup>
<tr><td>1  <td>79  <td>12  <td>86  <td>13  <td>&minus;1   <td>1
<tr><td>2  <td>80  <td>14  <td>96  <td>20  <td>&minus;6   <td>36
<tr><td>3  <td>51  <td>1   <td>53  <td>1   <td>0          <td>0
<tr><td>4  <td>76  <td>9   <td>77  <td>8.5 <td>0.5        <td>0.25
<tr><td>5  <td>99  <td>20  <td>90  <td>17.5<td>2.5        <td>6.25
<tr><td>6  <td>78  <td>11  <td>85  <td>11.5<td>&minus;0.5 <td>0.25
<tr><td>7  <td>76  <td>9   <td>88  <td>14.5<td>&minus;5.5 <td>30.25
<tr><td>8  <td>80  <td>14  <td>85  <td>11.5<td>2.5        <td>6.25
<tr><td>9  <td>52  <td>2.5 <td>55  <td>3   <td>&minus;0.5 <td>0.25
<tr><td>10 <td>55  <td>4   <td>56  <td>4   <td>0          <td>0
<tr><td>11 <td>90  <td>18  <td>88  <td>14.5<td>3.5        <td>12.25
<tr><td>12 <td>62  <td>6   <td>66  <td>6   <td>0          <td>0
<tr><td>13 <td>81  <td>16  <td>90  <td>17.5<td>&minus;1.5 <td>2.25
<tr><td>14 <td>52  <td>2.5 <td>54  <td>2   <td>0.5        <td>0.25
<tr><td>15 <td>69  <td>7   <td>58  <td>5   <td>2          <td>4
<tr><td>16 <td>76  <td>9   <td>77  <td>8.5 <td>0.5        <td>0.25
<tr><td>17 <td>58  <td>5   <td>68  <td>7   <td>&minus;2   <td>4
<tr><td>18 <td>90  <td>18  <td>89  <td>16  <td>2          <td>4
<tr><td>19 <td>80  <td>14  <td>84  <td>10  <td>4          <td>16
<tr><td>20 <td>90  <td>18  <td>95  <td>19  <td>&minus;1   <td>1
<tr><td colspan="6"> <td>124.50 = &sum; <i>d</i><sub>i</sub><sup>2</sup>
</table>


<p>
Calculating <i>r</i><sub>S</sub> without assuming a large number of ties:

<p align="center">
<table class="eqnserif">
<tr>
 <td rowspan="2"><i>r</i><sub>S</sub> = 1 &minus;
 <td style="border-bottom:1px solid black;">
   6(124.50)
 <td rowspan="2"> = 0.9064
<tr><td>20(20<sup>2</sup> &minus; 1)
</table>

<p>
The critical value is therefore 0.3789 from the table, and because
0.9064 &gt; 0.3789, we conclude there is a relationship between
supervisor&#146;s ratings and aptitude scores.  Now assume the
large number of ties (in variable X, there are three sets of
3-way ties, and one set of 2-way ties; in variable Y, there
four sets of 2-way ties):

<p>
<table class="eqnserif" style="margin-top:1em;margin-left:15%;">
<tr>
 <td rowspan="2"><i>T<sub>x</sub></i> =
 <td style="border-bottom:1px solid black;">3<sup>3</sup> &minus; 3
 <td rowspan="2">+
 <td style="border-bottom:1px solid black;">3<sup>3</sup> &minus; 3
 <td rowspan="2">+
 <td style="border-bottom:1px solid black;">2<sup>3</sup> &minus; 3
 <td rowspan="2">= 4.5
<tr>
 <td>12<td>12<td>12
</table>

<table class="eqnserif" style="margin-left:15%;">
<tr>
 <td rowspan="2"><i>T<sub>y</sub></i> = 4
 <td style="border-bottom:1px solid black;">2<sup>3</sup> &minus; 3
 <td rowspan="2">= 4.5
<tr>
 <td>12
</table>

<table class="eqnserif" style="margin-left:15%;">
 <td rowspan="2">&sum; <i>x</i><sup>2</sup> =
 <td style="border-bottom:1px solid black;">
     20<sup>3</sup> &minus; 20
 <td rowspan="2">&minus; 4.5 = 660.5
<tr><td>12
</table>

<table class="eqnserif" style="margin-left:15%;">
 <td rowspan="2">&sum; <i>y</i><sup>2</sup> =
 <td style="border-bottom:1px solid black;">
     20<sup>3</sup> &minus; 20
 <td rowspan="2">&minus; 2 = 663
<tr><td>12
</table>
<table class="eqnserif" style="margin-left:15%;">
 <td rowspan="2"><i>r</i><sub>S</sub> =
 <td style="border-bottom:1px solid black;">
     660.5 + 663 &minus; 124.50
 <td rowspan="2"> = 0.9059
<tr><td>2&radic;<span class="overbar">(660.5)(663)</span>
</table>


<p>
This value for <i>r</i><sub>S</sub> is very close to the one where
we assume no ties.  Because of the large number of ties, use of
the table is not strictly valid.  If we calculate the <i>t</i>
statistic, <i>t</i> = 9.08, and this is significant at 0.05.
When the standard normal is used, <i>z</i> = 3.95, and this
is also significant at 0.05.

<h2>Median Test</h2>

Often the question is whether the means of two populations are equal.
The median can also be used as a measure of the central tendency,
for when the mean and median coincide, inferences about the mean
apply to the median as well.

<p>
To use this test, sampling must be random and observations independent
of one another.  Data must be measured on an ordinal scale, at the
minimum.

<p>
Two samples of unequal sizes from each population are obtained.
We basically construct a 2x2 contingency table to see how values
lie around the median independently and when samples are combined.

<p align="center">
<table>
<tr>
 <th rowspan="2">Number of observations
 <th colspan="2">Sample
 <th rowspan="2">Total
<tr>
 <th>1
 <th>2
<tr>
 <td>Greater than median
 <td><i>a</i>
 <td><i>b</i>
 <td><i>a + b</i>
<tr>
 <td>Less than or equal to median
 <td style="border-bottom:1px solid black;"><i>c</i>
 <td style="border-bottom:1px solid black;"><i>d</i>
 <td style="border-bottom:1px solid black;"><i>c + d</i>
<tr>
 <td>
 <td><i>a</i> + <i>c</i> = <i>n</i><sub>1</sub>
 <td><i>b</i> + <i>d</i> = <i>n</i><sub>2</sub>
 <td><i>N</i> = <i>n</i><sub>1</sub> + <i>n</i><sub>2</sub>
</table>

<p>
As an example, suppose an educator wants to decide if freshman
students of a given high school coming from rural elementary
schools differ with students coming from urban elementary schools
insofar as their median verbal reasoning scores.  A random sample
of 15 students from rural schools and a similar sample of 18
students from urban schools is tested for verbal reasoning.
The following scores are obtained:

<table>
<tr>
 <th>School type
 <th>Raw verbal reasoing scores
<tr>
 <td>rural
 <td>66  73  63  65  80  74  69  80  63  54  42  80  57  79  52
<tr>
 <td>urban
 <td>68  91  89  60  73  90  79  97  64  62  91  69  64  89  99  76  77  96
</table>

<p>
The hypotheses are set up:

<p>
<i>H</i><sub>0</sub>:  The medians of the two populations are equal<br>
<i>H</i><sub>0</sub>:  The medians are not equal

<p>
The median of the combined scores is 73.

<table>
<tr>
 <th rowspan="2">Number of observations
 <th colspan="2">Sample
 <th rowspan="2">Total
<tr>
 <th>Rural
 <th>Urban
<tr>
 <td>Greater than 73
 <td>5
 <td>11
 <td>16
<tr>
 <td>Less than or equal to 73
 <td style="border-bottom:1px solid black;">10
 <td style="border-bottom:1px solid black;">7
 <td style="border-bottom:1px solid black;">17

<tr>
 <td>
 <td>15
 <td>18
 <td>33
</table>

<p>
We compute the chi-square test value and test for one degree of
freedom using &alpha; = 0.05.

<table class="eqnserif" style="margin-top:1em;">
 <td rowspan="2">&chi;<sup>2</sup> =
 <td style="border-bottom:1px solid black;">
     33[(5)(7) &minus; (11)(10)]<sup>2</sup>
 <td rowspan="2"> = 2.528
<tr><td>(15)(18)(17)(16)
</table>

<p>
The table for the chi-square distribution indicates
&chi;<sup>2</sup> = 2.528 is not significant and so the median test
suggest the medians may possibly be equal.

<h2>Sign Test</h2>

With the median test, there must be two independent samples
with both samples measuring the same characteristic.  When
samples are not independent but related, such as when the
investigator attempts to eliminate one or more extraneous
sources of variation (e.g., taking &quot;before&quot; and
&#147;after&#148; measurements in relation to a treatment
or matching subjects to eliminate a source of bias), and
assumptions are not met for comparing groups using the
<i>t</i> test, the researcher may consider the sign test.
The sign test is like the median test in that it measures
the central tendency to the median rather than the mean.

<p>
For this test to be valid, the data must be on at least
an ordinal scale and the related pairs of observations
must be mutually independent.

<p>
The sign test is usually used to see if the median difference
is zero.  Data pairs are collected:  (<i>x</i><sub>1</sub>,
<i>y</i><sub>1</sub>), (<i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>,
&hellip;(<i>x<sub>i</sub></i>, <i>y<sub>i</sub></i>),&hellip;,
(<i>x<sub>n</sub></i>, <i>y<sub>n</sub></i>).
Differences between the data are then calculated:
<i>x</i><sub>1</sub> &minus; <i>y</i><sub>1</sub>,
<i>x</i><sub>2</sub> &minus; <i>y</i><sub>2</sub>, &hellip;,
<i>x<sub>n</sub></i> &minus; <i>y<sub>n</sub></i>.
When <i>x<sub>i</sub></i> is smaller than <i>y<sub>i></sub></i>,
the difference is noted as minus (&minus;); when <i>x<sub>i</sub></i>
is greater than <i>y<sub>i</sub></i>, the difference is plus (+).

<p>
If it is true that a sample is normally distributed, its median
difference is likely zero.  The null hypothesis is:

<p>
<i>H</i><sub>0</sub>:  <i>P</i>(+) = <i>P</i>(&minus;) = 0.5

<p>
This should be a simple case of the binomial test.  When applying
the test, the sign occurring less is the object of the hypothesis.
Note that where <i>x<sub>i</sub></i> &minus; <i>y<sub>i</sub></i> = 0,
the pair is eliminated from the sample and not included in the sample
size total.

<p>
As an example, suppose the mental health status of retired persons
is assessed by giving some sort of test two years before and one year
after a retirement date.  15 subjects are analyzed, and their scores
are: (76, 70), (80, 75), (86, 84), (87, 90), (85, 81), (95, 95),
(97, 87), (75, 72), (87, 92), (96, 85), (98, 88), (77, 76), (80, 85),
(87, 81), (89, 84).  Evaluation of the differences shows 11 pluses (+),
3 minuses (&minus;), and 1 zero difference.

<p>
Again it is assumed the probability of a plus or minus is 0.5,
and that is the null hypothesis.  Since there are many more pluses
than minuses in this sampling, the alternative hypothesis is that
the probability of a plus is greater than a minus (in which case,
mental health status declines after retirement).

<p>
Using a binomial distribution table with <i>n</i> = 14 and
<i>p</i> = 0.5,

<table class="eqnserif" style="margin-top:1em;">
<col><col><col style="text-align:center;">
<tr>
 <td rowspan="2"><i>P</i>(<i>x</i>|<i>n</i>,<i>p</i>)
 <td rowspan="2">= <span style="font-size:200%;">(</span>
 <td><i>n</i>
 <td rowspan="2"><span style="font-size:200%;">)</span><i>p<sup>x</sup>q<sup>n&minus;x</sup>
<tr><td><i>x</i>
<tr>
 <td>
 <td rowspan="2">= <span style="font-size:200%;">(</span>
 <td>14
 <td  rowspan="2"><span style="font-size:200%;">)</span>(0.5)<sup>3</sup>(0.5)<sup>11</sup>
<tr><td><td>3
<tr><td><td colspan="3">= 0.022
</table>

<p>
This value is less than &alpha; = 0.05, the chosen level of
significance, so we reject the idea that the median difference
is zero and accept that it is positive.

<p>
For <i>n</i> &ge; 12, the binomial distribution is
approximately normal, and we may use the <i>z</i> statistic
to compute significance.

<p align="center">
<table class="eqnserif">
<tr>
 <td rowspan="2"><i>z</i> =
 <td style="border-bottom:1px solid black;">(<i>k</i> + 0.5) &minus; 0.5<i>n</i>
<tr><td>0.5&radic;<span class="overbar">n</span>
</table>

<p>
Here <i>k</i> is the number of occurrences of the less frequent sign.
Note that if we use it for the present data, <i>z</i> = &minus;1.87.
Since this is not less than &minus;1.96, we cannot reject
<i>H</i><sub>0</sub>, and therefore the median difference may be zero.

<h2>Wilcoxon Matched-Pairs Signed-Ranks Test</h2>

This test also compares related (non-independent) samples of data.
This test goes beyond the sign test in that it examines the magnitude
of the differences inasmuch as their direction (plus or minus).
The Wilcoxon test should be used in preference to the sign test
because it provides more information about the relationship.

<p>
The following assumptions hold for the valid use of this test:

<ol>
<li>Observed matched pairs are a random (bivariate) sampling
  of the variables <i>X</i> and <i>Y</i>.
<li>Differences between matched pairs are mutually independent
  values of a continuous random variable.
<li>The distribution of differences is symmetric.
<li>The differences are measured on at least an interval scale.
</ol>

<p>
The following sets of null and alternative hypotheses explain
the proper form for understanding this test:

<table class="eqnserif" style="margin-top:1em;">
<col style="width:2em;vertical-align:top;">
<col style="width:50%;">
<col style="width:30%;">
<tr>
  <td rowspan="2">(a)
  <td><i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) &ge; <i>E</i>(<i>Y</i>)
  <td>(or &mu;<sub>X</sub> &ge; &mu;<sub>Y</sub>)
<tr>
  <td><i>H</i><sub>1</sub>: <i>E</i>(<i>X</i>) &lt; <i>E</i>(<i>Y</i>)
  <td>(or &mu;<sub>X</sub> &lt; &mu;<sub>Y</sub>)
<tr>
  <td rowspan="2">(b)
  <td><i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) &le; <i>E</i>(<i>Y</i>)
  <td>(or &mu;<sub>X</sub> &le; &mu;<sub>Y</sub>)
<tr>
  <td><i>H</i><sub>1</sub>: <i>E</i>(<i>X</i>) &gt; <i>E</i>(<i>Y</i>)
  <td>(or &mu;<sub>X</sub> &gt; &mu;<sub>Y</sub>)
<tr>
  <td rowspan="2">(c)
  <td><i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) = <i>E</i>(<i>Y</i>)
  <td>(or &mu;<sub>X</sub> = &mu;<sub>Y</sub>)
<tr>
  <td><i>H</i><sub>1</sub>: <i>E</i>(<i>X</i>) &ne; <i>E</i>(<i>Y</i>)
  <td>(or &mu;<sub>X</sub> &ne; &mu;<sub>Y</sub>)
</table>


<p>
To perform the test,

<ol>
<li>Determine the difference <i>d<sub>i</sub></i> =
  <i>y<sub>i</sub></i> &minus; <i>x<sub>i</sub></i>, where
  <i>y<sub>i</sub></i> is the data item matched to an
  <i>x<sub>i</sub></i>.  Do not include pairs for which the
  difference is zero, i.e., <i>x<sub>i</sub></i> = <i>y<sub>i</sub></i>.
  Reduce <i>n</i> accordingly.
<li>Rank the absolute value of the differences <i>d<sub>i</sub></i>,
  in other words, ignore the sign during ranking.  When two or more
  <i>d<sub>i</sub></i> are tied, then assign the mean of the rank
  as the rank.  Hence, if the third, fourth, and fifth ranks would
  be assigned to a three-way tie, the mean of that rank
  (3 + 4 + 5)/3 = 4 would be assigned to all three <i>d<sub>i</sub></i>.
<li>Now assign to each <i>rank</i> the <i>sign</i> (+ or &minus;)
  of the <i>d<sub>i</sub>i</i> giving rise to that rank.
<li>Compute <i>T</i> = sum of the positively signed ranks.
<li>Compare the computed <i>T</i> with critical values provided
  in a table of the quantiles of the Wilcoxon matched-pair
  signed-ranks test.  Once you have the critical value and
  a given level of significance (&alpha;), check the following
  situations for deciding whether to reject
  <i>H</i><sub>0</sub>.
</ol>

<p>
For (a)  <i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) &ge; <i>E</i>(<i>Y</i>),
  sufficiently large <i>T</i> will lead to rejection of
  <i>H</i><sub>0</sub>.  This is reasonable since large <i>T</i>
  means the differences are larger.  Reject <i>H</i><sub>0</sub>
  if <i>T</i> is greater than <i>w</i><sub>1&minus;&alpha;</sub>.
  <i>w</i><sub>1&minus;&alpha;</sub> = <i>n</i>(<i>n</i> + 1)/2
  &minus; <i>w</i><sub>&alpha;</sub>. <i>w</i><sub>&alpha;</sub>
  is a critical value taken from the table using the desired
  level of significance and the <i>T</i> value.

<p>
For (b)  <i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) &le; <i>E</i>(<i>Y</i>),
  sufficiently small <i>T</i> will lead to rejection of
  <i>H</i><sub>0</sub>.  This is also reasonable since small
  <i>T</i> means the differences are large as well, but that
  the other variable is just smaller than the other variable.
  Reject <i>H</i><sub>0</sub> if <i>T</i> is less than
  <i>w</i><sub>&alpha;</sub>.  <i>w</i><sub>&alpha;</sub>
   is the critical value taken from the table.

<p>
For (c)  <i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) = <i>E</i>(<i>Y</i>),
either small or large <i>T</i> values will lead to rejection.
<i>H</i><sub>0</sub> is rejected either when <i>T</i> exceeds
<i>w</i><sub>1-&alpha;/2</sub> = <i>n</i>(<i>n</i> + 1)/2 &minus;
<i>w</i><sub>&alpha;/2</sub> or when <i>T</i> is less than
<i>w</i><sub>&alpha;/2</sub>.

<p>
For an instance, consider the example in the section on the
signed test regarding the mental health status of people before
and after retirement.  Recall there were 15 subjects.  The table
below shows how to compute the <i>T</i> value for this data.

<table>
<tr>
 <th rowspan="2">Subject
 <th colspan="3">Mental Health Status Score
 <th rowspan="2">Rank of |<i>d<sub>i</sub></i>|
 <th rowspan="2">Signed Ranks
<tr>
 <th>Before, <i>X</i>
 <th>After, <i>Y</i>
 <th><i>d<sub>i</sub> = y<sub>i</sub> &minus; x<sub>i</sub>
<tr><td>1   <td>76   <td>70   <td>&minus;6    <td>10.5   <td>&minus;10.5
<tr><td>2   <td>80   <td>75   <td>&minus;5    <td>7.5    <td>&minus;7.5
<tr><td>3   <td>86   <td>84   <td>&minus;2    <td>2      <td>&minus;2
<tr><td>4   <td>87   <td>90   <td>+3          <td>3.5    <td>+3.5
<tr><td>5   <td>85   <td>81   <td>&minus;4    <td>5      <td>&minus;5
<tr><td>6   <td>95   <td>95   <td>0           <td>&nbsp; <td>&nbsp;
<tr><td>7   <td>97   <td>87   <td>&minus;10   <td>12.5   <td>&minus;12.5
<tr><td>8   <td>75   <td>72   <td>&minus;3    <td>3.5    <td>&minus;3.5
<tr><td>9   <td>87   <td>92   <td>+5          <td>7.5    <td>+7.5
<tr><td>10  <td>96   <td>85   <td>&minus;11   <td>14     <td>&minus;14
<tr><td>11  <td>98   <td>88   <td>&minus;10   <td>12.5   <td>&minus;12.5
<tr><td>12  <td>77   <td>76   <td>&minus;1    <td>1      <td>&minus;1
<tr><td>13  <td>80   <td>85   <td>+5          <td>7.5    <td>+7.5
<tr><td>14  <td>87   <td>81   <td>&minus;6    <td>10.5   <td>&minus;10.5
<tr><td>15  <td>89   <td>84   <td>&minus;5    <td>7.5    <td>&minus;7.5
<tr><td colspan="5"> <td><i>T</i> = 18.5
</table>

<p>
The critical value (<i>w</i><sub>&alpha;</sub>) for a = 0.05 and
<i>n</i> = 14 is 26 for the one-sided test.  In this case, <i>T</i>
is less than <i>w</i><sub>&alpha;</sub>, and so <i>H</i><sub>0</sub>
is rejected, meaning we accept that mental health scores are better
before retirement than after (or <i>E</i>(<i>X</i>) &gt;
<i>E</i>(<i>Y</i>)).  This is the same conclusion that was
reached using the sign test.

<p>
For large samples where <i>n</i> &gt; 20, the quantile table
can not be used.  In this case the distribution approximates
the standard normal:

<table class="eqnserif" style="margin-top:1em;">
<tr>
 <td rowspan="2"><i>z</i> =
 <td style="border-bottom:1px solid black;"><i>T</i> &minus;
    [<i>n</i>(<i>n</i> + 1)]/4
<tr><td>&radic;<span class="overbar">[<i>n</i>(<i>n</i> +
  1)(2<i>n</i> + 1)]/24
</table>

<h2>The Mann-Whitney Test</h2>

The sign test and Wilcoxon matched-pairs signed-rank tests are
used for related samples.  The Mann-Whitney test is used for
inferential procedures based on two independent samples.
The test is a nonparametric alternative to the two-sample
<i>t</i> test regarding the testing of a null hypothesis
that the means of two populations are equal, and used when
assumptions regarding the <i>t</i> test can not be met.

<p>
To use the Mann-Whitney test, the following assumptions must apply.

<ol>
<li>Sampling from a population is random.
<li>The observations within a sample and between the two samples
  must be independent.
<li>The random variable is continuous in both populations.
<li>The data represent measurements on an ordinal scale (at the least).
<li>The two population distribution coefficients differ only with
  respect to location, if they differ at all.
</ol>

<p>
Suppose population 1 has a sample size <i>n</i> with ordinal
values of <i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>,
<i>x</i><sub>3</sub>, &hellip;, <i>x<sub>i</sub></i>, &hellip;,
<i>x<sub>n</sub></i>, and population 2 with sample size <i>m</i>
has values <i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, &hellip;,
<i>y<sub>i</sub></i>, &hellip;,<i>y<sub>m</sub></i>.
If <i>X</i> and <i>Y</i> represent the values of interest,
the following hypotheses are possible:

<p>(a)  <i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) = <i>E</i>(<i>Y</i>),
  <i>H</i><sub>1</sub>:  <i>E</i>(<i>X</i>) &ne; <i>E</i>(<i>Y</i>)
<p>(b)  <i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) &ge; <i>E</i>(<i>Y</i>),
  <i>H</i><sub>1</sub>:  <i>E</i>(<i>X</i>) &lt; <i>E</i>(<i>Y</i>)
<p>(c)  <i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) &le; <i>E</i>(<i>Y</i>),
  <i>H</i><sub>1</sub>:  <i>E</i>(<i>X</i>) &gt; <i>E</i>(<i>Y</i>)

<p>
The test statistic is:

<table class="eqnserif" style="margin-top:1em;">
<col><col style="text-align:center;">
<tr>
 <td rowspan="2"><i>T</i> = <i>S</i> &minus;
 <td style="border-bottom:1px solid black;"><i>n</i>(<i>n</i> + 1)
<tr><td>2
</table>

<p>
where <i>S</i> is the sum of the ranks assigned to the
<i>x<sub>i</sub></i>.  Ranks are assigned as follows:
after combining both groups of values, the smallest is
assigned a rank of 1, the next smallest a rank of 2 and
so on until all <i>n</i> + <i>m</i> values are assigned ranks.
For tied observations, the mean of the rank positions they
would have occupied had there been no ties is assigned.

<p>
A decision to reject <i>H</i><sub>0</sub> is based on the level
of significance (&alpha;) and depends on the value of <i>T</i>.
For the case of

<p>
(a)  <i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) = <i>E</i>(<i>Y</i>),
either certain small or large values of <i>T</i> lead to its rejection.
<i>H</i><sub>0</sub> is rejected when <i>T</i> &lt;
<i>w</i><sub>&alpha;/2</sub> or <i>T</i> &gt;
<i>w</i><sub>1&minus;&alpha;/2</sub>.  <i>w</i><sub>&alpha;/2</sub>
is a critical value of <i>T</i> found in tables for the test,
and <i>w</i><sub>1&minus;&alpha;/2</sub> is calculated as
<i>w</i><sub>1&minus;&alpha;/2</sub> = <i>nm</i> &minus;
<i>w</i><sub>&alpha;/2</sub>.

<p>
(b)  <i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) &ge; <i>E</i>(<i>Y</i>),
  small values of <i>T</i> will likely cause rejection.
  Reject <i>H</i><sub>0</sub> when <i>T</i> &lt; <i>w</i><sub>&alpha;</sub>.
  Obtain <i>w</i><sub>&alpha;</sub> from a table for the test using
  <i>n</i>, <i>m</i>, and &alpha;.

<p>
(c)  <i>H</i><sub>0</sub>: <i>E</i>(<i>X</i>) &le; <i>E</i>(<i>Y</i>),
  large values of <i>T</i> will lead to rejection.  If <i>T</i> is
  greater than <i>w</i><sub>1&minus;&alpha;</sub>, reject
  <i>H</i><sub>0</sub>.  For <i>w</i><sub>1&minus;&alpha;</sub>,
  use <i>w</i><sub>1&minus;&alpha;</sub> = <i>nm</i> &minus;
  <i>w</i><sub>&alpha;</sub>

<p>
The example of the educator&#146;s question in the section on the
median test will be used here.

<p>
First a table of ranks needs to be created from this data:

<table align="center">
<tr>
 <th>Rural, <i>X</i>
 <th>Urban, <i>Y</i>
 <th>Rank
 <th style="background-color:#aaa;">
 <th>Rural, <i>X</i>
 <th>Urban, <i>Y</i>
 <th>Rank
 <th style="background-color:#aaa;">
 <th>Rural, <i>X</i>
 <th>Urban, <i>Y</i>
 <th>Rank
<tr><td>42     <td>&nbsp;  <td>1    <td style="background-color:#aaa;">
    <td>66     <td>&nbsp;  <td>12   <td style="background-color:#aaa;">
    <td>80     <td>&nbsp;  <td>24
<tr><td>52     <td>&nbsp;  <td>2    <td style="background-color:#aaa;">
    <td>&nbsp; <td>68      <td>13   <td style="background-color:#aaa;">
    <td>80     <td>&nbsp;  <td>24
<tr><td>54     <td>&nbsp;  <td>3    <td style="background-color:#aaa;">
    <td>69     <td>&nbsp;  <td>14.5 <td style="background-color:#aaa;">
    <td>80     <td>&nbsp;  <td>24
<tr><td>57     <td>&nbsp;  <td>4    <td style="background-color:#aaa;">
    <td>&nbsp; <td>69      <td>14.5 <td style="background-color:#aaa;">
    <td>&nbsp; <td>89      <td>26.5
<tr><td>&nbsp; <td>60      <td>5    <td style="background-color:#aaa;">
    <td>73     <td>&nbsp;  <td>16.5 <td style="background-color:#aaa;">
    <td>&nbsp; <td>89      <td>26.5
<tr><td>&nbsp; <td>62      <td>6    <td style="background-color:#aaa;">
    <td>&nbsp; <td>73      <td>16.5 <td style="background-color:#aaa;">
    <td>&nbsp; <td>90      <td>28
<tr><td>63     <td>&nbsp;  <td>7.5  <td style="background-color:#aaa;">
    <td>74     <td>&nbsp;  <td>18   <td style="background-color:#aaa;">
    <td>&nbsp; <td>91      <td>29.5
<tr><td>63     <td>&nbsp;  <td>7.5  <td style="background-color:#aaa;">
    <td>&nbsp; <td>76      <td>19   <td style="background-color:#aaa;">
    <td>&nbsp; <td>91      <td>29.5
<tr><td>&nbsp; <td>64      <td>9.5  <td style="background-color:#aaa;">
    <td>&nbsp; <td>77      <td>20   <td style="background-color:#aaa;">
    <td>&nbsp; <td>96      <td>31
<tr><td>&nbsp; <td>64      <td>9.5  <td style="background-color:#aaa;">
    <td>79     <td>&nbsp;  <td>21.5 <td style="background-color:#aaa;">
    <td>&nbsp; <td>97      <td>32
<tr><td>65     <td>&nbsp;  <td>11   <td style="background-color:#aaa;">
    <td>&nbsp; <td>79      <td>21.5 <td style="background-color:#aaa;">
    <td>&nbsp; <td>99      <td>33
</table>

<p>
The null hypothesis is <i>H</i><sub>0</sub>0:
<i>E</i>(<i>X</i>) = <i>E</i>(<i>Y</i>) and the alternative
<i>H</i><sub>1</sub>: <i>E</i>(<i>X</i>) &ne; <i>E</i>(<i>Y</i>).
&alpha; = 0.05.

<p>
The sum of the ranks (<i>S</i>) is calculated to be 190.5.
Our test statistic is therefore:

<p align="center">
<table class="eqnserif" style="margin-top:1em;">
<col><col style="text-align:center;">
<tr>
 <td rowspan="2"><i>T</i> = 190.5 &minus;
 <td style="border-bottom:1px solid black;">15(15 + 1)
 <td rowspan="2"> = 70.5
<tr><td>2
</table>

<p>
The critical values of <i>T</i> for the two-sided test will be
81 and 189, determined from a table in which <i>m</i> = 18,
<i>n</i> = 15, and &alpha;/2 = 0.025.  Note that although the
table provides only <i>w</i><sub>&alpha;/2</sub> (= 81),
<i>w</i><sub>1&minus;&alpha;/2</sub> = <i>nm</i> &minus;
<i>w</i><sub>&alpha;/2</sub>, or (15)(18) &minus; 81 = 189.

<p>
Since 70.5 &lt; 81, we reject <i>H</i><sub>0</sub>, and we assume
there is a difference in the mean scores of the students.  If we
had applied the median test, we would have come to the opposite
conclusion.  In truth the Mann-Whitney test is more powerful
than the median test.

<p>
Note that some tables have a limitation on the maximal sample size,
namely if <i>n</i> or <i>m</i> are greater than a certain value
(say 20).  In order to arrive at the critical value, a normal
approximation of the hypothesis testing procedure may be adopted.

<p align="center">
<table class="eqnserif" style="margin-top:1em;">
<col><col style="text-align:center;">
<tr>
 <td rowspan="3"><i>z</i> =
 <td colspan="2" style="border-bottom:1px solid black;">
   <i>T</i> &minus; <i>nm</i>/2
<tr>
 <td rowspan="2"><span style="font-size:200%;">&radic;</span>
 <td style="border-top:1px solid black;"><i>nm</i>(<i>n</i> +
    <i>m</i> + 1)
<tr><td  style="border-top:1px solid black;">12
</table>

<p>
This is because, when <i>n</i> or <i>m</i> are large, they
approximate a standard normal distribution.

<h2>The Kruskal-Wallis Test</h2>

A nonparametric alternative to one-way ANOVA.  Samples are
combined and then ranked, with ties being the average of an
assigned rank as usual.  The test statistic is

<p align="center">
<table class="eqnserif" style="margin-top:1em;">
<col><col style="text-align:center;">
<tr><td rowspan="2"><i>H</i> =
 <td style="border-bottom:1px solid black;">
   12 &sum; <i>n<sub>i</sub></i>[<i><span class="overbar">R</span><sub>i</sub>
    &minus; <span class="overbar">R</span></i>]<sup>2</sup>
<tr><td><i>N</i>(<i>N</i> + 1)
</table>
<p>
Compare this to another computational form:
<p align="center">
<table class="eqnserif" style="margin-top:1em;">
<col><col style="text-align:center;">
<tr><td rowspan="2"><i>H</i> =
 <td style="border-bottom:1px solid black;">12
 <td rowspan="2">&sum;<sub
  style="vertical-align:-1.5em;margin-left:-1em;"><i>i</i>=1</sub>
  <sup style="vertical-align:1.5em;margin-left:-1.5em;"><i>k</i></sup>
 <td style="border-bottom:1px solid black;"><i>R<sub>i</sub><sup>2</sup>
 <td rowspan="2">&minus; 3(<i>N</i> + 1)
<tr><td><i>N</i>(<i>N</i> + 1)
 <td><i>n<sub>i</sub></i>
</table>

<p>
<i>n<sub>i</sub></i> is the number of observations in group <i>i</i>,
<i>N</i> is the total sample size,
<i><span class="overbar">R</span><sub>i</sub></i> is the
average of the ranks of group <i>i</i>, and
<i><span class="overbar">R</span></i> is the
average of all ranks.  The distribution of <i>H</i> is approximately
similar to a &chi;<sup>2</sup> distribution with <i>k</i> &minus; 1
degrees of freedom, reasonable if no group has fewer than five
observations.  Where <i>H</i> is large, this would indicate
that differences are to be significant.

<p>
Some have suggested adjusting the value of <i>H</i> where the
number of tied data scores are significant.  The correction
is as follows:

<p align="center">
<table class="eqnserif" style="margin-top:1em;">
<col><col style="text-align:center;">
<tr>
 <td rowspan="2"><i>H</i><sub>adj</sub> =
 <td style="border-bottom:1px solid black;"><i>H</i>
<tr><td>1 &minus; [&sum; (<i>d<sub>j</sub></i><sup>3</sup> &minus;
    <i>d<sub>j</sub></i>)/(<i>N</i> <sup>3</sup> &minus; <i>N</i>)]
</table>

<p>
If there are <i>j</i> distinct values among the <i>N</i>
observations and, for the <i>j</i>th distinct value,
there are <i>d<sub>j</sub></i> tied observations
(<i>d<sub>j</sub></i> = 1 when there are no ties),
the above formula is applied.  Naturally when there
are no ties, <i>H</i><sub>adj</sub> should equal <i>H</i>.
The distribution of <i>H</i><sub>adj</sub> is similar to
<i>H</i> itself.   When samples are small, exact tables
should be used rather than formula.

<p>
A <i>z</i>-value calculated for each group can be obtained
with the following formula:

<p align="center">
<table class="eqnserif" style="margin-top:1em;">
<col><col style="text-align:center;">
<tr>
 <td rowspan="2"><i>z<sub>i</sub></i> =
 <td colspan="2" style="border-bottom:1px solid black;">
  <i><span class="overbar">R</span><sub>i</sub></i> &minus;
  (<i>N</i> + 1)/2
<tr><td>&radic;<td style="border-top:1px solid black;">
   (<i>N</i> + 1)(<i>N</i>/<i>n<sub>i</sub></i> &minus; 1)/12
</table>

<p>
This <i>z</i>-value should be used to assess the significance
for the differences between the group mean ranks
(<i>R<sub>i</sub></i>-bar) and the overall mean rank (<i>R</i>-bar).

<h2>The Friedman Test</h2>

This is the nonparametric for ANOVA of a randomized block design
experiment, which is a bivariate ANOVA.  Randomized block designs
are generalizations of paired experiments, and the Friedman test
is a generalization of the paired sign test.  The null hypothesis
to the Friedman test is that treatment has no effect.

<p>
The test statistic is

<p align="center">
<table class="eqnserif" style="margin-top:1em;">
<col><col style="text-align:center;">
<tr>
 <td><i>S</i> = <i>C</i> <span style="font-size:125%;">&sum;</span>
    [<i>R<sub>j</sub></i> &minus; <span
   class="overbar"><i>R</i></span>)<sup>2</sup>]
</table>

<p>
<i>C</i> is a constant and <i>R<sub>j</sub></i> is the sum of the
ranks within a particular treatment group <i>j</i>.  <span
class="overbar"><i>R</i></span> is
the average of all <i><span class="overbar">R</span><sub>i</sub></i>.
When there are many ties,
the computation of <i>S</i> is complicated and Hollander and Wolfe
(<i>Nonparametric Statistical Methods</i>, J. Wiley &amp; Sons, 1973)
should be consulted.

<p>
Treatment effects are calculated by first finding the median
difference between pairs of treatments.  Gather all the median
differences together.  The effect for each treatment is
the average of the median differences of that treatment
with all other treatments (<i>including</i> itself).
Hence, (effect of treatment i) = &sum; (median difference
between treatment pairs) / number of treatments.

<p>
Now the difference between each observation in a group and
the calculated treatment effect above is calculated (subtract
the observation value using the calculated treatment effect).
The adjusted group/block medians are simply the medians in
treatment block which are now adjusted for the effect of
the treatment.  The grand median is calculated as the median
of these adjusted block medians.  The estimated median for
each treatment group is the effect of the treatment + grand median.
Note that the average of the treatment medians is the grand median.
A residual is calculated as the observation value adjusted for
the effect of treatment subtracted by the adjusted block median.
The fitted value will be the effect of treatment plus the adjusted
block median, or the observation minus the residual.

<h2>The Walsh average</h2>

This is simply the average of all possible pairs of
data/observations/value in a set, including the pairing of
a value with itself.  For a set of <i>n</i> observations,
there should be a matrix with<br>

<i>n</i> (<i>n</i> + 1)/2 Walsh averages.

</body>
</html>
